<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Guide to Nearcore Development</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Architecture</li><li class="chapter-item expanded "><a href="architecture/index.html"><strong aria-hidden="true">1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="architecture/how/index.html"><strong aria-hidden="true">2.</strong> How neard works</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/how/sync.html"><strong aria-hidden="true">2.1.</strong> How Sync Works</a></li><li class="chapter-item expanded "><a href="architecture/how/gc.html"><strong aria-hidden="true">2.2.</strong> Garbage Collection</a></li><li class="chapter-item expanded "><a href="architecture/how/epoch.html"><strong aria-hidden="true">2.3.</strong> How Epoch Works</a></li><li class="chapter-item expanded "><a href="architecture/how/tx_routing.html"><strong aria-hidden="true">2.4.</strong> Transaction Routing</a></li><li class="chapter-item expanded "><a href="architecture/how/tx_receipts.html"><strong aria-hidden="true">2.5.</strong> Transactions And Receipts</a></li><li class="chapter-item expanded "><a href="architecture/how/cross-shard.html"><strong aria-hidden="true">2.6.</strong> Cross shard transactions - deep dive</a></li><li class="chapter-item expanded "><a href="architecture/how/serialization.html"><strong aria-hidden="true">2.7.</strong> Serialization: Borsh, Json, ProtoBuf</a></li><li class="chapter-item expanded "><a href="architecture/how/proofs.html"><strong aria-hidden="true">2.8.</strong> Proofs</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/next/index.html"><strong aria-hidden="true">3.</strong> How neard will work</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/next/catchup_and_state_sync.html"><strong aria-hidden="true">3.1.</strong> Catchup and state sync improvements</a></li><li class="chapter-item expanded "><a href="architecture/next/malicious_chunk_producer_and_phase2.html"><strong aria-hidden="true">3.2.</strong> Malicious producers and phase 2</a></li></ol></li><li class="chapter-item expanded "><a href="architecture/trie.html"><strong aria-hidden="true">4.</strong> Trie</a></li><li class="chapter-item expanded "><a href="architecture/network.html"><strong aria-hidden="true">5.</strong> Network</a></li><li class="chapter-item expanded "><a href="architecture/gas/index.html"><strong aria-hidden="true">6.</strong> Gas Cost Parameters</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="architecture/gas/parameter_definition.html"><strong aria-hidden="true">6.1.</strong> Parameter Definitions</a></li><li class="chapter-item expanded "><a href="architecture/gas/gas_profile.html"><strong aria-hidden="true">6.2.</strong> Gas Profile</a></li><li class="chapter-item expanded "><a href="architecture/gas/estimator.html"><strong aria-hidden="true">6.3.</strong> Runtime Parameter Estimator</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Practices</li><li class="chapter-item expanded "><a href="practices/index.html"><strong aria-hidden="true">7.</strong> Overview</a></li><li class="chapter-item expanded "><a href="practices/rust.html"><strong aria-hidden="true">8.</strong> Rust 🦀</a></li><li class="chapter-item expanded "><a href="practices/workflows/index.html"><strong aria-hidden="true">9.</strong> Workflows</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="practices/workflows/run_a_node.html"><strong aria-hidden="true">9.1.</strong> Run a Node</a></li><li class="chapter-item expanded "><a href="practices/workflows/deploy_a_contract.html"><strong aria-hidden="true">9.2.</strong> Deploy a Contract</a></li><li class="chapter-item expanded "><a href="practices/workflows/gas_estimations.html"><strong aria-hidden="true">9.3.</strong> Run Gas Estimations</a></li></ol></li><li class="chapter-item expanded "><a href="practices/style.html"><strong aria-hidden="true">10.</strong> Code Style</a></li><li class="chapter-item expanded "><a href="practices/docs.html"><strong aria-hidden="true">11.</strong> Documentation</a></li><li class="chapter-item expanded "><a href="practices/tracking_issues.html"><strong aria-hidden="true">12.</strong> Tracking Issues</a></li><li class="chapter-item expanded "><a href="practices/security_vulnerabilities.html"><strong aria-hidden="true">13.</strong> Security Vulnerabilities</a></li><li class="chapter-item expanded "><a href="practices/fast_builds.html"><strong aria-hidden="true">14.</strong> Fast Builds</a></li><li class="chapter-item expanded "><a href="practices/testing/index.html"><strong aria-hidden="true">15.</strong> Testing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="practices/testing/python_tests.html"><strong aria-hidden="true">15.1.</strong> Python Tests</a></li><li class="chapter-item expanded "><a href="practices/testing/test_utils.html"><strong aria-hidden="true">15.2.</strong> Testing Utils</a></li></ol></li><li class="chapter-item expanded "><a href="practices/protocol_upgrade.html"><strong aria-hidden="true">16.</strong> Protocol Upgrade</a></li><li class="chapter-item expanded affix "><li class="part-title">Advanced configuration</li><li class="chapter-item expanded "><a href="advanced_configuration/networking.html"><strong aria-hidden="true">17.</strong> Networking</a></li><li class="chapter-item expanded affix "><li class="part-title">Misc</li><li class="chapter-item expanded "><a href="misc/index.html"><strong aria-hidden="true">18.</strong> Misc</a></li><li class="chapter-item expanded "><a href="misc/database.html"><strong aria-hidden="true">19.</strong> Database Format</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Guide to Nearcore Development</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/near/nearcore/tree/master/docs" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to the nearcore development guide!</p>
<p>The target audience of this guide are developers of nearcore itself. If you are
a user of NEAR (either a contract developer, or validator running a node),
please refer to the user docs at <a href="https://docs.near.org">https://docs.near.org</a>.</p>
<p>This guide is built with <a href="https://rust-lang.github.io/mdBook/">mdBook</a>
from sources in the <a href="https://github.com/near/nearcore/">nearcore repository</a>.
You can edit it by pressing the &quot;edit&quot; icon in the top right corner, we welcome
all contributions. The guide is hosted at <a href="https://near.github.io/nearcore/">https://near.github.io/nearcore/</a>.</p>
<p>The guide is organized as a collection of loosely coupled chapters -- you don't
need to read them in order, feel free to peruse the TOC, and focus on
the interesting bits. The chapters are classified into three parts:</p>
<ul>
<li><a href="./architecture/"><strong>Architecture</strong></a> talks about how the code works.
So, for example, if you are interested in how a transaction flows through the
system, look there!</li>
<li><a href="./practices/"><strong>Practices</strong></a> describe, broadly, how we write code.
For example, if you want to learn about code style, issue tracking, or
debugging performance problems, this is the chapter for you.</li>
<li>Finally, the <a href="./misc/"><strong>Misc</strong></a> part holds various assorted bits
and pieces. We are trying to bias ourselves towards writing more docs, so, if
you want to document something and it doesn't cleanly map to a category above,
just put it in misc!</li>
</ul>
<p>If you are unsure, start with <a href="./architecture/">Architecture Overview</a> and then
read <a href="./practices/workflows/run_a_node.html">Run a Node</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>This document describes the high-level architecture of nearcore. The focus here
is on the implementation of the blockchain protocol, not the protocol itself.
For reference documentation of the protocol, please refer to
<a href="https://nomicon.io/">nomicon</a></p>
<p>Some parts of our architecture are also covered in this <a href="https://www.youtube.com/playlist?list=PL9tzQn_TEuFV4qlts0tVgndnytFs4QSYo">video series on YouTube</a>.</p>
<h2 id="birds-eye-view"><a class="header" href="#birds-eye-view">Bird's Eye View</a></h2>
<p>If we put the entirety of nearcore onto one picture, we get something like this:</p>
<p><img src="architecture/../images/architecture.svg" alt="" /></p>
<p>Don't worry if this doesn't yet make a lot of sense: hopefully, by the end of
this document the above picture would become much clearer!</p>
<h2 id="overall-operation"><a class="header" href="#overall-operation">Overall Operation</a></h2>
<p><code>nearcore</code> is a blockchain node -- it's a single binary (<code>neard</code>) which runs on
some machine and talks to other similar binaries running elsewhere. Together,
the nodes agree (using a distributed consensus algorithm) on a particular
sequence of transactions. Once transaction sequence is established, each node
applies transactions to the current state. Because transactions are fully
deterministic, each node in the network ends up with identical state. To allow
greater scalability, NEAR protocol uses sharding, which allows a node to hold
only a small subset (shard) of the whole state.</p>
<p><code>neard</code> is a stateful, restartable process. When <code>neard</code> starts, the node
connects to the network and starts processing blocks (block is a batch of
transactions, processed together; transactions are batched into blocks for
greater efficiency). The results of processing are persisted in the database.
RocksDB is used for storage. Usually, the node's data is found in the <code>~/.near</code>
directory. The node can be stopped at any moment and be restarted later. While
the node is offline it misses the block, so, after a restart, the sync process
kicks in which brings the node up-to-speed with the network by downloading the
missing bits of history from more up-to-date peer nodes.</p>
<p>Major components of nearcore:</p>
<ul>
<li>
<p><strong>JSON RPC</strong>. This HTTP RPC interface is how <code>neard</code> communicates with
non-blockchain outside world. For example, to submit a transaction, some
client sends an RPC request with it to some node in the network. From that
node, the transaction propagates through the network, until it is included in
some block. Similarly, a client can send an HTTP request to a node to learn
about current state of the blockchain. The <strong>JSON RPC</strong> interface is documented
<a href="https://docs.near.org/api/rpc/introduction">here</a>.</p>
</li>
<li>
<p><strong>Network</strong>. If RPC is aimed &quot;outside&quot; the blockchain, &quot;network&quot; is how peer
<code>neard</code> nodes communicate with each other within the blockchain. RPC carries
requests from users of the blockchain, while network carries various messages
needed to implement consensus. Two directly connected nodes communicate by
sending protobuf-encoded messages over TCP. A node also includes logic to
route messages for indirect peers through intermediaries. Oversimplifying a
lot, it's enough for a new node to know an IP address of just one other
network participant. From this bootstrap connection, the node learns how to
communicate with any other node in the network.</p>
</li>
<li>
<p><strong>Client</strong>. Somewhat confusingly named, <strong>client</strong> is the logical state of the
blockchain. After receiving and decoding a request, both <strong>RPC</strong> and <strong>network</strong>
usually forward it in the parsed form to the <strong>client</strong>. Internally, <strong>client</strong> is
split in two somewhat independent components: <strong>chain</strong> and <strong>runtime</strong>.</p>
</li>
<li>
<p><strong>Chain</strong>. The job of <strong>chain</strong>, in a nutshell, is to determine a global order of
transactions. <strong>Chain</strong> builds and maintains the blockchain data structure. This
includes block and chunk production and processing, consensus, and validator
selection. However, <strong>chain</strong> is not responsible for actually applying
transactions and receipts.</p>
</li>
<li>
<p><strong>Runtime</strong>. If <strong>chain</strong> selects the <em>order</em> of transactions, <strong>Runtime</strong> applies
transaction to the state. <strong>Chain</strong> guarantees that everyone agrees on the order
and content of transactions, and <strong>Runtime</strong> guarantees that each transaction is
fully deterministic. It follows that everyone agrees on the &quot;current state&quot; of
the blockchain. Some transactions are as simple as &quot;transfer X tokens from
Alice to Bob&quot;. But a much more powerful class of transactions is supported:
&quot;run this arbitrary WebAssembly code in the context of the current state of
the chain&quot;. Running such &quot;smart contract&quot; transactions securely and
efficiently is a major part of what <strong>Runtime</strong> does. Today, <strong>Runtime</strong> uses a JIT
compiler to do that.</p>
</li>
<li>
<p><strong>Storage</strong>. <strong>Storage</strong> is more of a cross-cutting concern, than an isolated
component. Many parts of a node want to durably persist various bits of state
to disk. One notable case is the logical state of the blockchain, and, in
particular, data associated with each account. Logically, the state of an account
on a chain is a key-value map: <code>HashMap&lt;Vec&lt;u8&gt;, Vec&lt;u8&gt;&gt;</code>. But there is a
twist: it should be possible to provide a succinct proof that a particular key
indeed holds a particular value. To allow that internally the state is
implemented as a persistent (in both senses, &quot;functional&quot; and &quot;on disk&quot;)
merkle-patricia trie.</p>
</li>
<li>
<p><strong>Parameter Estimator</strong>. One kind of transaction we support is &quot;run this
arbitrary, Turing-complete computation&quot;. To protect from a <code>loop {}</code>
transaction halting the whole network, <strong>Runtime</strong> implements resource limiting:
each transaction runs with a certain finite amount of &quot;gas&quot;, and each
operation costs a certain amount of gas to perform. <strong>Parameter estimator</strong> is
essentially a set of benchmarks used to estimate relative gas costs of
various operations.</p>
</li>
</ul>
<h2 id="entry-points"><a class="header" href="#entry-points">Entry Points</a></h2>
<p><code>neard/src/main.rs</code> contains the main function that starts a blockchain node.
However, this file mostly only contains the logic to parse arguments and
dispatch different commands. <code>start_with_config</code> in <code>nearcore/src/lib.rs</code> is the
actual entry point and it starts all the actors.</p>
<p><code>JsonRpcHandler::process</code> in the <code>jsonrpc</code> crate is the RPC entry point. It
implements the public API of a node, which is documented
<a href="https://docs.near.org/api/rpc/introduction">here</a>.</p>
<p><code>PeerManagerActor::spawn</code> in the <code>network</code> is an entry for the other point of
contract with the outside world -- the peer-to-peer network.</p>
<p><code>Runtime::apply</code> in the <code>runtime</code> crate is the entry point for transaction
processing logic. This is where state transitions actually happen, after chain
decided, according to distributed consensus, which transitions need  to
happen.</p>
<h2 id="code-map"><a class="header" href="#code-map">Code Map</a></h2>
<p>This section contains some high-level overview of important crates and data
structures.</p>
<h3 id="coreprimitives"><a class="header" href="#coreprimitives"><code>core/primitives</code></a></h3>
<p>This crate contains most of the types that are shared across different crates.</p>
<h3 id="coreprimitives-core"><a class="header" href="#coreprimitives-core"><code>core/primitives-core</code></a></h3>
<p>This crate contains types needed for runtime.</p>
<h3 id="corestoretrie"><a class="header" href="#corestoretrie"><code>core/store/trie</code></a></h3>
<p>This directory contains the MPT state implementation. Note that we usually use
<code>TrieUpdate</code> to interact with the state.</p>
<h3 id="chainchain"><a class="header" href="#chainchain"><code>chain/chain</code></a></h3>
<p>This crate contains most of the chain logic (consensus, block processing, etc).
<code>ChainUpdate::process_block</code> is where most of the block processing logic
happens.</p>
<p><strong>Architecture Invariant</strong>: interface between chain and runtime is defined by
<code>RuntimeAdapter</code>. All invocations of runtime go through <code>RuntimeAdapter</code></p>
<p><strong>State update</strong></p>
<p>The blockchain state of a node can be changed in the following two ways:</p>
<ul>
<li>Applying a chunk. This is how the state is normally updated: through
<code>Runtime::apply</code>.</li>
<li>State sync. State sync can happen in two cases:
<ul>
<li>A node is far enough behind the most recent block and triggers state sync to
fast forward to the state of a very recent block without having to apply
blocks in the middle.</li>
<li>A node is about to become validator for some shard in the next epoch, but it
does not yet have the state for that shard. In this case, it would run state
sync through the <code>catchup</code> routine.</li>
</ul>
</li>
</ul>
<h3 id="chainchunks"><a class="header" href="#chainchunks"><code>chain/chunks</code></a></h3>
<p>This crate contains most of the sharding logic which includes chunk creation,
distribution, and processing. <code>ShardsManager</code> is the main struct that
orchestrates everything here.</p>
<h3 id="chainclient"><a class="header" href="#chainclient"><code>chain/client</code></a></h3>
<p>This crate defines two important structs, <code>Client</code> and <code>ViewClient</code>. <code>Client</code>
includes everything necessary for the chain (without network and runtime) to
function and runs in a single thread. <code>ViewClient</code> is a &quot;read-only&quot; client that
answers queries without interfering with the operations of <code>Client</code>.
<code>ViewClient</code> runs in multiple threads.</p>
<h3 id="chainnetwork"><a class="header" href="#chainnetwork"><code>chain/network</code></a></h3>
<p>This crate contains the entire implementation of the p2p network used by NEAR
blockchain nodes.</p>
<p>Two important structs here: <code>PeerManagerActor</code> and <code>Peer</code>. Peer manager
orchestrates all the communications from network to other components and from
other components to network. <code>Peer</code> is responsible for low-level network
communications from and to a given peer. Peer manager runs in one thread while
each <code>Peer</code> runs in its own thread.</p>
<!--TODO: Maybe add more clarification about what Peer is? -->
<p><strong>Architecture Invariant</strong>: Network communicates to <code>Client</code> through
<code>NetworkClientMessages</code> and to <code>ViewClient</code> through <code>NetworkViewClientMessages</code>.
Conversely, <code>Client</code> and <code>ViewClient</code> communicates to network through
<code>NetworkRequests</code>.</p>
<h3 id="chainepoch_manager"><a class="header" href="#chainepoch_manager"><code>chain/epoch_manager</code></a></h3>
<p>This crate is responsible for determining validators and other epoch related
information such as epoch id for each epoch.</p>
<p><strong>Note</strong>: <code>EpochManager</code> is constructed in <code>NightshadeRuntime</code> rather than in
<code>Chain</code>, partially because we had this idea of making epoch manager a smart
contract.</p>
<h3 id="chainjsonrpc"><a class="header" href="#chainjsonrpc"><code>chain/jsonrpc</code></a></h3>
<p>This crate implements <a href="https://www.jsonrpc.org/">JSON-RPC</a> API server to enable
submission of new transactions and inspection of the blockchain data, the
network state, and the node status. When a request is processed, it generates a
message to either <code>ClientActor</code> or <code>ViewClientActor</code> to interact with the
blockchain. For queries of blockchain data, such as block, chunk, account, etc,
the request usually generates a message to <code>ViewClientActor</code>. Transactions, on
the other hand, are sent to <code>ClientActor</code> for further processing.</p>
<h3 id="runtimeruntime"><a class="header" href="#runtimeruntime"><code>runtime/runtime</code></a></h3>
<p>This crate contains the main entry point to runtime -- <code>Runtime::apply</code>. This
function takes <code>ApplyState</code>, which contains necessary information passed from
chain to runtime, a list of <code>SignedTransaction</code> and a list of <code>Receipt</code>, and
returns a <code>ApplyResult</code>, which includes state changes, execution outcomes, etc.</p>
<p><strong>Architecture Invariant</strong>: The state update is only finalized at the end of
<code>apply</code>. During all intermediate steps state changes can be reverted.</p>
<h3 id="runtimenear-vm-logic"><a class="header" href="#runtimenear-vm-logic"><code>runtime/near-vm-logic</code></a></h3>
<p><code>VMLogic</code> contains all the implementations of host functions and is the
interface between runtime and wasm. <code>VMLogic</code> is constructed when runtime
applies function call actions. In <code>VMLogic</code>, interaction with NEAR blockchain
happens in the following two ways:</p>
<ul>
<li><code>VMContext</code>, which contains lightweight information such as current block
hash, current block height, epoch id, etc.</li>
<li><code>External</code>, which is a trait that contains functions to interact with
blockchain by either reading some nontrivial data, or writing to the
blockchain.</li>
</ul>
<h3 id="runtimenear-vm-runner"><a class="header" href="#runtimenear-vm-runner"><code>runtime/near-vm-runner</code></a></h3>
<p><code>run</code> function in <code>runner.rs</code> is the entry point to the vm runner. This function
essentially spins up the vm and executes some function in a contract. It
supports different wasm compilers including wasmer0, wasmer2, and wasmtime
through compile-time feature flags. Currently we use wasmer0 and wasmer2 in
production. The <code>imports</code> module exposes host functions defined in
<code>near-vm-logic</code> to WASM code. In other words, it defines the ABI of the
contracts on NEAR.</p>
<h3 id="neard"><a class="header" href="#neard"><code>neard</code></a></h3>
<p>As mentioned before, <code>neard</code> is the crate that contains that main entry points.
All the actors are spawned in <code>start_with_config</code>. It is also worth noting that
<code>NightshadeRuntime</code> is the struct that implements <code>RuntimeAdapter</code>.</p>
<!-- TODO: Maybe add RuntimeAdapter mention or explanation in runtime/runtime chapter? -->
<h3 id="corestoresrcdbrs"><a class="header" href="#corestoresrcdbrs"><code>core/store/src/db.rs</code></a></h3>
<p>This file contains the schema (DBCol) of our internal RocksDB storage - a good
starting point when reading the code base.</p>
<h2 id="cross-cutting-concerns"><a class="header" href="#cross-cutting-concerns">Cross Cutting Concerns</a></h2>
<h3 id="observability"><a class="header" href="#observability">Observability</a></h3>
<p>The <a href="https://tracing.rs">tracing</a> crate is used for structured, hierarchical
event output and logging. We also integrate <a href="https://prometheus.io">Prometheus</a>
for light-weight metric output. See the <a href="architecture/./style.html">style</a> documentation for
more information on the usage.</p>
<h3 id="testing"><a class="header" href="#testing">Testing</a></h3>
<p>Rust has built-in support for writing unit tests by marking functions
with the <code>#[test]</code> directive.  Take full advantage of that!  Testing not
only confirms that what was written works the way it was intended to but
also helps during refactoring since it catches unintended behaviour
changes.</p>
<p>Not all tests are created equal though and while some may only need
milliseconds to run, others may run for several seconds or even
minutes.  Tests that take a long time should be marked as such with an
<code>expensive_tests</code> feature, for example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[test]
#[cfg_attr(not(feature = &quot;expensive_tests&quot;), ignore)]
fn test_catchup_random_single_part_sync() {
    test_catchup_random_single_part_sync_common(false, false, 13)
}
<span class="boring">}
</span></code></pre></pre>
<p>Such tests will be ignored by default and can be executed by using
<code>--ignored</code> or <code>--include-ignored</code> flag as in <code>cargo test -- --ignored</code> or by compiling the tests with <code>expensive_tests</code> feature
enabled.</p>
<p>Because expensive tests are not run by default, they are also not run
in CI.  Instead, they are run nightly and need to be explicitly
included in <code>nightly/expensive.txt</code> file; for example:</p>
<pre><code class="language-text">expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync
expensive --timeout=1800 near-client near_client tests::catching_up::test_catchup_random_single_part_sync --features nightly
</code></pre>
<p>For more details regarding nightly tests see <code>nightly/README.md</code>.</p>
<p>Note that what counts as a slow test isn’t exactly defined as of now.
If it takes just a couple seconds than it’s probably fine.  Anything
slower should probably be classified as an expensive test.  In
particular, if libtest complains the test takes more than 60 seconds
than it definitely is and expensive test.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-neard-works"><a class="header" href="#how-neard-works">How neard works</a></h1>
<p>This chapter describes how neard works with a focus on implementation details
and practical scenarios. To get a better understanding of how the protocol
works, please refer to <a href="https://nomicon.io">nomicon</a>. For a high-level code map
of nearcore, please refer to this <a href="architecture/how/../">document</a>.</p>
<h2 id="high-level-overview"><a class="header" href="#high-level-overview">High level overview</a></h2>
<p>On the high level, neard is a daemon that periodically receives messages from
the network and sends messages to peers based on different triggers. Neard is
implemented using an <a href="https://en.wikipedia.org/wiki/Actor_model">actor
framework</a> called
<a href="https://docs.rs/actix">actix</a>.</p>
<p><strong>Note</strong>: Using actix was decided in the early days of the implementation of
nearcore and by no means represents our confidence in actix. On the contrary, we
have noticed a number of issues with actix and are considering implementing an
actor framework in house.</p>
<p>There are several important actors in neard:</p>
<ul>
<li>
<p><code>PeerActor</code> - Each peer is represented by one peer actor and runs in a separate
thread. It is responsible for sending messages to and receiving messages from
a given peer. After <code>PeerActor</code> receives a message, it will route it to
<code>ClientActor</code>, <code>ViewClientActor</code>, or <code>PeerManagerActor</code> depending on the type
of the message.</p>
</li>
<li>
<p><code>PeerManagerActor</code> - Peer Manager is responsible for receiving messages to send
to the network from either <code>ClientActor</code> or <code>ViewClientActor</code> and routing them to
the right <code>PeerActor</code> to send the bytes over the wire. It is also responsible for
handling some types of network messages received and routed through <code>PeerActor</code>.
For the purpose of this document, we only need to know that <code>PeerManagerActor</code>
handles <code>RoutedMessage</code>s. Peer manager would decide whether the <code>RoutedMessage</code>s
should be routed to <code>ClientActor</code> or <code>ViewClientActor</code>.</p>
</li>
<li>
<p><code>ClientActor</code> - Client actor is the “core” of neard. It contains all the main
logic including consensus, block and chunk processing, state transition, garbage
collection, etc. Client actor is single threaded.</p>
</li>
<li>
<p><code>ViewClientActor</code> - View client actor can be thought of as a read-only interface
to <strong>client</strong>. It only accesses data stored in a node’s storage and does not mutate
any state. It is used for two purposes:</p>
<ul>
<li>Answering RPC requests by fetching the relevant piece of data from storage.</li>
<li>Handling some network requests that do not require any changes to the
storage, such as header sync, state sync, and block sync requests.</li>
</ul>
<p><code>ViewClientActor</code> runs in four threads by default but this number is configurable.</p>
</li>
</ul>
<h2 id="data-flow-within-neard"><a class="header" href="#data-flow-within-neard">Data flow within <code>neard</code></a></h2>
<p>Flow for incoming messages:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195619986-25798cde-8a91-4721-86bd-93fa924b483a.png" alt="" /></p>
<p>Flow for outgoing messages:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195626792-7697129b-7f9c-4953-b939-0b9bcacaf72c.png" alt="" /></p>
<h2 id="how-neard-operates-when-it-is-fully-synced"><a class="header" href="#how-neard-operates-when-it-is-fully-synced">How neard operates when it is fully synced</a></h2>
<p>When a node is fully synced, the main logic of the node operates in the
following way (the node is assumed to track all shards, as most nodes on mainnet
do today):</p>
<ol>
<li>A block is produced by some block producer and sent to the node through
broadcasting.</li>
<li>The node receives a block and tries to process it. If the node is synced it
presumably has the previous block and the state before the current block to
apply. It then checks whether it has all the chunks available. If the node is
not a validator node, it won’t have any chunk parts and therefore won’t have
the chunks available. If the node is a validator node, it may already have
chunk parts through chunk parts forwarding from other nodes and therefore may
have already reconstructed some chunks. Regardless, if the node doesn’t have all
chunks for all shards, it will request them from peers by parts.</li>
<li>The chunk requests are sent and the node waits for enough chunk parts to be
received to reconstruct the chunks. For each chunk, 1/3 of all the parts<!-- TODO: Is 100 the number of all the parts or one third of all the parts? -->
(100) is sufficient to reconstruct a chunk. If new blocks arrive while waiting
for chunk parts, they will be put into a <code>OrphanPool</code>, waiting to be processed.
If a chunk part request is not responded to within <code>chunk_request_retry_period</code>,
which is set to 400ms by default, then a request for the same chunk part
would be sent again.</li>
<li>After all chunks are reconstructed, the node processes the current block by
applying transactions and receipts from the chunks. Afterwards, it will
update the head according to the fork choice rule, which only looks at block
height. In other words, if the newly processed block is of higher height than
the current head of the node, the head is updated.</li>
<li>The node checks whether any blocks in the <code>OrphanPool</code> are ready to be
processed in a BFS order and processes all of them until none can be
processed any more. Note that a block is put into the <code>OrphanPool</code> if and
only if its previous block is not accepted.</li>
<li>Upon acceptance of a block, the node would check whether it needs to run
garbage collection. If it needs to, it would garbage collect two blocks worth
of data at a time. The logic of garbage collection is complicated and could
be found <a href="architecture/how/./gc.html">here</a>.</li>
<li>If the node is a validator node, it would start a timer after the current
block is accepted. After <code>min_block_production_delay</code> which is currently
configured to be 1.3s on mainnet, it would send an approval to the block
producer of the next block (current block height + 1).</li>
</ol>
<p>The main logic is illustrated below:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195635652-f0c7ebae-a2e5-423f-8e62-b853b815fcec.png" alt="" /></p>
<h2 id="how-neard-works-when-it-is-synchronizing"><a class="header" href="#how-neard-works-when-it-is-synchronizing">How neard works when it is synchronizing</a></h2>
<p><code>PeerManagerActor</code> periodically sends a <code>NetworkInfo</code> message to <code>ClientActor</code>
to update it on the latest peer information, which includes the height of each
peer. Once <code>ClientActor</code> realizes that it is more than <code>sync_height_threshold</code>
(which by default is set to 1) behind the highest height among peers, it starts
to sync. The synchronization process is done in three steps:</p>
<ol>
<li>
<p>Header sync. The node first identifies the headers it needs to sync through a
<a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/chain/*client/src/sync.rs#L332"><code>get_locator</code></a>
calculation. This is essentially an exponential backoff computation that
tries to identify commonly known headers between the node and its peers. Then
it would request headers from different peers, at most
<code>MAX_BLOCK_HEADER_HASHES</code> (which is 512) headers at a time.</p>
</li>
<li>
<p>After the headers are synced, the node would determine whether it needs to
run state sync. The exact condition can be found
<a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/chain/client/src/sync.rs#L458">here</a>
but basically a node would do state sync if it is more than 2 epochs behind
the head of the network. State sync is a very complex process and warrants
its own section. We will give a high level overview here.</p>
<ol>
<li>First, the node computes <code>sync_hash</code> which is the hash of the block that
identifies the state that the node wants to sync. This is guaranteed to be
the first block of the most recent epoch. In fact, there is a
<a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/chain/chain/src/chain.rs#L4292">check</a>
on the receiver side that this is indeed the case. The node would also
request the block whose hash is <code>sync_hash</code></li>
<li>The node <a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/chain/chain/src/chain.rs#L1809">deletes basically all data (blocks, chunks, state) from its
storage</a>.
This is not an optimal solution, but it makes the implementation for
combining state easier when there is no stale data in storage.</li>
<li>For the state of each shard that the node needs to download, it first
requests a
<a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/core/primitives/src/syncing.rs#L40">header</a>
that contains some metadata the node needs to know about. Then the node
computes the number of state parts it needs to download and requests those
parts from different peers who track the shard.</li>
<li>After all parts are downloaded, the node <a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/chain/client/src/client_actor.rs#L1877">combines those state
parts</a>
and then
<a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/chain/chain/src/chain.rs#L3065">finalizes</a>
the state sync by applying the last chunk included in or before the sync
block so that the node has the state after applying sync block to be able
to apply the next block.</li>
<li>The node <a href="https://github.com/near/nearcore/blob/279044f09a7e6e5e3f26db4898af3655dae6eda6/chain/chain/src/chain.rs#L1874">resets
heads</a>
properly after state sync.</li>
</ol>
</li>
<li>
<p>Block Sync. The node first gets the block with highest height that is on the
canonical chain and request from there <code>MAX_BLOCK_REQUESTS</code> (which is set to 5)
blocks from different peers in a round robin order. The block sync routine
runs again if head has changed (progress is made) or if a timeout (which is
set to 2s) has happened.</p>
</li>
</ol>
<p><strong>Note</strong>: when a block is received and its height is no more than 500 + the
node’s current head height, then the node would request its previous block
automatically. This is called orphan sync and helps to speed up the syncing
process. If, on the other hand, the height is more than 500 + the node’s current
head height, the block is simply dropped.</p>
<!-- TODO: Either this note is incorrect or the block processing diagram is. -->
<h2 id="how-clientactor-works"><a class="header" href="#how-clientactor-works">How <code>ClientActor</code> works</a></h2>
<p>ClientActor has some periodically running routines that are worth noting:</p>
<ul>
<li><a href="https://github.com/near/nearcore/blob/fa78002a1b4119e5efe277c3073b3f333f451ffc/chain/client/src/client_actor.rs#L1198">Doomslug
timer</a> - 
This routine runs every <code>doosmslug_step_period</code> (set to 100ms by default) and
updates consensus information. If the node is a validator node, it also sends
approvals when necessary.</li>
<li><a href="https://github.com/near/nearcore/blob/fa78002a1b4119e5efe277c3073b3f333f451ffc/chain/client/src/client_actor.rs#L991">Block
production</a> - 
This routine runs every <code>block_production_tracking_delay</code> (which is set to
100ms by default) and checks if the node should produce a block.</li>
<li><a href="https://github.com/near/nearcore/blob/fa78002a1b4119e5efe277c3073b3f333f451ffc/chain/client/src/client_actor.rs#L1790">Log
summary</a> - 
Prints a log line that summarizes block rate, average gas used, the height of
the node, etc. every 10 seconds.</li>
<li><a href="https://github.com/near/nearcore/blob/fa78002a1b4119e5efe277c3073b3f333f451ffc/chain/chunks/src/lib.rs#L910">Resend chunk
requests</a> - 
This routine runs every <code>chunk_request_retry_period</code> (which is set to 400ms).
It resends the chunk part requests for those that are not yet responded to.</li>
<li><a href="https://github.com/near/nearcore/blob/fa78002a1b4119e5efe277c3073b3f333f451ffc/chain/client/src/client_actor.rs#L1629">Sync</a> - 
This routine runs every <code>sync_step_period</code> (which is set to 10ms by default)
and checks whether the node needs to sync from its peers and, if needed, also
starts the syncing process.</li>
<li><a href="https://github.com/near/nearcore/blob/fa78002a1b4119e5efe277c3073b3f333f451ffc/chain/client/src/client_actor.rs#L1581">Catch
up</a> - 
This routine runs every <code>catchup_step_period</code> (which is set to 100ms by
default) and runs the catch up process. This only applies if a node validates
shard A in epoch X and is going to validate a different shard B in epoch X+1.
In this case, the node would start downloading the state for shard B at the
beginning of epoch X. After the state downloading is complete, it would apply
all blocks in the current epoch (epoch X) for shard B to ensure that the node
has the state needed to validate shard B when epoch X+1 starts.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-sync-works"><a class="header" href="#how-sync-works">How Sync Works</a></h1>
<h2 id="basics"><a class="header" href="#basics">Basics</a></h2>
<p>While Sync and Catchup sounds similar - they are actually describing two
completely different things.</p>
<p><strong>Sync</strong> - is used when your node falls ‘behind’ other nodes in the network (for
example because it was down for some time or it took longer to process some
blocks etc).</p>
<p><strong>Catchup</strong> - is used when you want (or have to) start caring about (a.k.a.
tracking) additional shards in the future epochs. Currently it should be a no-op
for 99% of nodes (see below).</p>
<p><strong>Tracking shards</strong>: as you know our system has multiple shards (currently 4).
Currently 99% of nodes are tracking all the shards: validators have to - as they
have to validate the chunks from all the shards, and normal nodes mostly also
track all the shards as this is default.</p>
<p>But in the future - we will have more and more people tracking only a subset of
the shards, so the catchup will be increasingly important.</p>
<h2 id="sync"><a class="header" href="#sync">Sync</a></h2>
<p>If your node is behind the head - it will start the sync process (this code is
running periodically in the client_actor and if you’re behind for more than
<code>sync_height_threshold</code> (currently 50) blocks - it will enable the sync.</p>
<p>The Sync behavior differs depending on whether you’re an archival node (which
means you care about the state of each block) or ‘normal’ node - where you care
mostly about the Tip of the network.</p>
<h3 id="step-1-header-sync-archival-node--normal-node-downloading-headers"><a class="header" href="#step-1-header-sync-archival-node--normal-node-downloading-headers">Step 1: Header Sync [archival node &amp; normal node*] (“downloading headers”)</a></h3>
<p>The goal of the header sync is to get all the block headers from your current
HEAD all the way to the top of the chain.</p>
<p>As headers are quite small, we try to request multiple of them in a single call
(currently we ask for 512 headers at once).</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195892312-2fbd8241-87ce-4241-a44d-ff3056b12bab.png" alt="image" /></p>
<h3 id="step-1a-epoch-sync-normal-node--not-implemented-yet"><a class="header" href="#step-1a-epoch-sync-normal-node--not-implemented-yet">Step 1a: Epoch Sync [normal node*] // not implemented yet</a></h3>
<p>While currently normal nodes are using Header sync, we could actually allow them
to do something faster - “light client sync” a.k.a “epoch sync”.</p>
<p>The idea of the epoch sync, is to read “just” a single block header from each
epoch - that has to contain additional information about validators.</p>
<p>This way it would drastically reduce both the time needed for the sync and the
db resources.</p>
<p>We plan to enable it in Q4 2022.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195892336-cc117c08-d3ad-43f7-9304-3233b25e8bb1.png" alt="image" /></p>
<p>Notice that in the image above - it is enough to only get the ‘last’ header from
each epoch. For the ‘current’ epoch, we still need to get all the headers.</p>
<h3 id="step-2-state-sync-normal-node"><a class="header" href="#step-2-state-sync-normal-node">Step 2: State sync [normal node]</a></h3>
<p>After header sync - if you notice that you’re too far behind (controlled by
<code>block_fetch_horizon</code> config option) <strong>AND</strong> that the chain head is in a different
epoch than your local head - the node will try to do the ‘state sync’.</p>
<p>The idea of the state sync - is rather than trying to process all the blocks -
try to ‘jump’ ahead by downloading the freshest state instead - and continue
processing blocks from that place in the chain. As a side effect, it is going to
create a ‘gap’ in the chunks/state on this node (which is fine - as the data
will be garbage collected after 5 epochs anyway). State sync will ONLY sync to
the beginning of the epoch - it cannot sync to any random block.</p>
<p>This step is never run on the archival nodes - as these nodes want to have whole
history and cannot have any gaps.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195892354-cf2befed-98e9-40a2-9b81-b5cf738406e0.png" alt="image" /></p>
<p>In this case, we can skip processing transactions that are in the blocks 124 - 128, and start from 129 (after sync state finishes)</p>
<h3 id="step-3-block-sync-aka-body-sync-archival-node-normal-node-downloading-blocks"><a class="header" href="#step-3-block-sync-aka-body-sync-archival-node-normal-node-downloading-blocks">Step 3: Block sync (a.k.a Body sync) [archival node, normal node] (“downloading blocks”)</a></h3>
<p>The final step is to start requesting and processing blocks as soon as possible,
hoping to catch up with the chain.</p>
<p>Block sync will request up to 5  (<code>MAX_BLOCK_REQUESTS</code>) blocks at a time - sending
explicit Network BlockRequests for each one.</p>
<p>After the response (Block) is received - the code will execute the ‘standard’ path
that tries to add this block to the chain (see section below).</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195892370-b177228b-2520-486a-94fc-67a91978cb58.png" alt="image" /></p>
<p>In this case, we are processing each transaction for each block - until we catch
up with the chain.</p>
<h2 id="side-topic-how-blocks-are-added-to-the-chain"><a class="header" href="#side-topic-how-blocks-are-added-to-the-chain">Side topic: how blocks are added to the chain?</a></h2>
<p>A node can receive a Block in two ways:</p>
<ul>
<li>Either by broadcasting - when a new block is produced, its contents are
broadcasted within the network by the nodes</li>
<li>Or by explicitly sending a BlockRequest to another peer - and getting a Block
in return.</li>
</ul>
<p>(in case of broadcasting, the node will automatically reject any Blocks that are
more than 500 (<code>BLOCK_HORIZON</code>) blocks away from the current HEAD).</p>
<p>When a given block is received, the node checks if it can be added to the
current chain.</p>
<p>If block’s “parent” (<code>prev_block</code>) is not in the chain yet - the block gets added
to the orphan list.</p>
<p>If the parent is already in the chain - we can try to add the block as the head
of the chain.</p>
<p>Before adding the block, we want to download the chunks for the shards that we
are tracking - so in many cases, we’ll call <code>missing_chunks</code> functions that will
try to go ahead and request those chunks.</p>
<p><strong>Note</strong>: as an optimization, we’re also sometimes trying to fetch chunks for
the blocks that are in the orphan pool – but only if they are not more than 3
(<code>NUM_ORPHAN_ANCESTORS_CHECK</code>) blocks away from our head.</p>
<p>We also keep a separate job in client_actor that keeps retrying chunk fetching
from other nodes if the original request fails.</p>
<p>After all the chunks for a given block are received (we have a separate HashMap
that checks how many chunks are missing for each block) - we’re ready to
process the block and attach it to the chain.</p>
<p>Afterwards, we look at other entries in the orphan pool to see if any of them
are a direct descendant of the block that we just added - and if yes, we repeat
the process.</p>
<h2 id="catchup"><a class="header" href="#catchup">Catchup</a></h2>
<h3 id="the-goal-of-catchup"><a class="header" href="#the-goal-of-catchup">The goal of catchup</a></h3>
<p>Catchup is needed when not all nodes in the network track all shards and nodes
can change the shard they are tracking during different epochs.</p>
<p>For example, if a node tracks shard 0 at epoch T and tracks shard 1 at epoch T+1,
it actually needs to have the state of shard 1 ready before the beginning of
epoch T+1. We make sure this happens by making the node start downloading
the state for shard 1 at the beginning of epoch T and applying blocks during
epoch T to shard 1’s state. Because downloading state can take time, the
node may have already processed some blocks (for shard 0 at this epoch), so when
the state finishes downloading, the node needs to “catch up” processing these
blocks for shard 1.</p>
<p>Right now, all nodes do track all shards, so technically we shouldn’t need the
catchup process, but it is still implemented for the future.</p>
<p>Image below: Example of the node, that tracked only shard 0 in epoch T-1, and
will start tracking shard 0 &amp; 1 in epoch T+1.</p>
<p>At the beginning of the epoch T, it will initiate the state download (green) and
afterwards will try to ‘catchup’ the blocks (orange). After blocks are caught
up, it will continue processing as normal.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195892395-2e12808e-002b-4c04-9505-611288386dc8.png" alt="image" /></p>
<h3 id="how-catchup-interact-with-normal-block-processing"><a class="header" href="#how-catchup-interact-with-normal-block-processing">How catchup interact with normal block processing</a></h3>
<p>The catchup process has two phases: downloading states for shards that we are
going to care about in epoch T+1 and catching up blocks that have already been
applied.</p>
<p>When epoch T starts, the node will start downloading states of shards that it
will track for epoch T+1, which it doesn't track already. Downloading happens in
a different thread so <code>ClientActor</code> can still process new blocks. Before the
shard states for epoch T+1 are ready, processing new blocks only applies chunks
for the shards that the node is tracking in epoch T. When the shard states for
epoch T+1 finish downloading, the catchup process needs to reprocess the
blocks that have already been processed in epoch T to apply the chunks for the
shards in epoch T+1.</p>
<p>In other words, there are three modes for applying chunks and two code paths,
either through the normal <code>process_block</code> (blue) or through <code>catchup_blocks</code>
(orange). When <code>process_block</code>, either that the shard states for the next epoch
are ready, corresponding to <code>IsCaughtUp</code> and all shards the node is tracking in
this, or will be tracking in the next, epoch will be applied, or when the
states are not ready, corresponding to <code>NotCaughtUp</code>, then only the shards for
this epoch will be applied. When <code>catchup_blocks</code>, shards for the next epoch
will be applied.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>enum ApplyChunksMode {
    IsCaughtUp,
    CatchingUp,
    NotCaughtUp,
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="how-catchup-works"><a class="header" href="#how-catchup-works">How catchup works</a></h3>
<p>The catchup process is initiated by <code>process_block</code>, where we check if the block
is caught up and if we need to download states. The logic works as follows:</p>
<ul>
<li>For the first block in an epoch T, we check if the previous block is caught
up, which signifies if the state of the new epoch is ready. If the previous
block is not caught up, the block will be orphaned and not processed for now
because it is not ready to be processed yet. Ideally, this case should never
happen, because the node will appear stalled until the blocks in the previous
epoch are catching up.</li>
<li>Otherwise, we start processing blocks for the new epoch T. For the first
block, we always consider it as not caught up and will initiate the process
for downloading states for shards that we are going to care about in epoch
T+1.</li>
<li>For other blocks, we consider it caught up if the previous block is caught up.</li>
<li><code>process_block</code> will apply chunks differently depending on whether the block
is caught up or not, as we discussed in <code>ApplyChunksMode</code>.</li>
</ul>
<p>The information of which blocks need to catch up (<code>add_block_to_catch_up</code>) and
which new states need to be downloaded (<code>add_state_dl_info</code>) are stored in
storage to be persisted across different runs.</p>
<p>The catchup process is implemented through the function <code>Client::run_catchup</code>.
<code>ClientActor</code> schedules a call to <code>run_catchup</code> every 100ms. However, the call
can be delayed if ClientActor has a lot of messages in its actix queue.</p>
<p>Every time <code>run_catchup</code> is called, it checks the store to see if there are any
shard states that should be downloaded (<code>iterate_state_sync_infos</code>). If so, it
initiates the syncing process for these shards. After the state is downloaded,
<code>run_catchup</code> will start to apply blocks that need to be caught up.</p>
<p>One thing to note is that <code>run_catchup</code> is located at <code>ClientActor</code>, but
intensive work such as applying state parts and applying blocks is actually
offloaded to <code>SyncJobActor</code> in another thread, because we don’t want
<code>ClientActor</code> to be blocked by this. <code>run_catchup</code> is simply responsible for
scheduling <code>SyncJobActor</code> to do the intensive job. Note that <code>SyncJobActor</code> is
state-less, it doesn’t have write access to the chain. It will return the changes
that need to be made as part of the response to <code>ClientActor</code>, and <code>ClientActor</code>
is responsible for applying these changes. This is to ensure only one thread
(<code>ClientActor</code>) has write access to the chain state. However, this also adds a
lot of limits, for example, <code>SyncJobActor</code> can only be scheduled to apply one
block at a time. Because <code>run_catchup</code> is only scheduled to run every 100ms, the
speed of catching up blocks is limited to 100ms per block, even when blocks
applying can be faster. Similar constraints happen to apply state parts.</p>
<h3 id="improvements"><a class="header" href="#improvements">Improvements</a></h3>
<p>There are three improvements we can make to the current code.</p>
<p>First, currently we always initiate the state downloading process at the first
block of an epoch, even when there are no new states to be downloaded for the
new epoch. This is unnecessary.</p>
<p>Second, even though <code>run_catchup</code> is scheduled to run every 100ms, the call can
be delayed if ClientActor has messages in its actix queue. A better way to do
this is to move the scheduling of <code>run_catchup</code> to <code>check_triggers</code>.</p>
<p>Third, because of how <code>run_catchup</code> interacts with <code>SyncJobActor</code>, <code>run_catchup</code>
can catch up at most one block every 100 ms. This is because we don’t want to
write to <code>ChainStore</code> in multiple threads. However, the changes that catching up
blocks make do not interfere with regular block processing and they can be
processed at the same time. However, to restructure this, we will need to
re-implement <code>ChainStore</code> to separate the parts that can be shared among threads
and the part that can’t.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="garbage-collection"><a class="header" href="#garbage-collection">Garbage Collection</a></h1>
<p>This document covers the basics of Chain garbage collection.</p>
<p>Currently we run garbage collection only in non-archival nodes,  to keep the
size of the storage under control. Therefore, we remove blocks, chunks and state
that is ‘old’ enough  - which in current configuration means 5 epochs ago.</p>
<p>We run a single ‘round’ of GC after a new block is accepted to the chain - and
in order not to delay the chain too much, we make sure that each round removes
at most 2 blocks from the chain.</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How it works:</a></h2>
<p>Imagine the following chain (with 2 forks)</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195649805-e7997192-be3a-4bf0-992d-d35b2ad80847.png" alt="" /></p>
<p>In the pictures below, let’s assume that epoch length is 5 and we keep only 3
epochs (rather than 5 that is currently set in production) - otherwise the image
becomes too large 😉.</p>
<p>If head is in the middle of the epoch, the <code>gc_stop</code> will be set to the first
block of epoch T-2, and <code>tail</code> &amp; <code>fork_tail</code> will be sitting at the last block of
epoch T-3.</p>
<p>(and no GC is happening in this round - as tail is next to <code>gc_stop</code>).</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195649850-95dee667-b88b-4ef6-b08c-77a17b8d4ae2.png" alt="" /></p>
<p>Next block was accepted on the chain (head jumped ahead), but still no GC
happening in this round:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195649879-e29cc826-dfd8-4cbc-a66d-72e42202d26a.png" alt="" /></p>
<p>Now interesting things will start happening once head ‘crosses’ over to the
next epoch.</p>
<p>First, the <code>gc_stop</code> will jump to the beginning of the next epoch.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195649928-0401b221-b6b3-4986-8931-54fbdd1adda0.png" alt="" /></p>
<p>Then we’ll start the GC of the forks: by first moving the <code>fork_tail</code> to match
the <code>gc_stop</code> and going backwards from there.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195649966-dac6a4dd-f04b-4131-887a-58efe89d456a.png" alt="" /></p>
<p>It will start removing all the blocks that don’t have a successor (a.k.a the tip
of the fork). And then it will proceed to lower height.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195650003-90e1fde7-18a6-4343-b0dd-9a10a596f136.png" alt="" /></p>
<p>Will keep going until it ‘hits’ the tail.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195650059-dd6b3d30-7dd5-4324-8e65-80f955960c47.png" alt="" /></p>
<p>In order not to do too much in one go, we’d only remove up to 2 block in each
run  (that happens after each head update).</p>
<p>Now, the forks are gone, so we can proceed with GCing of the blocks from
the canonical chain:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195650101-dc6953a7-0d55-4db8-a78b-6a52310410b2.png" alt="" /></p>
<p>Same as before, we’d remove up to 2 blocks in each run:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195650127-b30865e1-d9c1-4950-8607-67d82a185b76.png" alt="" /></p>
<p>Until we catch up to the <code>gc_stop</code>.</p>
<p>(the original drawings for this document are 
<a href="https://docs.google.com/document/d/1BiEuJqm4phwQbi-fjzHMZPzDL-94z9Dqkc3XPNnxKJM/edit?usp=sharing">here</a>)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-epoch-works"><a class="header" href="#how-epoch-works">How Epoch Works</a></h1>
<p>This short document will tell you all you need to know about Epochs in NEAR
protocol.</p>
<p>You can also find additional information about epochs in
<a href="https://nomicon.io/BlockchainLayer/EpochManager/">nomicon</a>.</p>
<h2 id="what-is-an-epoch"><a class="header" href="#what-is-an-epoch">What is an Epoch?</a></h2>
<p>Epoch is a sequence of consecutive blocks.
Within one epoch, the set of validators is fixed, and validator rotation
happens at epoch boundaries.</p>
<p>Basically almost all the changes that we do are happening at epoch boundaries:</p>
<ul>
<li>sharding changes</li>
<li>protocol version changes</li>
<li>validator changes</li>
<li>changing tracking shards</li>
<li>state sync</li>
</ul>
<h2 id="where-does-the-epoch-id-come-from"><a class="header" href="#where-does-the-epoch-id-come-from">Where does the Epoch Id come from?</a></h2>
<p><code>EpochId</code> for epoch T+2 is the last hash of the block of epoch T.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/195907256-c4b1d956-632c-4c11-aa38-17603b1fcc40.png" alt="image" /></p>
<p>Situation at genesis is interesting. We have three blocks:</p>
<p>dummy ← genesis ← first-block</p>
<h2 id="where-do-we-set-the-epoch-length"><a class="header" href="#where-do-we-set-the-epoch-length">Where do we set the epoch length?</a></h2>
<p>Epoch length is set in the genesis config. Currently in mainnet it is set to 43200 blocks:</p>
<pre><code class="language-json">  &quot;epoch_length&quot;: 43200
</code></pre>
<!-- TODO: Where is this supposed to point to? -->
<p>See http://go/mainnet-genesis for more details.</p>
<p>This means that each epoch lasts around 15 hours.</p>
<p><strong>Important</strong>: sometimes there might be ‘troubles’ on the network, that might result
in epoch lasting a little bit longer (if we cannot get enough signatures on the
last blocks of the previous epoch).</p>
<p>You can read specific details on our
<a href="https://nomicon.io/BlockchainLayer/EpochManager/Epoch">nomicon page</a>.</p>
<h2 id="how-do-we-pick-the-next-validators"><a class="header" href="#how-do-we-pick-the-next-validators">How do we pick the next validators?</a></h2>
<p><strong>TL;DR</strong>: in the last block of the epoch T, we look at the accounts that have
highest stake and we pick them to become validators in <strong>T+2</strong>.</p>
<p>We are deciding on validators for T+2 (and not T+1) as we want to make sure that
validators have enough time to prepare for block production and validation (they
have to download the state of shards etc).</p>
<p>For more info on how we pick validators please look at
<a href="https://nomicon.io/Economics/Economic#validator-selection">nomicon</a>.</p>
<h2 id="epoch-and-sharding"><a class="header" href="#epoch-and-sharding">Epoch and Sharding</a></h2>
<p>Sharding changes happen only on epoch boundary - that’s why many of the requests
(like which shard does my account belong to), require also an <code>epoch_id</code> as a
parameter.</p>
<p>As of April 2022 we don’t have dynamic sharding yet, so the whole chain is
simply using 4 shards.</p>
<h3 id="how-can-i-get-more-information-about-currentprevious-epochs"><a class="header" href="#how-can-i-get-more-information-about-currentprevious-epochs">How can I get more information about current/previous epochs?</a></h3>
<p>We don’t show much information about Epochs in Explorer. Today, you can use
<code>state_viewer</code> (if you have access to the network database).</p>
<p>At the same time, we’re working on a small debug dashboard, to show you the
basic information about past epochs - stay tuned.</p>
<h2 id="technical-details"><a class="header" href="#technical-details">Technical details</a></h2>
<h3 id="where-do-we-store-epoch-info"><a class="header" href="#where-do-we-store-epoch-info">Where do we store epoch info?</a></h3>
<p>We use a couple columns in the database to store epoch information:</p>
<ul>
<li><strong>ColEpochInfo = 11</strong> - is storing the mapping from EpochId to EpochInfo
structure that contains all the details.</li>
<li><strong>ColEpochStart = 23</strong> - has a mapping from EpochId to the first block height
of that epoch.</li>
<li><strong>ColEpochValidatorInfo = 47</strong> - contains validator statistics (blocks
produced etc.) for each epoch.</li>
</ul>
<h3 id="how-does-epoch-info-look-like"><a class="header" href="#how-does-epoch-info-look-like">How does epoch info look like?</a></h3>
<p>Here’s the example epoch info from a localnet node. As you can see below,
EpochInfo mostly contains information about who is the validator and in which
order should they produce the blocks.</p>
<pre><code>EpochInfo.V3(
  epoch_height=7,
  validators=ListContainer([
    validator_stake.V1(account_id='node0', public_key=public_key.ED25519(tuple_data=ListContainer([b'7PGseFbWxvYVgZ89K1uTJKYoKetWs7BJtbyXDzfbAcqX'])), stake=51084320187874404740382878961615),
    validator_stake.V1(account_id='node2', public_key=public_key.ED25519(tuple_data=ListContainer([b'GkDv7nSMS3xcqA45cpMvFmfV1o4fRF6zYo1JRR6mNqg5'])), stake=51084320187874404740382878961615),
    validator_stake.V1(account_id='node1', public_key=public_key.ED25519(tuple_data=ListContainer([b'6DSjZ8mvsRZDvFqFxo8tCKePG96omXW7eVYVSySmDk8e'])), stake=50569171534262067815663761517574)]),

  validator_to_index={'node0': 0, 'node1': 2, 'node2': 1},

  block_producers_settlement=ListContainer([0, 1, 2]),
  chunk_producers_settlement=ListContainer([ListContainer([0, 1, 2]), ListContainer([0, 1, 2]), ListContainer([0, 1, 2]), ListContainer([0, 1, 2]), ListContainer([0, 1, 2])]),

  hidden_validators_settlement=ListContainer([]),
  fishermen=ListContainer([]),
  fishermen_to_index={},
  stake_change={'node0': 51084320187874404740382878961615, 'node1': 50569171534262067815663761517574, 'node2': 51084320187874404740382878961615},
  validator_reward={'near': 37059603312899067633082436, 'node0': 111553789870214657675206177, 'node1': 110428850075662293347329569, 'node2': 111553789870214657675206177},
  validator_kickout={},
  minted_amount=370596033128990676330824359,
  seat_price=24438049905601740367428723111,
  protocol_version=52
)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transaction-routing"><a class="header" href="#transaction-routing">Transaction Routing</a></h1>
<p>We all know that transactions are ‘added’ to the chain - but how do they get
there?</p>
<p>Hopefully by the end of this article, the image below should make total sense.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/196204937-d6828382-16df-42bd-b59b-50eb2e6f07af.png" alt="image" /></p>
<h2 id="step-1-transaction-creatorauthor"><a class="header" href="#step-1-transaction-creatorauthor">Step 1: Transaction creator/author</a></h2>
<p>The journey starts with the author of the transaction - who creates the
transaction object (basically list of commands) - and signs them with their
private key.</p>
<p>Basically, they prepare the payload that looks like this:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SignedTransaction {
    pub transaction: Transaction,
    pub signature: Signature,
}
<span class="boring">}
</span></code></pre></pre>
<p>With such a payload, they can go ahead and send it as a JSON-RPC request to ANY
node in the system (they can choose between using ‘sync’ or ‘async’ options).</p>
<p>From now on, they’ll also be able to query the status of the transaction - by
using the hash of this object.</p>
<p><strong>Fun fact</strong>: the <code>Transaction</code> object also contains some fields to prevent
attacks: like <code>nonce</code> to prevent replay attack, and <code>block_hash</code> to limit the
validity of the transaction (it must be added within
<code>transaction_validity_period</code> (defined in genesis) blocks of <code>block_hash</code>).</p>
<h2 id="step-2-inside-the-node"><a class="header" href="#step-2-inside-the-node">Step 2: Inside the node</a></h2>
<p>Our transaction has made it to a node in the system - but most of the nodes
are not validators - which means that they cannot mutate the chain.</p>
<p>That’s why the node has to forward it to someone who can - the upcoming
validator.</p>
<p>The node, roughly, does the following steps:</p>
<ul>
<li>verify transaction’s metadata - check signatures etc. (we want to make sure
that we don’t forward bogus data)</li>
<li>forward it to the ‘upcoming’ validator - currently we pick the validators that
would be a chunk creator in +2, +3, +4 and +8 blocks (this is controlled by
<code>TX_ROUTING_HEIGHT_HORIZON</code>) - and send the transaction to all of them.</li>
</ul>
<h2 id="step-3-en-route-to-validatorproducer"><a class="header" href="#step-3-en-route-to-validatorproducer">Step 3: En-route to validator/producer</a></h2>
<p>Great, the node knows to send (forward) the transaction to the validator, but
how does the routing work? How do we know which peer is hosting a validator?</p>
<p>Each validator is regularly (every <code>config.ttl_account_id_router</code>/2 seconds == 30
minutes in production) broadcasting so called <code>AnnounceAccount</code>, which is
basically a pair of <code>(account_id, peer_id)</code>, to the whole network. This way each
node knows which <code>peer_id</code> to send the message to.</p>
<p>Then it asks the routing table about the shortest path to the peer, and sends
the <code>ForwardTx</code> message to the peer.</p>
<h2 id="step-4-chunk-producer"><a class="header" href="#step-4-chunk-producer">Step 4: Chunk producer</a></h2>
<p>When a validator receives such a forwarded transaction, it double-checks that it is
about to produce the block, and if so, it adds the transaction to the mempool
(<code>TransactionPool</code>) for this shard, where it waits to be picked up when the chunk
is produced.</p>
<p>What happens afterwards will be covered in future episodes/articles.</p>
<h2 id="additional-notes"><a class="header" href="#additional-notes">Additional notes:</a></h2>
<h3 id="transaction-being-added-multiple-times"><a class="header" href="#transaction-being-added-multiple-times">Transaction being added multiple times</a></h3>
<p>But such a approach means, that we’re forwarding the same transaction to multiple
validators (currently 4) - so can it be added multiple times?</p>
<p>No. Remember that a transaction has a concrete hash which is used as a global
identifier. If the validator sees that the transaction is present in the chain,
it removes it from its local mempool.</p>
<h3 id="can-transaction-get-lost"><a class="header" href="#can-transaction-get-lost">Can transaction get lost?</a></h3>
<p>Yes - they can and they do. Sometimes a node doesn’t have a path to a given
validator or it didn’t receive an <code>AnnouceAccount</code> for it, so it doesn’t know
where to forward the message. And if this happens to all 4 validators that we
try to send to, then the message can be silently dropped.</p>
<p>We’re working on adding some monitoring to see how often this happens.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transactions-receipts-and-chunk-surprises"><a class="header" href="#transactions-receipts-and-chunk-surprises">Transactions, Receipts and Chunk Surprises</a></h1>
<p>We finished the previous article (<a href="architecture/how/./tx_routing.html">Transaction routing</a>)
where a transaction was successfully added to the soon-to-be block
producer’s mempool.</p>
<p>In this article, we’ll cover what happens next:
How it is changed into a receipt and executed, potentially creating even
more receipts in the process.</p>
<p>First, let’s look at the ‘high level view’:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/198282472-3883dcc1-77ca-452c-b21e-0a7af1435ede.png" alt="image" /></p>
<h2 id="transaction-vs-receipt"><a class="header" href="#transaction-vs-receipt">Transaction vs Receipt</a></h2>
<p>As you can see from the image above:</p>
<p><strong>Transactions</strong> are ‘external’ communication - they are coming from the
outside.</p>
<p><strong>Receipts</strong> are used for ‘internal’ communication (cross shard, cross
contract) - they are created by the block/chunk producers.</p>
<h2 id="life-of-a-transaction"><a class="header" href="#life-of-a-transaction">Life of a Transaction</a></h2>
<p>If we ‘zoom-in', the chunk producer's work looks like this:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/198282518-cdeb375e-8f1c-4634-842c-6490020ad9c0.png" alt="image" /></p>
<h3 id="step-1-process-transaction-into-receipt"><a class="header" href="#step-1-process-transaction-into-receipt">Step 1: Process Transaction into receipt</a></h3>
<p>Once a chunk producer is ready to produce a chunk, it will fetch the
transactions from its mempool, check that they are valid, and if so, prepare to
process them into receipts.</p>
<p><strong>Note</strong>: There are additional restrictions (e.g. making sure that we take them in
the right order, that we don’t take too many, etc.) - that you can see in
nomicon’s <a href="https://nomicon.io/ChainSpec/Transactions">transaction page</a>.</p>
<p>You can see this part in explorer:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/198282561-c97235a1-93a1-4dc8-b6bc-ee9983376b2c.png" alt="image" /></p>
<h3 id="step-2-sending-receipt-to-the-proper-destination"><a class="header" href="#step-2-sending-receipt-to-the-proper-destination">Step 2: Sending receipt to the proper destination</a></h3>
<p>Once we have a receipt, we have to send it to the proper destination - by adding
it to the <code>outgoing_receipt</code> list, which will be forwarded to the chunk
producers from the next block.</p>
<p><strong>Note</strong>: There is a special case here - if the sender of the receipt is the
same as the receiver, then the receipt will be added to the <code>local_receipts</code>
queue and executed in the same block.</p>
<h3 id="step-3-when-an-incoming-receipt-arrives"><a class="header" href="#step-3-when-an-incoming-receipt-arrives">Step 3: When an incoming receipt arrives</a></h3>
<p>(<strong>Note</strong>: this happens in the ‘next’ block)</p>
<p>When a chunk producer receives an incoming receipt, it will try to execute its
actions (creating accounts, executing function calls etc).</p>
<p>Such actions might generate additional receipts (for example a contract might
want to call other contracts). All these outputs are added to the outgoing
receipt queue to be executed in the next block.</p>
<p>If the incoming receipt queue is too large to execute in the current chunk,
the producer will put the remaining receipts onto the ‘delayed’ queue.</p>
<h3 id="step-4-profit"><a class="header" href="#step-4-profit">Step 4: Profit</a></h3>
<p>When all the ‘dependant’ receipts are executed for a given transaction, we can
consider the transaction to be successful.</p>
<h3 id="advanced-but-reality-is-more-complex"><a class="header" href="#advanced-but-reality-is-more-complex">[Advanced] But reality is more complex</a></h3>
<p><strong>Caution</strong>: In the section below, some things are simplified and do not match exactly 
how the current code works.</p>
<p>Let’s quickly also check what’s inside a Chunk:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ShardChunkV2 {
    pub chunk_hash: ChunkHash,
    pub header: ShardChunkHeader,
    pub transactions: Vec&lt;SignedTransaction&gt;,
    pub receipts: Vec&lt;Receipt&gt;, // outgoing receipts from 'previous' block
}
<span class="boring">}
</span></code></pre></pre>
<p>Yes, it is a little bit confusing, that receipts here are NOT the ‘incoming’
ones for this chunk, but instead the ‘outgoing’ ones from the previous block.  Why?!?!</p>
<p>This has to do with performance.</p>
<h4 id="simple-approach"><a class="header" href="#simple-approach">Simple approach</a></h4>
<p>First, let’s imagine how the system would look like, if chunks contained things
that we’d expect:</p>
<ul>
<li>list of transactions</li>
<li>list of incoming receipts</li>
<li>list of outgoing receipts</li>
<li>hash of the final state</li>
</ul>
<p>This means, that the chunk producer has to compute all this information first,
before sending the chunk to other validators.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/198282601-383977f1-08dd-45fe-aa19-70556d585034.png" alt="image" /></p>
<p>Once the other validators receive the chunk, they can start their own processing to
verify those outgoing receipts/final state - and then do the signing. Only then,
can the next chunk producer start creating the next chunk.</p>
<p>While this approach does work, we can do it faster.</p>
<h4 id="faster-approach"><a class="header" href="#faster-approach">Faster approach</a></h4>
<p>What if the chunk didn’t contain the ‘output’ state? This changes our ‘mental’ model
a little bit, as now when we’re singing the chunk, we’d actually be
verifying the previous chunk - but that’s the topic for the next article (to be added).</p>
<!-- TODO: add future link to article about signatures and verification -->
<p>For now, imagine if the chunk only had:</p>
<ul>
<li>a list of transactions</li>
<li>a list of incoming receipts</li>
</ul>
<p>In this case, the chunk producer could send the chunk a lot earlier, and
validators (and chunk producer) could do their processing at the same time:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/198282641-1e728088-6f2b-4cb9-90c9-5eb09304e72a.png" alt="image" /></p>
<p>Now the last mystery:
Why do we have ‘outgoing’ receipts from previous chunks rather than incoming
to the current one?</p>
<p>This is yet another optimization. This way the chunk producer can send out the
chunk a little bit earlier - without having to wait for all the other shards.</p>
<p>But that’s a topic for another article (to be added).</p>
<!-- TODO: add future link to article about chunk fragments etc. -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="cross-shard-transactions---deep-dive"><a class="header" href="#cross-shard-transactions---deep-dive">Cross shard transactions - deep dive</a></h1>
<p>In this article, we'll look deeper into how cross-shard transactions are working
on the simple example of user <code>shard0</code> transfering money to user <code>shard1</code>.</p>
<p>These users are on separate shards (<code>shard0</code> is on shard 0 and <code>shard1</code> is on
shard 1).</p>
<p>Imagine, we run the following command in the command line:</p>
<pre><code class="language-console">$ NEAR_ENV=local near send shard0 shard1 500
</code></pre>
<p>What happens under the hood? How is this transaction changed into receipts and
processed by near?</p>
<h2 id="from-explorer-perspective"><a class="header" href="#from-explorer-perspective">From Explorer perspective</a></h2>
<p>If you look at a simple token transfer in explorer
(<a href="https://explorer.testnet.near.org/transactions/6cNJpNKWP55YrxrgRzc2gi6BM91fo3V6n2vVAo41MvZv">example</a>),
you can see that it is broken into three separate sections:</p>
<ul>
<li>convert transaction into receipt ( executed in block B )</li>
<li>receipt that transfers tokens ( executed in block B+1 )</li>
<li>receipt that refunds gas ( executed in block B+2 )</li>
</ul>
<p>But under the hood, the situation is a little bit more complex, as there is
actually one more receipt (that is created after converting the transaction).
Let's take a deeper look.</p>
<h2 id="internal-perspective-transactions--receipts"><a class="header" href="#internal-perspective-transactions--receipts">Internal perspective (Transactions &amp; Receipts)</a></h2>
<p>One important thing to remember is that NEAR is sharded - so in all our
designs, we have to assume that each account is on a separate shard. So that the
fact that some of them are colocated doesn't give any advantage.</p>
<h3 id="step-1---transaction"><a class="header" href="#step-1---transaction">Step 1 - Transaction</a></h3>
<p>This is the part which we receive from the user (<code>SignedTransaction</code>) - it has 3
parts:</p>
<ul>
<li>signer (account + key) who signed the transaction</li>
<li>receiver (in which account context should we execute this)</li>
<li>payload - a.k.a Actions to execute.</li>
</ul>
<p>As the first step, we want to change this transaction into a Receipt (a.k.a
'internal' message) - but before doing that, we must verify that:</p>
<ul>
<li>the message signature matches (that is - that this message was actually signed
by this key)</li>
<li>that this key is authorized to act on behalf of that account (so it is a full
access key to this account - or a valid fuction key).</li>
</ul>
<p>The last point above means, that we MUST execute this (Transaction to Receipt)
transition within the shard that the <code>signer</code> belongs to (as other shards don't
know the state that belongs to signer - so they don't know which keys it has).</p>
<p>So actually if we look inside the chunk 0 (where <code>shard0</code> belongs) at block
B, we'll see the transaction:</p>
<pre><code>Chunk: Ok(
    V2(
        ShardChunkV2 {
            chunk_hash: ChunkHash(
                8mgtzxNxPeEKfvDcNdFisVq8TdeqpCcwfPMVk219zRfV,
            ),
            header: V3(
                ShardChunkHeaderV3 {
                    inner: V2(
                        ShardChunkHeaderInnerV2 {
                            prev_block_hash: CgTJ7FFwmawjffrMNsJ5XhvoxRtQPXdrtAjrQjG91gkQ,
                            prev_state_root: 99pXnYjQbKE7bEf277urcxzG3TaN79t2NgFJXU5NQVHv,
                            outcome_root: 11111111111111111111111111111111,
                            encoded_merkle_root: 67zdyWTvN7kB61EgTqecaNgU5MzJaCiRnstynerRbmct,
                            encoded_length: 187,
                            height_created: 1676,
                            shard_id: 0,
                            gas_used: 0,
                            gas_limit: 1000000000000000,
                            balance_burnt: 0,
                            outgoing_receipts_root: 8s41rye686T2ronWmFE38ji19vgeb6uPxjYMPt8y8pSV,
                            tx_root: HyS6YfQbfBRniVSbWRnxsxEZi9FtLqHwyzNivrF6aNAM,
                            validator_proposals: [],
                        },
                    ),
                    height_included: 0,
                    signature: ed25519:uUvmvDV2cRVf1XW93wxDU8zkYqeKRmjpat4UUrHesJ81mmr27X43gFvFuoiJHWXz47czgX68eyBN38ejwL1qQTD,
                    hash: ChunkHash(
                        8mgtzxNxPeEKfvDcNdFisVq8TdeqpCcwfPMVk219zRfV,
                    ),
                },
            ),
            transactions: [
                SignedTransaction {
                    transaction: Transaction {
                        signer_id: AccountId(
                            &quot;shard0&quot;,
                        ),
                        public_key: ed25519:Ht8EqXGUnY8B8x7YvARE1LRMEpragRinqA6wy5xSyfj5,
                        nonce: 11,
                        receiver_id: AccountId(
                            &quot;shard1&quot;,
                        ),
                        block_hash: 6d5L1Vru2c4Cwzmbskm23WoUP4PKFxBHSP9AKNHbfwps,
                        actions: [
                            Transfer(
                                TransferAction {
                                    deposit: 500000000000000000000000000,
                                },
                            ),
                        ],
                    },
                    signature: ed25519:63ssFeMyS2N1khzNFyDqiwSELFaUqMFtAkRwwwUgrPbd1DU5tYKxz9YL2sg1NiSjaA71aG8xSB7aLy5VdwgpvfjR,
                    hash: 6NSJFsTTEQB4EKNKoCmvB1nLuQy4wgSKD51rfXhmgjLm,
                    size: 114,
                },
            ],
            receipts: [],
        },
    ),
)
</code></pre>
<p><strong>Side note</strong>: When we're changing the transaction into a receipt, we also use
this moment to deduct prepaid gas fees and transfered tokens from the 'signer'
account. The details on how much gas is charged etc will be in a separate
article.</p>
<!-- TODO: maybe add the link to that article here? -->
<h2 id="step-2---cross-shard-receipt"><a class="header" href="#step-2---cross-shard-receipt">Step 2 - cross shard receipt</a></h2>
<p>After transaction was changed into a receipt, this receipt must now be sent to
the shard where the <code>receiver</code> is (in our example <code>shard1</code> is on shard 1).</p>
<p>We can actually see this in the chunk of the next block:</p>
<pre><code>Chunk: Ok(
    V2(
        ShardChunkV2 {
            chunk_hash: ChunkHash(
                DoF7yoCzyBSNzB8R7anWwx6vrimYqz9ZbEmok4eqHZ3m,
            ),
            header: V3(
                ShardChunkHeaderV3 {
                    inner: V2(
                        ShardChunkHeaderInnerV2 {
                            prev_block_hash: 82dKeRnE262qeVf31DXaxHvbYEugPUDvjGGiPkjm9Rbp,
                            prev_state_root: DpsigPFeVJDenQWVueGKyTLVYkQuQjeQ6e7bzNSC7JVN,
                            outcome_root: H34BZknAfWrPCcppcHSqbXwFvAiD9gknG8Vnrzhcc4w,
                            encoded_merkle_root: 3NDvQBrcRSAsWVPWkUTTrBomwdwEpHhJ9ofEGGaWsBv9,
                            encoded_length: 149,
                            height_created: 1677,
                            shard_id: 0,
                            gas_used: 223182562500,
                            gas_limit: 1000000000000000,
                            balance_burnt: 22318256250000000000,
                            outgoing_receipts_root: Co1UNMcKnuhXaHZz8ozMnSfgBKPqyTKLoC2oBtoSeKAy,
                            tx_root: 11111111111111111111111111111111,
                            validator_proposals: [],
                        },
                    ),
                    height_included: 0,
                    signature: ed25519:32hozA7GMqNqJzscEWzYBXsTrJ9RDhW5Ly4sp7FXP1bmxoCsma8Usxry3cjvSuywzMYSD8HvGntVtJh34G2dKJpE,
                    hash: ChunkHash(
                        DoF7yoCzyBSNzB8R7anWwx6vrimYqz9ZbEmok4eqHZ3m,
                    ),
                },
            ),
            transactions: [],
            receipts: [
                Receipt {
                    predecessor_id: AccountId(
                        &quot;shard0&quot;,
                    ),
                    receiver_id: AccountId(
                        &quot;shard1&quot;,
                    ),
                    receipt_id: 3EtEcg7QSc2CYzuv67i9xyZTyxBD3Dvx6X5yf2QgH83g,
                    receipt: Action(
                        ActionReceipt {
                            signer_id: AccountId(
                                &quot;shard0&quot;,
                            ),
                            signer_public_key: ed25519:Ht8EqXGUnY8B8x7YvARE1LRMEpragRinqA6wy5xSyfj5,
                            gas_price: 103000000,
                            output_data_receivers: [],
                            input_data_ids: [],
                            actions: [
                                Transfer(
                                    TransferAction {
                                        deposit: 500000000000000000000000000,
                                    },
                                ),
                            ],
                        },
                    ),
                },
            ],
        },
    ),
)
</code></pre>
<p><strong>Side comment</strong>: notice that the receipt itself no longer has a <code>signer</code> field, but
a <code>predecessor_id</code> one.</p>
<p>Such a receipt is sent to the destination shard (we'll explain this process in a
separate article) where it can be executed.</p>
<!-- TODO: maybe add the link to that article here? -->
<h2 id="3-gas-refund"><a class="header" href="#3-gas-refund">3. Gas refund.</a></h2>
<p>When shard 1 processes the receipt above, it is then ready to refund the unused
gas to the original account (<code>shard0</code>). So it also creates the receipt, and puts
it inside the chunk. This time it is in shard 1 (as that's where it was
executed).</p>
<pre><code>Chunk: Ok(
    V2(
        ShardChunkV2 {
            chunk_hash: ChunkHash(
                8sPHYmBFp7cfnXDAKdcATFYfh9UqjpAyqJSBKAngQQxL,
            ),
            header: V3(
                ShardChunkHeaderV3 {
                    inner: V2(
                        ShardChunkHeaderInnerV2 {
                            prev_block_hash: Fj7iu26Yy9t5e9k9n1fSSjh6ZoTafWyxcL2TgHHHskjd,
                            prev_state_root: 4y6VL9BoMJg92Z9a83iqKSfVUDGyaMaVU1RNvcBmvs8V,
                            outcome_root: 7V3xRUeWgQa7D9c8s5jTq4dwdRcyTuY4BENRmbWaHiS5,
                            encoded_merkle_root: BnCE9LZgnFEjhQv1fSYpxPNw56vpcLQW8zxNmoMS8H4u,
                            encoded_length: 149,
                            height_created: 1678,
                            shard_id: 1,
                            gas_used: 223182562500,
                            gas_limit: 1000000000000000,
                            balance_burnt: 22318256250000000000,
                            outgoing_receipts_root: HYjZzyTL5JBfe1Ar4C4qPKc5E6Vbo9xnLHBKLVAqsqG2,
                            tx_root: 11111111111111111111111111111111,
                            validator_proposals: [],
                        },
                    ),
                    height_included: 0,
                    signature: ed25519:4FzcDw2ay2gAGosNpFdTyEwABJhhCwsi9g47uffi77N21EqEaamCg9p2tALbDt5fNeCXXoKxjWbHsZ1YezT2cL94,
                    hash: ChunkHash(
                        8sPHYmBFp7cfnXDAKdcATFYfh9UqjpAyqJSBKAngQQxL,
                    ),
                },
            ),
            transactions: [],
            receipts: [
                Receipt {
                    predecessor_id: AccountId(
                        &quot;system&quot;,
                    ),
                    receiver_id: AccountId(
                        &quot;shard0&quot;,
                    ),
                    receipt_id: 6eei79WLYHGfv5RTaee4kCmzFx79fKsX71vzeMjCe6rL,
                    receipt: Action(
                        ActionReceipt {
                            signer_id: AccountId(
                                &quot;shard0&quot;,
                            ),
                            signer_public_key: ed25519:Ht8EqXGUnY8B8x7YvARE1LRMEpragRinqA6wy5xSyfj5,
                            gas_price: 0,
                            output_data_receivers: [],
                            input_data_ids: [],
                            actions: [
                                Transfer(
                                    TransferAction {
                                        deposit: 669547687500000000,
                                    },
                                ),
                            ],
                        },
                    ),
                },
            ],
        },
    ),
)
</code></pre>
<p>Such gas refund receipts are a little bit special - as we'll set the
<code>predecessor_id</code> to be <code>system</code> - but the receiver is what we expect (<code>shard0</code>
account).</p>
<p><strong>Note</strong>: <code>system</code> is a special account that doesn't really belong to any shard.
As you can see in this example, the receipt was created within shard 1.</p>
<p>So putting it all together would look like this:</p>
<p><img src="https://user-images.githubusercontent.com/91919554/200617392-00b9fa0c-2f15-40ad-9802-137ca9a5a15d.png" alt="image" /></p>
<p>But wait - NEAR was saying that transfers are happening with 2 blocks - but here
I see that it took 3 blocks. What's wrong?</p>
<p>The image above is a simplification, and reality is a little bit tricker -
especially as receipts in a given chunks are actually receipts received as a
result from running a PREVIOUS chunk from this shard.</p>
<p>We'll explain it more in the next section.</p>
<h1 id="advanced-whats-actually-going-on"><a class="header" href="#advanced-whats-actually-going-on">Advanced: What's actually going on?</a></h1>
<p>As you could have read in <a href="architecture/how/./tx_receipts.html">Transactions And Receipts</a> - the
'receipts' field in the chunk is actually representing 'outgoing' receipts
from the previous block.</p>
<p>So our image should look more like this:</p>
<p><img src="https://user-images.githubusercontent.com/91919554/200621066-a5d06f2d-ff43-44ce-a52b-47dc44d6f8ab.png" alt="image" /></p>
<p>In this example, the black boxes are representing the 'processing' of the chunk,
and red arrows are cross-shard communication.</p>
<p>So when we process Shard 0 from block 1676, we read the transation, and output
the receipt - which later becomes the input for shard 1 in block 1677.</p>
<p>But you might still be wondering - so why didn't we add the Receipt (transfer)
to the list of receipts of shard0 1676?</p>
<p>That's because the shards &amp; blocks are set BEFORE we do any computation. So the
more correct image would look like this:</p>
<p><img src="https://user-images.githubusercontent.com/91919554/200621808-1ce78047-6968-4af5-9c2a-805a0f1643fc.png" alt="image" /></p>
<p>Here you can clearly see that chunk processing (black box), is happening AFTER
the chunk is set.</p>
<p>In this example, the blue arrows are showing the part where we persist the
result (receipt) into next block's chunk.</p>
<!-- TODO: maybe add the link to that article here? -->
<p>In a future article, we'll discuss how the actual cross-shard communication
works (red arrows) in the picture, and how we could guarantee that a given shard
really gets all the red arrows, before is starts processing.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="serialization-borsh-json-protobuf"><a class="header" href="#serialization-borsh-json-protobuf">Serialization: Borsh, Json, ProtoBuf</a></h1>
<p>If you spent some time looking at NEAR code, you’ll notice that we have
different methods of serializing structures into strings. So in this article,
we’ll compare these different approaches, and explain how and where we’re using
them.</p>
<h2 id="json"><a class="header" href="#json">JSON</a></h2>
<p>JSON doesn’t need much introduction. We’re using it for external APIs (jsonrpc)
and configuration. It is a very popular, flexible and human-readable format.</p>
<h2 id="proto-protocol-buffers"><a class="header" href="#proto-protocol-buffers">Proto (Protocol Buffers)</a></h2>
<p>We started using proto recently - and we plan to use it mostly for our network
communication. Protocol buffers are strongly typed - they require you to create
a .proto file, where you describe the contents of your message.</p>
<p>For example:</p>
<pre><code class="language-proto">message HandshakeFailure {
  // Reason for rejecting the Handshake.
  Reason reason = 1;

  // Data about the peer.
  PeerInfo peer_info = 2;
  // GenesisId of the NEAR chain that the peer belongs to.
  GenesisId genesis_id = 3;
}
</code></pre>
<p>Afterwards, such a proto file is fed to protoc ‘compiler’ that returns
auto-generated code (in our case Rust code) - that can be directly imported into
your library.</p>
<p>The main benefit of protocol buffers is their backwards compatibility (as long
as you adhere to the rules and don’t reuse the same field ids).</p>
<h2 id="borsh"><a class="header" href="#borsh">Borsh</a></h2>
<p>Borsh is our custom serializer (<a href="https://github.com/near/borsh">link</a>), that we use
mostly for things that have to be hashed.</p>
<p>The main feature of Borsh is that, there are no two binary representations that
deserialize into the same object.</p>
<p>You can read more on how Borsh serializes the data, by looking at the Specification
tab on <a href="https://borsh.io">borsh.io</a>.</p>
<p>The biggest pitfall/risk of Borsh, is that any change to the structure, might
cause previous data to no longer be parseable.</p>
<p>For example, inserting a new enum ‘in the middle’:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum MyCar {
  Bmw,
  Ford,
}

If we change our enum to this:

pub enum MyCar {
  Bmw,
  Citroen,
  Ford, // !! WRONG - Ford objects cannot be deserialized anymore
}
<span class="boring">}
</span></code></pre></pre>
<p>This is especially tricky if we have conditional compilation:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub enum MyCar {
  Bmw,
  #[cfg(feature = &quot;french_cars&quot;)]
  Citroen,
  Ford,
}
<span class="boring">}
</span></code></pre></pre>
<p>Is such a scenario - some of the objects created by binaries with this feature
enabled, will not be parseable by binaries without this feature.</p>
<p>Removing and adding fields to structures is also dangerous.</p>
<p>Basically - the only ‘safe’ thing that you can do with Borsh - is add a new Enum
value at the end.</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>So to recap what we’ve learned:</p>
<p>JSON - mostly used for external APIs - look for serde::Serialize/Deserialize</p>
<p>Proto - currently being developed to be used for network connections - objects
have to be specified in proto file.</p>
<p>Borsh - for things that we hash (and currently also for all the things that we
store on disk - but we might move to proto with this in the future). Look for
BorshSerialize/BorshDeserialize</p>
<h2 id="questions"><a class="header" href="#questions">Questions</a></h2>
<h3 id="why-dont-you-use-json-for-everything-"><a class="header" href="#why-dont-you-use-json-for-everything-">Why don’t you use JSON for everything ?</a></h3>
<p>While this is a tempting option, JSON has a few drawbacks:</p>
<ul>
<li>size (json is self-describing, so all the field names etc are included every time)</li>
<li>non-canonical: JSON doesn’t specify strict ordering of the fields, so we’d
have to do additional restrictions/rules on that - otherwise the same
‘conceptual’ message would end up with different hashes.</li>
</ul>
<h3 id="ok---so-how-about-proto-for-everything"><a class="header" href="#ok---so-how-about-proto-for-everything">Ok - so how about proto for everything?</a></h3>
<p>There are couple risks related with using proto for things that have to be
hashed. A Serialized protocol buffer can contain additional data (for example
fields with tag ids that you’re not using) and still successfully parse (that’s
how it achieves backward compatibility).</p>
<p>For example, in this proto:</p>
<pre><code class="language-proto">message First {
  string foo = 1;
  string bar = 2;
}
message Second {
  string foo = 1;
}
</code></pre>
<p>Every ‘First’ message will be successfully parsed as ‘Second’ message - which
could lead to some programmatic bugs.</p>
<h2 id="advanced-section---rawtrienode"><a class="header" href="#advanced-section---rawtrienode">Advanced section - RawTrieNode</a></h2>
<p>But there is one more place in the code, where we use a ‘custom’ encoding (very
similar to Borsh, but a little different): RawTrieNode.</p>
<p>If you look into store/src/trie/mod.rs, you’ll be able to find a method called <code>encode_into</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn encode_into(&amp;self, out: &amp;mut Vec&lt;u8&gt;) {
    // size in state_parts = size + 8 for RawTrieNodeWithSize + 8 for borsh vector length
    match &amp;self {
        // size &lt;= 1 + 4 + 4 + 32 + key_length + value_length
        RawTrieNode::Leaf(key, value_length, value_hash) =&gt; {
            out.push(LEAF_NODE);
            out.extend((key.len() as u32).to_le_bytes());
            out.extend(key);
            out.extend((*value_length as u32).to_le_bytes());
            out.extend(value_hash.as_bytes());
        }
     //... more code
<span class="boring">}
</span></code></pre></pre>
<p>which is responsible for generating this custom encoding.</p>
<p>If you analyse the code carefully - you’ll notice that this custom encoding
differs from Borsh in one place - in how it encodes ‘children’
(<code>[Option&lt;CryptoHash&gt;; 16]</code>) with a help of a bitmask, while Borsh would have
used a different layout.</p>
<p>Imagine a children vector like this:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>[Some(0x11), None, Some(0x12), None, None, …]
<span class="boring">}
</span></code></pre></pre>
<p>Custom encoder:</p>
<pre><code>// Number of children detetermined by the bitmask
[16 bits bitmask][32 bytes child][32 bytes child]
[3][0x11][0x12]
// Total size: 2 + 32 + 32 = 68 bytes
</code></pre>
<p>Borsh:</p>
<pre><code>[8 bits - 0 or 1][32 bytes child][8 bits 0 or 1][8 bits ]
[1][0x11][0][1][0x11][0][0]...
// Total size: 16 + 32 + 32 = 80 bytes
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="proofs"><a class="header" href="#proofs">Proofs</a></h1>
<p>“Don’t trust, but verify” - let’s talk about proofs</p>
<h2 id="was-your-transaction-included"><a class="header" href="#was-your-transaction-included">Was your transaction included?</a></h2>
<p>How do you know that your transaction was actually included in the blockchain?
Sure, you can “simply” ask the RPC node, and it might say “yes”, but is it
enough?</p>
<p>The other option would be to ask many nodes - hoping that at least one of them
would be telling the truth. But what if that is not enough?</p>
<p>The final solution would be to run your own node - this way you’d check all the
transactions yourself, and then you could be sure - but this can become a quite
expensive endeavour - especially when many shards are involved.</p>
<p>But there is actually a better solution - that doesn’t require you to trust the
single (or many) RPC nodes, and to verify yourself that your transaction was
actually executed.</p>
<h2 id="lets-talk-about-proofs-merkelization"><a class="header" href="#lets-talk-about-proofs-merkelization">Let’s talk about proofs (merkelization):</a></h2>
<p>Imagine you have 4 values that you’d like to store, in such a way, that you can
easily prove that a given value is present.</p>
<p><img src="https://user-images.githubusercontent.com/1711539/198579560-923f1f97-a8df-486d-b68e-8796c6aaa300.png" alt="image" /></p>
<p>One way to do it, would be to create a binary tree, where each node would hold a
hash:</p>
<ul>
<li>leaves would hold the hashes that represent the hash of the respective value.</li>
<li>internal nodes would hold the hash of “concatenation of hashes of their children”</li>
<li>the top node would be called a a root node (in this image it is a node n7)</li>
</ul>
<p>With such setup, you can prove that a given value exists in this tree, by
providing a “path” from the corresponding leaf to the root, and including all
the siblings.</p>
<p>For example to prove that value v[1] exists, we have to provide all the nodes
marked as green, with the information about which sibling (left or right) they
are:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/198579596-488c540b-cd24-4d38-bc07-4dc3378c53d0.png" alt="image" /></p>
<pre><code># information needed to verify that node v[1] is present in a tree
# with a given root (n7)
[(Left, n0), (Right, n6)]

# Verification
assert_eq(root, hash(hash(n0, hash(v[1])),n6)))
</code></pre>
<p>We use the technique above (called merkelization) in couple places in our
protocol, but for today’s article, I’d like to focus on receipts &amp; outcome
roots.</p>
<h2 id="merkelization-receipts-and-outcomes"><a class="header" href="#merkelization-receipts-and-outcomes">Merkelization, receipts and outcomes</a></h2>
<p>In order to prove that a given receipt belongs to a given block, we will need to
fetch some additional information.</p>
<p>As NEAR is sharded, the receipts actually belong to “Chunks” not Blocks
themselves, so the first step is to find the correct chunk and fetch its
<code>ChunkHeader</code>.</p>
<pre><code>ShardChunkHeaderV3 {
    inner: V2(
        ShardChunkHeaderInnerV2 {
            prev_block_hash: `C9WnNCbNvkQvnS7jdpaSGrqGvgM7Wwk5nQvkNC9aZFBH`,
            prev_state_root: `5uExpfRqAoZv2dpkdTxp1ZMcids1cVDCEYAQwAD58Yev`,
            outcome_root: `DBM4ZsoDE4rH5N1AvCWRXFE9WW7kDKmvcpUjmUppZVdS`,
            encoded_merkle_root: `2WavX3DLzMCnUaqfKPE17S1YhwMUntYhAUHLksevGGfM`,
            encoded_length: 425,
            height_created: 417,
            shard_id: 0,
            gas_used: 118427363779280,
            gas_limit: 1000000000000000,
            balance_burnt: 85084341232595000000000,
            outgoing_receipts_root: `4VczEwV9rryiVSmFhxALw5nCe9gSohtRpxP2rskP3m1s`,
            tx_root: `11111111111111111111111111111111`,
            validator_proposals: [],
        },
    ),
</code></pre>
<p>The field that we care about is called <code>outcome_root</code>. This value represents the
root of the binary merkle tree, that is created based on all the receipts that
were processed in this chunk.</p>
<p><strong>Note:</strong> You can notice that we also have a field here called
<code>encoded_merkle_root</code> - this is another case where we use merkelization in our
chain - this field is a root of a tree that holds hashes of all the &quot;partial
chunks&quot; into which we split the chunk to be distributed over the network.</p>
<p>So, in order to verify that a given receipt/transaction was really included, we
have to compute its hash (see details below), get the path to the root, and
voila, we can confirm that it was really included.</p>
<p>But how do we get the siblings on the path to the root? This is actually
something that RPC nodes do return in their responses.</p>
<p>If you ever looked closely at NEAR’s tx-status response, you can notice a
&quot;proof&quot; section there. For every receipt, you'd see something like this:</p>
<pre><code>proof: [
    {
        direction: 'Right',
        hash: '2wTFCh2phFfANicngrhMV7Po7nV7pr6gfjDfPJ2QVwCN'
    },
    {
        direction: 'Right',
        hash: '43ei4uFk8Big6Ce6LTQ8rotsMzh9tXZrjsrGTd6aa5o6'
    },
    {
        direction: 'Left',
        hash: '3fhptxeChNrxWWCg8woTWuzdS277u8cWC9TnVgFviu3n'
    },
    {
        direction: 'Left',
        hash: '7NTMqx5ydMkdYDFyNH9fxPNEkpgskgoW56Y8qLoVYZf7'
    }
]
</code></pre>
<p>And the values in there are exactly the siblings (plus info on which side of the
tree the sibling is), on the path to the root.</p>
<p><strong>Note:</strong> proof section doesn’t contain the root itself and also doesn’t include
the hash of the receipt.</p>
<h2 id="advanced-section-lets-look-at-the-concrete-example"><a class="header" href="#advanced-section-lets-look-at-the-concrete-example">[Advanced section]: Let’s look at the concrete example</a></h2>
<p>Imagine that we have a following receipt:</p>
<pre><code>{
  block_hash: '7FtuLHR3VSNhVTDJ8HmrzTffFWoWPAxBusipYa2UfrND',
  id: '6bdKUtGbybhYEQ2hb2BFCTDMrtPBw8YDnFpANZHGt5im',
  outcome: {
    executor_id: 'node0',
    gas_burnt: 223182562500,
    logs: [],
    metadata: { gas_profile: [], version: 1 },
    receipt_ids: [],
    status: { SuccessValue: '' },
    tokens_burnt: '0'
  },
  proof: [
    {
      direction: 'Right',
      hash: 'BWwZ4wHuzaUxdDSrhAEPjFQtDgwzb8K4zoNzfX9A3SkK'
    },
    {
      direction: 'Left',
      hash: 'Dpg4nQQwbkBZMmdNYcZiDPiihZPpsyviSTdDZgBRAn2z'
    },
    {
      direction: 'Right',
      hash: 'BruTLiGx8f71ufoMKzD4H4MbAvWGd3FLL5JoJS3XJS3c'
    }
  ]
}
</code></pre>
<p>Remember that the outcomes of the execution will be added to the NEXT block, so
let’s find the next block hash, and the proper chunk.</p>
<p>(in this example, I’ve used the <code>view-state chain</code> from neard)</p>
<pre><code>417 7FtuLHR3VSNhVTDJ8HmrzTffFWoWPAxBusipYa2UfrND |      node0 | parent: 416 C9WnNCbNvkQvnS7jdpaSGrqGvgM7Wwk5nQvkNC9aZFBH | .... 0: E6pfD84bvHmEWgEAaA8USCn2X3XUJAbFfKLmYez8TgZ8 107 Tgas |1: Ch1zr9TECSjDVaCjupNogLcNfnt6fidtevvKGCx8c9aC 104 Tgas |2: 87CmpU6y7soLJGTVHNo4XDHyUdy5aj9Qqy4V7muF5LyF   0 Tgas |3: CtaPWEvtbV4pWem9Kr7Ex3gFMtPcKL4sxDdXD4Pc7wah   0 Tgas
418 J9WQV9iRJHG1shNwGaZYLEGwCEdTtCEEDUTHjboTLLmf |      node0 | parent: 417 7FtuLHR3VSNhVTDJ8HmrzTffFWoWPAxBusipYa2UfrND | .... 0: 7APjALaoxc8ymqwHiozB5BS6mb3LjTgv4ofRkKx2hMZZ   0 Tgas |1: BoVf3mzDLLSvfvsZ2apPSAKjmqNEHz4MtPkmz9ajSUT6   0 Tgas |2: Auz4FzUCVgnM7RsQ2noXsHW8wuPPrFxZToyLaYq6froT   0 Tgas |3: 5ub8CZMQmzmZYQcJU76hDC3BsajJfryjyShxGF9rzpck   1 Tgas
</code></pre>
<p>I know that receipt should belong to Shard 3 (TODO: how? :) )  so let’s fetch
the chunk header:</p>
<pre><code class="language-console">$ neard view-state chunks --chunk-hash 5ub8CZMQmzmZYQcJU76hDC3BsajJfryjyShxGF9rzpck
</code></pre>
<pre><code>ShardChunkHeaderV3 {
  inner: V2(
      ShardChunkHeaderInnerV2 {
          prev_block_hash: `7FtuLHR3VSNhVTDJ8HmrzTffFWoWPAxBusipYa2UfrND`,
          prev_state_root: `6rtfqVEXx5STLv5v4zwLVqAfq1aRAvLGXJzZPK84CPpa`,
          outcome_root: `2sZ81kLj2cw5UHTjdTeMxmaWn2zFeyr5pFunxn6aGTNB`,
          encoded_merkle_root: `6xxoqYzsgrudgaVRsTV29KvdTstNYVUxis55KNLg6XtX`,
          encoded_length: 8,
          height_created: 418,
          shard_id: 3,
          gas_used: 1115912812500,
          gas_limit: 1000000000000000,
          balance_burnt: 0,
          outgoing_receipts_root: `8s41rye686T2ronWmFE38ji19vgeb6uPxjYMPt8y8pSV`,
          tx_root: `11111111111111111111111111111111`,
          validator_proposals: [],
      },
  ),
  height_included: 0,
  signature: ed25519:492i57ZAPggqWEjuGcHQFZTh9tAKuQadMXLW7h5CoYBdMRnfY4g7A749YNXPfm6yXnJ3UaG1ahzcSePBGm74Uvz3,
  hash: ChunkHash(
      `5ub8CZMQmzmZYQcJU76hDC3BsajJfryjyShxGF9rzpck`,
  ),
},
</code></pre>
<p>So the outcome_root is <code>2sZ81kLj2cw5UHTjdTeMxmaWn2zFeyr5pFunxn6aGTNB</code> - let’s
verify it then.</p>
<p>Our first step is to compute the hash of the receipt, which is equal to
<code>hash([receipt_id, hash(borsh(receipt_payload)])</code></p>
<pre><code># this is a borsh serialized ExecutionOutcome struct.
# computing this, we leave as an exercise for the reader :-)
receipt_payload_hash = &quot;7PeGiDjssz65GMCS2tYPHUm6jYDeBCzpuPRZPmLNKSy7&quot;

receipt_hash = base58.b58encode(hashlib.sha256(struct.pack(&quot;&lt;I&quot;, 2) + base58.b58decode(&quot;6bdKUtGbybhYEQ2hb2BFCTDMrtPBw8YDnFpANZHGt5im&quot;)  + base58.b58decode(receipt_payload_hash)).digest())
</code></pre>
<p>And then we can start reconstructing the tree:</p>
<pre><code>def combine(a, b):
   return hashlib.sha256(a+b).digest()
# one node example
# combine(receipt_hash, &quot;BWwZ4wHuzaUxdDSrhAEPjFQtDgwzb8K4zoNzfX9A3SkK&quot;)
# whole tree
combine(combine(&quot;Dpg4nQQwbkBZMmdNYcZiDPiihZPpsyviSTdDZgBRAn2z&quot;, combine(receipt_hash, &quot;BWwZ4wHuzaUxdDSrhAEPjFQtDgwzb8K4zoNzfX9A3SkK&quot;)), &quot;BruTLiGx8f71ufoMKzD4H4MbAvWGd3FLL5JoJS3XJS3c&quot;)
# result == 2sZ81kLj2cw5UHTjdTeMxmaWn2zFeyr5pFunxn6aGTNB
</code></pre>
<p>And success - our result is matching the outcome root, so it means that our
receipt was indeed processed by the blockchain.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-neard-will-work"><a class="header" href="#how-neard-will-work">How neard will work</a></h1>
<p>The documents under this chapter are talking about the future of NEAR - what we're planning on improving and how.</p>
<p>(This also means that they can get out of date quickly :-).</p>
<p>If you have comments, suggestions or want to help us designing and implementing some of these things here - please reach out on Zulip or github.</p>
<div style="break-before: page; page-break-before: always;"></div><p>This document is still a DRAFT.</p>
<p>This document covers our improvement plans for state sync and catchup.
<strong>Before reading this doc, you should take a look at <a href="architecture/next/../how/sync.html">How sync works</a></strong></p>
<p>State sync is used in two situations:</p>
<ul>
<li>when your node is behind for more than 2 epochs (and it is not an archival node) - then rather than trying to apply block by block (that can take hours) - you 'give up' and download the fresh state (a.k.a state sync) and apply blocks from there.</li>
<li>when you're a block (or chunk) producer - and in the upcoming epoch, you'll have to track a shard that you are not currently tracking.</li>
</ul>
<p>In the past (and currently) - the state sync was mostly used in the first scenario (as all block &amp; chunk producers had to track all the shards for security reasons - so they didn't actually have to do catchup at all).</p>
<p>As we progress towards phase 2 and keep increasing number of shards - the catchup part starts being a lot more critical. When we're running a network with a 100 shards, the single machine is simply not capable of tracking (a.k.a applying all transactions) of all shards - so it will have to track just a subset. And it will have to change this subset almost every epoch (as protocol rebalances the shard-to-producer assignment based on the stakes).</p>
<p>This means that we have to do some larger changes to the state sync design, as requirements start to differ a lot:</p>
<ul>
<li>catchups are high priority (the validator MUST catchup within 1 epoch - otherwise it will not be able to produce blocks for the new shards in the next epoch - and therefore it will not earn rewards).</li>
<li>a lot more catchups in progress (with lots of shards basically every validator would have to catchup at least one shard at each epoch boundary) - this leads to a lot more potential traffic on the network</li>
<li>malicious attacks &amp; incentives - the state data can be large and can cause a lot of network traffic. At the same time it is quite critical (see point above), so we'll have to make sure that the nodes are incetivised to provide the state parts upon request.</li>
<li>only a subset of peers will be available to request the state sync from (as not everyone from our peers will be tracking the shard that we're interested in).</li>
</ul>
<h2 id="things-that-were-actively-analysing"><a class="header" href="#things-that-were-actively-analysing">Things that we're actively analysing</a></h2>
<h3 id="performance-of-state-sync-on-the-receiver-side"><a class="header" href="#performance-of-state-sync-on-the-receiver-side">Performance of state sync on the receiver side</a></h3>
<p>We're looking at the performance of state sync:</p>
<ul>
<li>how long does it take to create the parts,</li>
<li>pro-actively creating the parts as soon as epoch starts</li>
<li>creating them in parallel</li>
<li>allowing user to ask for many at once</li>
<li>allowing user to provide a bitmask of parts that are required (therefore allowing the server to return only the ones that it already cached).</li>
</ul>
<h3 id="better-performance-on-the-requestor-side"><a class="header" href="#better-performance-on-the-requestor-side">Better performance on the requestor side</a></h3>
<p>Currently the parts are applied only once all them are downloaded - instead we should try to apply them in parallel - after each part is received.</p>
<p>When we receive a part, we should announce this information to our peers - so that they know that they can request it from us if they need it.</p>
<h2 id="ideas---not-actively-working-on-them-yet"><a class="header" href="#ideas---not-actively-working-on-them-yet">Ideas - not actively working on them yet</a></h2>
<h3 id="better-networking-aka-tier-3"><a class="header" href="#better-networking-aka-tier-3">Better networking (a.k.a Tier 3)</a></h3>
<p>Currently our networking code is picking the peers to connect at random (as most of them are tracking all the shards). With phase2 it will no longer be the case, so we should work on improvements of our peer-selection mechanism.</p>
<p>In general - we should make sure that we have direct connection to at least a few nodes that are tracking the same shards that we're tracking right now (or that we'll want to track in the near future).</p>
<h3 id="dedicated-nodes-optimized-towards-state-sync-responses"><a class="header" href="#dedicated-nodes-optimized-towards-state-sync-responses">Dedicated nodes optimized towards state sync responses</a></h3>
<p>The idea is to create a set of nodes that would specialize in state sync responses (similar to how we have archival nodes today).</p>
<p>The sub-idea of this, is to store such data on one of the cloud providers (AWS, GCP).</p>
<h3 id="sending-deltas-instead-of-full-state-syncs"><a class="header" href="#sending-deltas-instead-of-full-state-syncs">Sending deltas instead of full state syncs</a></h3>
<p>In case of catchup, the requesting node might have tracked that shard in the past. So we could consider just sending a delta of the state rather than the whole state.</p>
<p>While this helps with the amount of data being sent - it might require the receiver to do a lot more work (as the data that it is about to send cannot be easily cached).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="malicious-producers-in-phase-2-of-sharding"><a class="header" href="#malicious-producers-in-phase-2-of-sharding">Malicious producers in phase 2 of sharding.</a></h1>
<p>In this document, we'll compare the impact of the hypothethical malicious producer on the NEAR system (both in the current setup and how it will work when phase2 is implemented).</p>
<h2 id="current-state-phase-1"><a class="header" href="#current-state-phase-1">Current state (Phase 1)</a></h2>
<p>Let's assume that a malicious chunk producer <code>C1</code> has produced a bad chunk 
and sent it to the block producer at this height <code>B1</code>. </p>
<p>The block producer IS going to add the chunk to the block (as we don't validate 
the chunks before adding to blocks - but only when signing the block - see 
<a href="architecture/next/./../how/tx_receipts.html">Transcations and receipts - last section</a>).</p>
<p>After this block is produced, it is sent to all the validators to get the 
signatures.</p>
<p>As currently all the validators are tracking all the shards - they will quickly 
notice that the chunk is invalid, so they will not sign the block.</p>
<p>Therefore the next block producer <code>B2</code> is going to ignore <code>B1</code>'s block, and 
select block from <code>B0</code> as a parent instead.</p>
<p>So TL;DR - <strong>a bad chunk would not be added to the chain.</strong></p>
<h2 id="phase-2-and-sharding"><a class="header" href="#phase-2-and-sharding">Phase 2 and sharding</a></h2>
<p>Unfortunately things get a lot more complicated, once we scale.</p>
<p>Let's assume the same setup as above (a single chunk producer <code>C1</code> being 
malicious). But this time, we have 100 shards - each validator is tracking just 
a few (they cannot track all - as today - as they would have to run super 
powerful machines with &gt; 100 cores).</p>
<p>So in the similar scenario as above - <code>C1</code> creates a malicious chunks, and 
sends it to <code>B1</code>, which includes it in the block.</p>
<p>And here's where the complexity starts - as most of the valiators will NOT 
track the shard which <code>C1</code> was producing - so they will still sign the block.</p>
<p>The validators that do track that shard will of course (assuming that they are non-malicious) refuse the sign. But overall, they will be a small majority - so the block is going to get enough signatures and be added to the chain.</p>
<h3 id="challenges-slashing-and-rollbacks"><a class="header" href="#challenges-slashing-and-rollbacks">Challenges, Slashing and Rollbacks</a></h3>
<p>So we're in a pickle - as a malicious chunk was just added to the chain. And
that's why need to have mechanisms to automatically recover from such situations:
Challenges, Slashing and Rollbacks.</p>
<h4 id="challenge"><a class="header" href="#challenge">Challenge</a></h4>
<p>Challenge is a self-contained proof, that something went wrong in the chunk 
processing. It must contain all the inputs (with their merkle proof), the code
that was executed, and the outputs (also with merkle proofs).</p>
<p>Such a challenge allows anyone (even nodes that don't track that shard or have 
any state) to verify the validity of the challenge.</p>
<p>When anyone notices that a current chain contains a wrong transition - they 
submit such challenge to the next block producer, which can easily verify it 
and it to the next block.</p>
<p>Then the validators do the verification themselves, and if successful, they 
sign the block.</p>
<p>When such block is succesfully signed, the protocol automatically slashes 
malicious nodes (more details below) and initiates the rollback to bring the 
state back to the state before the bad chunk (so in our case, back to the block 
produced by <code>B0</code>).</p>
<h4 id="slashing"><a class="header" href="#slashing">Slashing</a></h4>
<p>Slashing is the process of taking away the part of the stake from validators
that are considered malicious.</p>
<p>In the example above, we'll definately need to slash the <code>C1</code> - and potentially also any validators that were tracking that shard and did sign the bad block.</p>
<p>Things that we'll have to figure out in the future:</p>
<ul>
<li>how much do we slash? all of the stake? some part?</li>
<li>what happens to the slashed stake? is it burned? does it go to some pool?</li>
</ul>
<h4 id="state-rollbacks"><a class="header" href="#state-rollbacks">State rollbacks</a></h4>
<p>// TODO: add</p>
<h2 id="problems-with-the-current-phase-2-design"><a class="header" href="#problems-with-the-current-phase-2-design">Problems with the current Phase 2 design</a></h2>
<h3 id="is-slashing-painful-enough"><a class="header" href="#is-slashing-painful-enough">Is slashing painful enough?</a></h3>
<p>In the example above, we'd succesfully slash the <code>C1</code> producer - but was it<br />
enough?</p>
<p>Currently (with 4 shards) you need around 20k NEAR to become a chunk producer. 
If we increase the number of shards to 100, it would drop the minimum stake to 
around 1k NEAR.</p>
<p>In such scenario, by sacrificing 1k NEAR, the malicious node can cause the 
system to rollback a couple blocks (potentially having bad impact on the bridge 
contracts etc).</p>
<p>On the other side, you could be a non-malicious chunk producer with a corrupted 
database (or a nasty bug in the code) - and the effect would be the same - the 
chunk that you produced would be marked as malicious, and you'd lose your stake 
(which will be a super-scary even for any legitimate validator).</p>
<p>So the open question is - can we do something 'smarter' in the protocol to
detect the case, where there is 'just a single' malicious (or buggy) chunk 
producer and avoid the expensive rollback?</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="trie"><a class="header" href="#trie">Trie</a></h1>
<p>We use Merkle-Patricia Trie to store blockchain state. Trie is persistent, which
means that insertion of new node actually leads to creation of new path to this
node, and thus root of Trie after insertion will also be presented by new
object.</p>
<p>Here we describe its implementation details which are closely related to
Runtime.</p>
<h2 id="main-structures"><a class="header" href="#main-structures">Main structures</a></h2>
<h3 id="trie-1"><a class="header" href="#trie-1">Trie</a></h3>
<p>Trie stores the state - accounts, contract codes, access keys, etc. Each state
item corresponds to the unique trie key. All types of trie keys are described in
the <a href="architecture/trie.html#triekey">TrieKey</a> section. You can read more about this structure on
<a href="https://en.wikipedia.org/wiki/Trie">Wikipedia</a>.</p>
<p>Trie is stored in the RocksDB, which is persistent across node restarts. Trie
communicates with database using <code>TrieStorage</code>. On the database level, data is
stored in key-value format in <code>DBCol::State</code> column. There are two kinds of
records:</p>
<ul>
<li>trie nodes, for which key is constructed from shard id and
<code>RawTrieNodeWithSize</code> hash, and value is a <code>RawTrieNodeWithSize</code> serialized by
custom algorithm;</li>
<li>values (encoded contract codes, postponed receipts, etc.), for which key is
constructed from shard id and hash of value, which maps to the encoded value.</li>
</ul>
<p>So, value can be obtained from <code>TrieKey</code> as follows:</p>
<ul>
<li>start from the hash of <code>RawTrieNodeWithSize</code> corresponding to the root;</li>
<li>descend to the needed node using nibbles from <code>TrieKey</code>;</li>
<li>extract underlying <code>RawTrieNode</code>;</li>
<li>if it is a <code>Leaf</code> or <code>Branch</code>, it should contain hash of the value;</li>
<li>get value from storage by its hash and shard id.</li>
</ul>
<p>Note that <code>Trie</code> is almost never called directly from <code>Runtime</code>, modifications
are made using <code>TrieUpdate</code>.</p>
<h3 id="trieupdate"><a class="header" href="#trieupdate">TrieUpdate</a></h3>
<p>Provides a way to access storage and record changes to commit in the future.
Update is prepared as follows:</p>
<ul>
<li>changes are made using <code>set</code> and <code>remove</code> methods, which are added to
<code>prospective</code> field,</li>
<li>call <code>commit</code> method which moves <code>prospective</code> changes to <code>committed</code>,</li>
<li>call <code>finalize</code> method which prepares <code>TrieChanges</code> and state changes based on
<code>committed</code> field.</li>
</ul>
<p>Note that <code>finalize</code>, <code>Trie::insert</code> and <code>Trie::update</code> do not update the
database storage. These functions only modify trie nodes in memory. Instead,
these functions prepares <code>TrieChanges</code> object, and <code>Trie</code> is actually updated
when <code>ShardTries::apply_insertions</code> is called, which puts new values to
<code>DBCol::State</code> part of key-value database.</p>
<h3 id="triestorage"><a class="header" href="#triestorage">TrieStorage</a></h3>
<p>Stores all <code>Trie</code> nodes and allows to get serialized nodes by <code>TrieKey</code> hash
using <code>retrieve_raw_bytes</code> method.</p>
<p>There are three implementations of <code>TrieStorage</code>:</p>
<ul>
<li><code>TrieCachingStorage</code> - caches big values ever read by <code>retrieve_raw_bytes</code>.</li>
<li><code>TrieRecordingStorage</code> - records all key-value pairs ever read by
<code>retrieve_raw_bytes</code>. Used for obtaining state parts (and challenges in the
future).</li>
<li><code>TrieMemoryPartialStorage</code> - used for validating recorded partial storage.</li>
</ul>
<p>Note that these storages use database keys, which are retrieved using hashes of
trie nodes using <code>get_key_from_shard_id_and_hash</code> method.</p>
<h3 id="shardtries"><a class="header" href="#shardtries">ShardTries</a></h3>
<p>Contains stores and caches and allows to get <code>Trie</code> object for any shard.</p>
<h2 id="primitives"><a class="header" href="#primitives">Primitives</a></h2>
<h3 id="triekey"><a class="header" href="#triekey">TrieKey</a></h3>
<p>Describes all keys which may be inserted to <code>Trie</code>:</p>
<ul>
<li><code>Account</code></li>
<li><code>ContractCode</code></li>
<li><code>AccessKey</code></li>
<li><code>ReceivedData</code></li>
<li><code>PostponedReceiptId</code></li>
<li><code>PendingDataCount</code></li>
<li><code>PostponedReceipt</code></li>
<li><code>DelayedReceiptIndices</code></li>
<li><code>DelayedReceipt</code></li>
<li><code>ContractData</code></li>
</ul>
<p>Each key is uniquely converted to <code>Vec&lt;u8&gt;</code>. Internally, each such vector is
converted to <code>NibbleSlice</code> (nibble is a half of a byte), and each its item
corresponds to one step down in <code>Trie</code>.</p>
<h3 id="triechanges"><a class="header" href="#triechanges">TrieChanges</a></h3>
<p>Stores result of updating <code>Trie</code>.</p>
<ul>
<li><code>old_root</code>: root before updating <code>Trie</code>, i.e. inserting new nodes and deleting
old ones,</li>
<li><code>new_root</code>: root after updating <code>Trie</code>,</li>
<li><code>insertions</code>, <code>deletions</code>: vectors of <code>TrieRefcountChange</code>, describing all
inserted and deleted nodes.</li>
</ul>
<h3 id="trierefcountchange"><a class="header" href="#trierefcountchange">TrieRefcountChange</a></h3>
<p>Because we remove unused nodes during garbage collection, we need to track
reference count (<code>rc</code>) for each node. Another reason is that we can dedup
values. If the same contract is deployed 1000 times, we only store one contract
binary in storage and track its count.</p>
<p>This structure is used to update <code>rc</code> in the database:</p>
<ul>
<li><code>trie_node_or_value_hash</code> - hash of the trie node or value, used for uniting
with shard id to get DB key,</li>
<li><code>trie_node_or_value</code> - serialized trie node or value,</li>
<li><code>rc</code> - change of reference count.</li>
</ul>
<p>Note that for all reference-counted records, the actual value stored in DB is
the concatenation of <code>trie_node_or_value</code> and <code>rc</code>. The reference count is
updated using custom merge operation <code>merge_refcounted_records</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><p>This document describes how our network works. At this moment, it is known to be
somewhat outdated, as we are in the process of refactoring the network protocol
somewhat significantly.</p>
<h1 id="1-overview"><a class="header" href="#1-overview">1. Overview</a></h1>
<p>Near Protocol uses its own implementation of a custom peer-to-peer network Peers
who join network are represented by nodes and connections between them by edges.</p>
<p>The purpose of this document is to describe inner workings of <code>near-network</code>
package; and to be used as reference by future engineers to understand network
code without any prior knowledge.</p>
<h1 id="2-code-structure"><a class="header" href="#2-code-structure">2. Code structure</a></h1>
<p><code>near-network</code> runs on top of <code>actor</code> framework called <code>Actix</code>
(https://actix.rs/docs/). Code structure is split between 4 actors
<code>PeerManagerActor</code>, <code>PeerActor</code>, <code>RoutingTableActor</code>, <code>EdgeValidatorActor</code></p>
<h3 id="21-edgevalidatoractor-currently-called-edgeverifieractor-in-the-code-todo-rename"><a class="header" href="#21-edgevalidatoractor-currently-called-edgeverifieractor-in-the-code-todo-rename">2.1 <code>EdgeValidatorActor</code> (currently called <code>EdgeVerifierActor</code> in the code (TODO rename))</a></h3>
<p><code>EdgeValidatorActor</code> runs on separate thread. The purpose of this <code>actor</code> is to
validate <code>edges</code>, where each <code>edge</code> represents a connection between two peers,
and it's signed with a cryptographic signature of both parties. The process of
edge validation involves verifying cryptographic signatures, which can be quite
expensive, and therefore was moved to another thread.</p>
<p>Responsibilities:</p>
<ul>
<li>validating edges by checking whenever cryptographic signatures match.</li>
</ul>
<h3 id="22-routingtableactor"><a class="header" href="#22-routingtableactor">2.2 <code>RoutingTableActor</code></a></h3>
<p><code>RoutingTableActor</code> maintain view of the <code>P2P network</code> represented by set of
nodes and edges.</p>
<p>In case a message needs to be sent between two nodes, that can be done directly
through <code>Tcp</code> connection. Otherwise, <code>RoutingTableActor</code> is responsible for ping
the best path between them.</p>
<p>Responsibilities:</p>
<ul>
<li>keep set of all edges of <code>P2P network</code> called routing table</li>
<li>connects to <code>EdgeValidatorActor</code>, and asks for edges to be validated, when
needed</li>
<li>has logic related to exchanging edges between peers</li>
</ul>
<h3 id="23-peeractor"><a class="header" href="#23-peeractor">2.3 <code>PeerActor</code></a></h3>
<p>Whenever a new connection gets accepted, an instance of <code>PeerActor</code> gets
created. Each <code>PeerActor</code> keeps a physical a <code>TCP connection</code> to exactly one
peer.</p>
<p>Responsibilities:</p>
<ul>
<li>Maintaining physical connection.</li>
<li>Reading messages from peers, decoding them, and then forwarding them to the
right place.</li>
<li>Encoding messages, sending them to peers on physical layer.</li>
<li>Routing messages between <code>PeerManagerActor</code> and other peers.</li>
</ul>
<h3 id="24-peermanageractor"><a class="header" href="#24-peermanageractor">2.4 <code>PeerManagerActor</code></a></h3>
<p><code>PeerManagerActor</code> is the main actor of <code>near-network</code> crate. It's acts as a
bridge connecting to the world outside, the other peers, and <code>ClientActor</code> and
<code>ClientViewActor</code>, which handle processing any operations on the chain.
<code>PeerManagerActor</code> maintains information about p2p network via (Routing Table
Actor), and indirectly, through <code>PeerActor</code>, connections to all some nodes on
the network. All messages going to other nodes, or coming from other nodes will
be routed through this <code>Actor</code>. <code>PeerManagerActor</code> is responsible for accepting
incoming connections from the outside world and creating <code>PeerActors</code> to manage
them.</p>
<p>Responsibilities:</p>
<ul>
<li>Accepting new connections</li>
<li>Maintaining list of <code>PeerActors</code>, creating, deleting them</li>
<li>Routing information about new edges between <code>PeerActors</code> and
<code>RoutingTableManager</code></li>
<li>Routing messages between <code>ViewClient</code>, <code>ViewClientActor</code> and <code>PeerActors</code>, and
consequently other peers.</li>
<li>Maintains <code>RouteBack</code> structure, which has information on how to send replies to messages</li>
</ul>
<h1 id="3-code-flow---initialization"><a class="header" href="#3-code-flow---initialization">3. Code flow - initialization</a></h1>
<p><code>PeerManagerActor</code> actor gets started. <code>PeerManagerActor</code> open tcp server, which
listens to incoming connection. It starts <code>RoutingTableActor</code>, which then starts
<code>EdgeValidatorActor</code>. When connection incoming connection gets accepted, it
starts a new <code>PeerActor</code> on its own thread.</p>
<h1 id="4-networkconfig"><a class="header" href="#4-networkconfig">4. NetworkConfig</a></h1>
<p><code>near-network</code> reads configuration from <code>NetworkConfig</code>, which is a part <code>client config</code>.</p>
<p>Here is a list of features read from config</p>
<ul>
<li><code>boot_nodes</code> - list of nodes to connect to on start</li>
<li><code>addr</code> - listening address</li>
<li><code>max_num_peers</code> - by default we connect up to 40 peers, current implementation
supports upto 128 nodes.</li>
</ul>
<h1 id="5-connecting-to-other-peers"><a class="header" href="#5-connecting-to-other-peers">5. Connecting to other peers.</a></h1>
<p>Each peer maintains list of known peers. They are stored in the database. If
database is empty, the list of peers, called boot nodes, will be read from
<code>boot_nodes</code> option in config. Peer to connect to is chosen at random from list
of known nodes by <code>PeerManagerActor::sample_random_peer</code> method.</p>
<h1 id="6-edges--network---in-code-representation"><a class="header" href="#6-edges--network---in-code-representation">6. Edges &amp; network - in code representation</a></h1>
<p><code>P2P network</code> is represented by list of <code>peers</code>, where each <code>peer</code> is
represented by structure <code>PeerId</code>, which is defined by <code>peer's</code> public key
<code>PublicKey</code>. And a list of edges, where each edge is represented by the
structure <code>Edge</code>.</p>
<p>Both are defined below.</p>
<h1 id="61-publickey"><a class="header" href="#61-publickey">6.1 PublicKey</a></h1>
<p>We use two types of public keys:</p>
<ul>
<li>a 256 bit <code>ED25519</code> public key</li>
<li>a 512 bit <code>Secp256K1</code> public key</li>
</ul>
<p>Public keys are defined in <code>PublicKey</code> enum, which consists of those two
variants.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct ED25519PublicKey(pub [u8; 32]);
pub struct Secp256K1PublicKey([u8; 64]);
pub enum PublicKey {
    ED25519(ED25519PublicKey),
    SECP256K1(Secp256K1PublicKey),
}
<span class="boring">}
</span></code></pre></pre>
<h1 id="62-peerid"><a class="header" href="#62-peerid">6.2 PeerId</a></h1>
<p>Each <code>peer</code> is uniquely defined by its <code>PublicKey</code>, and represented by <code>PeerId</code>
struct.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct PeerId(PublicKey);
<span class="boring">}
</span></code></pre></pre>
<h1 id="63-edge"><a class="header" href="#63-edge">6.3 Edge</a></h1>
<p>Each <code>edge</code> is represented by <code>Edge</code> structure. It contains the following</p>
<ul>
<li>pair of nodes represented by their public keys.</li>
<li><code>nonce</code> - a unique number representing state of an edge. Starting with 1. Odd
number represents an active edge. Even number represent an edge in which one
of nodes, confirmed that the edge is removed.</li>
<li>Signatures from both peers for active edges.</li>
<li>Signature from one peers in case an edge got removed.</li>
</ul>
<h1 id="64-graph-representation"><a class="header" href="#64-graph-representation">6.4 Graph representation</a></h1>
<p><code>RoutingTableActor</code> is responsible for storing and maintaining set of all edges.
They are kept in <code>edge_info</code> data structure of type <code>HashSet&lt;Edge&gt;</code>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct RoutingTableActor {
    /// Collection of edges representing P2P network.
    /// It's indexed by `Edge::key()` key and can be search through by called `get()` function
    /// with `(PeerId, PeerId)` as argument.
    pub edges_info: HashSet&lt;Edge&gt;,
    /// ...
}
<span class="boring">}
</span></code></pre></pre>
<h1 id="7-code-flow---connecting-to-a-peer---handshake"><a class="header" href="#7-code-flow---connecting-to-a-peer---handshake">7. Code flow - connecting to a peer - handshake</a></h1>
<p>When <code>PeerManagerActor</code> starts it starts to listen to a specific port.</p>
<h2 id="71---step-1---monitor_peers_trigger-runs"><a class="header" href="#71---step-1---monitor_peers_trigger-runs">7.1 - Step 1 - <code>monitor_peers_trigger</code> runs</a></h2>
<p><code>PeerManager</code> checks if we need to connect to another peer by running
<code>PeerManager::is_outbound_bootstrap_needed</code> method. If <code>true</code> we will try to
connect to new node. Let's call current node, node <code>A</code>.</p>
<h2 id="72---step-2---choosing-node-to-connect-to"><a class="header" href="#72---step-2---choosing-node-to-connect-to">7.2 - Step 2 - choosing node to connect to</a></h2>
<p>Method <code>PeerManager::sample_random_peer</code> will be called, and it returns node <code>B</code>
that we will try to connect to.</p>
<h2 id="73---step-3---outboundtcpconnect-message"><a class="header" href="#73---step-3---outboundtcpconnect-message">7.3 - Step 3 - <code>OutboundTcpConnect</code> message</a></h2>
<p><code>PeerManagerActor</code> will send to itself a message <code>OutboundTcpConnect</code> in order
to connect to node <code>B</code>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct OutboundTcpConnect {
    /// Peer information of the outbound connection
    pub target_peer_info: PeerInfo,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="74---step-4---outboundtcpconnect-message"><a class="header" href="#74---step-4---outboundtcpconnect-message">7.4 - Step 4 - <code>OutboundTcpConnect</code> message</a></h2>
<p>On receiving the message <code>handle_msg_outbound_tcp_connect</code> method will be
called, which calls <code>TcpStream::connect</code> to create new connection.</p>
<h2 id="75---step-5---connection-gets-established"><a class="header" href="#75---step-5---connection-gets-established">7.5 - Step 5 - Connection gets established</a></h2>
<p>Once connection with outgoing peer gets established. <code>try_connect_peer</code> method
will be called. And then new <code>PeerActor</code> will be created and started. Once
<code>PeerActor</code> starts it will send <code>Handshake</code> message to outgoing node <code>B</code> over
tcp connection.</p>
<p>This message contains <code>protocol_version</code>, node's <code>A</code> metadata, as well as all
information necessary to create <code>Edge</code>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Handshake {
    /// Current protocol version.
    pub(crate) protocol_version: u32,
    /// Oldest supported protocol version.
    pub(crate) oldest_supported_version: u32,
    /// Sender's peer id.
    pub(crate) sender_peer_id: PeerId,
    /// Receiver's peer id.
    pub(crate) target_peer_id: PeerId,
    /// Sender's listening addr.
    pub(crate) sender_listen_port: Option&lt;u16&gt;,
    /// Peer's chain information.
    pub(crate) sender_chain_info: PeerChainInfoV2,
    /// Represents new `edge`. Contains only `none` and `Signature` from the sender.
    pub(crate) partial_edge_info: PartialEdgeInfo,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="76---step-6---handshake-arrives-at-node-b"><a class="header" href="#76---step-6---handshake-arrives-at-node-b">7.6 - Step 6 - <code>Handshake</code> arrives at node <code>B</code></a></h2>
<p>Node <code>B</code> receives <code>Handshake</code> message. Then it performs various validation
checks. That includes:</p>
<ul>
<li>Check signature of edge from the other peer.</li>
<li>Whenever, <code>nonce</code> is the edge send matches.</li>
<li>Check whenever the protocol is above the minimum
<code>OLDEST_BACKWARD_COMPATIBLE_PROTOCOL_VERSION</code></li>
<li>Other node <code>view of chain</code> state</li>
</ul>
<p>If everything is successful, <code>PeerActor</code> will send <code>RegisterPeer</code> message to
<code>PeerManagerActor</code>. This message contains everything needed to add <code>PeerActor</code>
to list of active connections in <code>PeerManagerActor</code>.</p>
<p>Otherwise, <code>PeerActor</code> will be stopped immediately or after some timeout.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct RegisterPeer {
    pub(crate) actor: Addr&lt;PeerActor&gt;,
    pub(crate) peer_info: PeerInfo,
    pub(crate) peer_type: PeerType,
    pub(crate) chain_info: PeerChainInfoV2,
    // Edge information from this node.
    // If this is None it implies we are outbound connection, so we need to create our
    // EdgeInfo part and send it to the other peer.
    pub(crate) this_edge_info: Option&lt;EdgeInfo&gt;,
    // Edge information from other node.
    pub(crate) other_edge_info: EdgeInfo,
    // Protocol version of new peer. May be higher than ours.
    pub(crate) peer_protocol_version: ProtocolVersion,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="77---step-7---peermanageractor-receives-registerpeer-message---node-b"><a class="header" href="#77---step-7---peermanageractor-receives-registerpeer-message---node-b">7.7 - Step 7 - <code>PeerManagerActor</code> receives <code>RegisterPeer</code> message - node <code>B</code></a></h2>
<p>In <code>handle_msg_consolidate</code> method <code>RegisterPeer</code> message will be validated. If
successful <code>register_peer</code> method will be called, which adds <code>PeerActor</code> to list
of connected peers.</p>
<p>Each connected peer is represented in <code>PeerActorManager</code> in <code>ActivePeer</code> data
structure.</p>
<p>TODO: Rename <code>ActivePeer</code> to <code>RegisterPeer</code>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Contains information relevant to an active peer.
struct ActivePeer { // will be renamed to `ConnectedPeer` see #5428
    addr: Addr&lt;PeerActor&gt;,
    full_peer_info: FullPeerInfo,
    /// Number of bytes we've received from the peer.
    received_bytes_per_sec: u64,
    /// Number of bytes we've sent to the peer.
    sent_bytes_per_sec: u64,
    /// Last time requested peers.
    last_time_peer_requested: Instant,
    /// Last time we received a message from this peer.
    last_time_received_message: Instant,
    /// Time where the connection was established.
    connection_established_time: Instant,
    /// Who started connection. Inbound (other) or Outbound (us).
    peer_type: PeerType,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="78---step-8---exchange-routing-table-part-1---node-b"><a class="header" href="#78---step-8---exchange-routing-table-part-1---node-b">7.8 - Step 8 - Exchange routing table part 1 - node <code>B</code></a></h2>
<p>At the end of <code>register_peer</code> method node <code>B</code> will performance
<code>RoutingTableSync</code> sync. Sending list of known <code>edges</code> representing full graph,
and list of known <code>AnnounceAccount</code>. Those will be covered later, in their
dedicated sections see sections TODO1, TODO2.</p>
<pre><code class="language-rust  ignore">message: PeerMessage::RoutingTableSync(SyncData::edge(new_edge)),
</code></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Contains metadata used for routing messages to particular `PeerId` or `AccountId`.
pub struct RoutingTableSync { // also known as `SyncData` (#5489)
    /// List of known edges from `RoutingTableActor::edges_info`.
    pub(crate) edges: Vec&lt;Edge&gt;,
    /// List of known `account_id` to `PeerId` mappings.
    /// Useful for `send_message_to_account` method, to route message to particular account.
    pub(crate) accounts: Vec&lt;AnnounceAccount&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="79---step-9----exchange-routing-table-part-2---node-a"><a class="header" href="#79---step-9----exchange-routing-table-part-2---node-a">7.9 - Step 9 -  Exchange routing table part 2 - node <code>A</code></a></h2>
<p>Upon receiving <code>RoutingTableSync</code> message. Node <code>A</code> will reply with own
<code>RoutingTableSync</code> message.</p>
<h2 id="710---step-10----exchange-routing-table-part-2---node-b"><a class="header" href="#710---step-10----exchange-routing-table-part-2---node-b">7.10 - Step 10 -  Exchange routing table part 2 - node <code>B</code></a></h2>
<p>Node <code>B</code> will get the message from <code>A</code> and update it's routing table.</p>
<h1 id="8-adding-new-edges-to-routing-tables"><a class="header" href="#8-adding-new-edges-to-routing-tables">8. Adding new edges to routing tables</a></h1>
<p>This section covers the process of adding new edges, received from another
nodes, to the routing table. It consists of several steps covered below.</p>
<h2 id="81-step-1"><a class="header" href="#81-step-1">8.1 Step 1</a></h2>
<p><code>PeerManagerActor</code> receives <code>RoutingTableSync</code> message containing list of new
<code>edges</code> to add. <code>RoutingTableSync</code> contains list of edges of the P2P network.
This message is then forwarded to <code>RoutingTableActor</code>.</p>
<h2 id="82-step-2"><a class="header" href="#82-step-2">8.2 Step 2</a></h2>
<p><code>PeerManagerActor</code> forwards those edges to <code>RoutingTableActor</code> inside of
<code>ValidateEdgeList</code> struct.</p>
<p><code>ValidateEdgeList</code> contains:</p>
<ul>
<li>list of edges to verify</li>
<li>peer who send us the edges</li>
</ul>
<h2 id="83-step-3"><a class="header" href="#83-step-3">8.3 Step 3</a></h2>
<p><code>RoutingTableActor</code> gets the <code>ValidateEdgeList</code> message. Filters out <code>edges</code>
that have already been verified, those that are already in
<code>RoutingTableActor::edges_info</code>.</p>
<p>Then, it updates <code>edge_verifier_requests_in_progress</code> to mark that edge
verifications are in progress, and edges shouldn't be pruned from Routing Table
(see section TODO).</p>
<p>Then, after removing already validated edges, the modified message is forwarded
to <code>EdgeValidatorActor</code>.</p>
<h2 id="84-step-4"><a class="header" href="#84-step-4">8.4 Step 4</a></h2>
<p><code>EdgeValidatorActor</code> goes through list of all edges. It checks whether all edges
are valid (their cryptographic signatures match, etc.).</p>
<p>If any edge is not valid peer will be banned.</p>
<p>Edges that are validated are written to a concurrent queue
<code>ValidateEdgeList::sender</code>. This queue is used to transfer edges from
<code>EdgeValidatorActor</code>, back to <code>PeerManagerActor</code>.</p>
<h2 id="85-step-5"><a class="header" href="#85-step-5">8.5 Step 5</a></h2>
<p><code>broadcast_validated_edges_trigger</code> runs, and gets validated edges from
<code>EdgeVerifierActor</code>.</p>
<p>Every new edge will be broadcast to all connected peers.</p>
<p>And then, all validated edges received from <code>EdgeVerifierActor</code> will be sent
again to <code>RoutingTableActor</code> inside <code>AddVerifiedEdges</code>.</p>
<h2 id="85-step-6"><a class="header" href="#85-step-6">8.5 Step 6</a></h2>
<p>When <code>RoutingTableActor</code> receives <code>RoutingTableMessages::AddVerifiedEdges</code>, the
method<code>add_verified_edges_to_routing_table</code> will be called. It will add edges to
<code>RoutingTableActor::edges_info</code> struct, and mark routing table, that it needs
recalculation see <code>RoutingTableActor::needs_routing_table_recalculation</code>.</p>
<h1 id="9-routing-table-computation"><a class="header" href="#9-routing-table-computation">9 Routing table computation</a></h1>
<p>Routing table computation does a few things:</p>
<ul>
<li>For each peer <code>B</code>, calculates set of peers <code>|C_b|</code>, such that each peer is on
the shortest path to <code>B</code>.</li>
<li>Removing unreachable edges from memory and storing them to disk.</li>
<li>The distance is calculated as the minimum number of nodes on the path from
given node <code>A</code>, to each other node on the network. That is, <code>A</code> has a distance
of <code>0</code> to itself. It's neighbors will have a distance of <code>1</code>. The neighbors of
theirs neighbors will have a distance of <code>2</code>, etc.</li>
</ul>
<h2 id="91-step-1"><a class="header" href="#91-step-1">9.1 Step 1</a></h2>
<p><code>PeerManagerActor</code> runs a <code>update_routing_table_trigger</code> every
<code>UPDATE_ROUTING_TABLE_INTERVAL</code> seconds.</p>
<p><code>RoutingTableMessages::RoutingTableUpdate</code> message is sent to
<code>RoutingTableActor</code> to request routing table re-computation.</p>
<h2 id="92-step-2"><a class="header" href="#92-step-2">9.2 Step 2</a></h2>
<p><code>RoutingTableActor</code> receives the message, and then</p>
<ul>
<li>calls <code>recalculate_routing_table</code> method, which computes
<code>RoutingTableActor::peer_forwarding: HashMap&lt;PeerId, Vec&lt;PeerId&gt;&gt;</code>. For each
<code>PeerId</code> on the network, gives list of connected peers, which are on the
shortest path to the destination. It marks reachable peers in
<code>peer_last_time_reachable</code> struct.</li>
<li>calls <code>prune_edges</code> which removes from memory all edges, that were not
reachable for at least 1 hour, based on <code>peer_last_time_reachable</code> data
structure. Those edges are then stored to disk.</li>
</ul>
<h2 id="93-step-3"><a class="header" href="#93-step-3">9.3 Step 3</a></h2>
<p><code>RoutingTableActor</code> sends <code>RoutingTableUpdateResponse</code> message back to
<code>PeerManagerActor</code>.</p>
<p><code>PeerManagerActor</code> keep local copy of <code>edges_info</code>, called <code>local_edges_info</code>
containing only edges adjacent to current node.</p>
<ul>
<li><code>RoutingTableUpdateResponse</code> contains list of local edges, which
<code>PeerManagerActor</code> should remove.</li>
<li><code>peer_forwarding</code> which represent on how to route messages in the P2P network</li>
<li><code>peers_to_ban</code> - list of peers to ban for sending us edges, which failed
validation in <code>EdgeVerifierActor</code>.</li>
</ul>
<h2 id="94-step-4"><a class="header" href="#94-step-4">9.4 Step 4</a></h2>
<p><code>PeerManagerActor</code> received <code>RoutingTableUpdateResponse</code> and then:</p>
<ul>
<li>updates local copy of<code>peer_forwarding</code>, used for routing messages.</li>
<li>removes <code>local_edges_to_remove</code> from <code>local_edges_info</code>.</li>
<li>bans peers, who sent us invalid edges.</li>
</ul>
<h1 id="10-message-transportation-layers"><a class="header" href="#10-message-transportation-layers">10. Message transportation layers.</a></h1>
<p>This section describes different protocols of sending messages currently used in
<code>Near</code></p>
<h2 id="101-messages-between-actors"><a class="header" href="#101-messages-between-actors">10.1 Messages between Actors.</a></h2>
<p><code>Near</code> is build on <code>Actix</code>'s <code>actor</code> framework.
(https://actix.rs/book/actix/sec-2-actor.html) Usually each actor runs on its
own dedicated thread. Some, like <code>PeerActor</code> have one thread per each instance.
Only messages implementing <code>actix::Message</code>, can be sent using between threads.
Each actor has its own queue; Processing of messages happens asynchronously.</p>
<p>We should not leak implementation details into the spec.</p>
<p>Actix messages can be found by looking for <code>impl actix::Message</code>.</p>
<h2 id="102-messages-sent-through-tcp"><a class="header" href="#102-messages-sent-through-tcp">10.2 Messages sent through TCP</a></h2>
<p>Near is using <code>borsh</code> serialization to exchange messages between nodes (See
https://borsh.io/). We should be careful when making changes to them. We have to
maintain backward compatibility. Only messages implementing <code>BorshSerialize</code>,
<code>BorshDeserialize</code> can be sent. We also use <code>borsh</code> for database storage.</p>
<h2 id="103-messages-sentreceived-through-chainjsonrpc"><a class="header" href="#103-messages-sentreceived-through-chainjsonrpc">10.3 Messages sent/received through <code>chain/jsonrpc</code></a></h2>
<p>Near runs a <code>json REST server</code>. (See <code>actix_web::HttpServer</code>). All messages sent
and received must implement <code>serde::Serialize</code> and <code>serde::Deserialize</code>.</p>
<h1 id="11-code-flow---routing-a-message"><a class="header" href="#11-code-flow---routing-a-message">11. Code flow - routing a message</a></h1>
<p>This is the example of the message that is being sent between nodes
(<code>RawRoutedMessage</code>)
(https://github.com/near/nearcore/blob/fa8749dc60fe0de8e94c3046571731c622326e9f/chain/network-primitives/src/types.rs#L362)</p>
<p>Each of these methods have a <code>target</code> - that is either the account_id or peer_id
or hash (which seems to be used only for route back...). If target is the
account - it will be converted using <code>routing_table.account_owner</code> to the peer.</p>
<p>Upon receiving the message, the <code>PeerManagerActor</code> will sign it
(https://github.com/near/nearcore/blob/master/chain/network/src/peer_manager.rs#L1285)
And convert into RoutedMessage (which also have things like TTL etc.).</p>
<p>Then it will use the <code>routing_table</code>, to find the route to the target peer (add
<code>route_back</code> if needed) and then send the message over the network as
<code>PeerMessage::Routed</code>. Details about routing table computations are covered in
section 8.</p>
<p>When Peer receives this message (as <code>PeerMessage::Routed</code>), it will pass it to
PeerManager (as <code>RoutedMessageFrom</code>), which would then check if the message is
for the current <code>PeerActor</code>. (if yes, it would pass it for the client) and if
not - it would pass it along the network.</p>
<p>All these messages are handled by <code>receive_client_message</code> in Peer.
(<code>NetworkClientMessags</code>) - and transferred to <code>ClientActor</code> in
(<code>chain/client/src/client_actor.rs</code>)</p>
<p><code>NetworkRequests</code> to <code>PeerManager</code> actor trigger the <code>RawRoutedMessage</code> for
messages that are meant to be sent to another <code>peer</code>.</p>
<p><code>lib.rs</code> (<code>ShardsManager</code>) has a <code>network_adapter</code> - coming from client’s
network_adapter that comes from <code>ClientActor</code> that comes from start_client call
that comes from <code>start_with_config</code> (that crates <code>PeerManagerActor</code> - that is
passed as target to <code>network_recipent</code>).</p>
<h1 id="12-database"><a class="header" href="#12-database">12. Database</a></h1>
<h3 id="121-storage-of-deleted-edges"><a class="header" href="#121-storage-of-deleted-edges">12.1 Storage of deleted edges</a></h3>
<p>Everytime a group of peers becomes unreachable at the same time; We store edges
belonging to them in components. We remove all of those edges from memory, and
save them to database, If any of them were to be reachable again, we would
re-add them. This is useful in case there is a network split, to recover edges
if needed.</p>
<p>Each component is assigned a unique <code>nonce</code>, where first one is assigned nonce
0. Each new component, a get assigned a consecutive integer.</p>
<p>To store components, we have the following columns in the DB.</p>
<ul>
<li><code>DBCol::LastComponentNonce</code> Stores <code>component_nonce: u64</code>, which is the last
used nonce.</li>
<li><code>DBCol::ComponentEdges</code> Mapping from <code>component_nonce</code> to list of edges.</li>
<li><code>DBCol::PeerComponent</code> Mapping from <code>peer_id</code> to last component <code>nonce</code> it belongs to.</li>
</ul>
<h3 id="122-storage-of-account_id-to-peer_id-mapping"><a class="header" href="#122-storage-of-account_id-to-peer_id-mapping">12.2 Storage of <code>account_id</code> to <code>peer_id</code> mapping</a></h3>
<p><code>ColAccountAnouncements</code> -&gt; Stores a mapping from <code>account_id</code> to tuple
(<code>account_id</code>, <code>peer_id</code>, <code>epoch_id</code>, <code>signature</code>).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gas-cost-parameters"><a class="header" href="#gas-cost-parameters">Gas Cost Parameters</a></h1>
<p>Gas in NEAR Protocol solves two problems.</p>
<ol>
<li>To avoid spam, validator nodes only perform work if a user's tokens are
burned. Tokens are automatically converted to gas using the current gas
price.</li>
<li>To synchronize shards, they must all produce chunks following a strict
schedule of 1 second execution time. Gas is used to measure how heavy the
workload of a transaction is, so that the number of transactions that fit in
a block can be deterministically computed by all nodes.</li>
</ol>
<p>In other words, each transaction costs a fixed amount of gas. This gas cost
determines how much a user has to pay and how much time nearcore has to execute
the transaction.</p>
<p>What happens if nearcore executes a transaction too slowly? Chunk production for
the shard gets delayed, which delays block production for the entire blockchain,
reducing latency and throughput for everybody. If the chunk is really late, the
block producer will decide to not include the chunk at all and inserts an empty
chunk. The chunk may be included in the next block.</p>
<p>By now, you probably wonder how we can know the time it takes to execute a
transaction, given that validators use hardware of their choice. Getting these
timings right is indeed a difficult problem. Or flipping the problem, assuming
the timings are already known, then we must implement nearcore such that it
guarantees to operate within the given time constraints. How we tackle this is
the topic of this chapter.</p>
<p>If you want to learn more about Gas from a user perspective, 
<a href="https://docs.near.org/concepts/basics/transactions/gas">Gas basic concepts</a>,
<a href="https://docs.near.org/concepts/basics/transactions/gas-advanced">Gas advanced concepts</a>,
and <a href="https://nomicon.io/RuntimeSpec/Fees/">the runtime fee specification</a> are
good places to dig deeper.</p>
<h2 id="hardware-and-timing-assumptions"><a class="header" href="#hardware-and-timing-assumptions">Hardware and Timing Assumptions</a></h2>
<p>For timing to make sense at all, we must first define hardware constraints. The
official hardware requirements for a validator is published on
<a href="https://near-nodes.io/validator/hardware">near-nodes.io/validator/hardware</a>. It
may change over time but the main principle is that a moderately configured,
cloud-hosted virtual machine suffices.</p>
<p>For our gas computation, we assume the minimum required hardware. Then we define
10<sup>15</sup> gas to be executed in at most 1s. We commonly use 1 Tgas (=
10<sup>12</sup> gas) in conversation, which corresponds to 1ms execution time.</p>
<p>Obviously, this definition means that a validator running more powerful hardware
will execute the transactions faster. That is perfectly okay, as far as the
protocol is concerned we just need to make sure the chunk is available in time.
If it is ready even faster, no problem.</p>
<p>Less obviously, this means that even a minimally configured validator is often
idle. Why is that? Well, the hardware must be prepared to execute chunks that
are always full. But that is rarely the case, as the gas price increases
exponentially when chunks are full, which would cause traffic to go back
eventually.</p>
<p>Futhermore, the hardware has to be ready for transactions of all types,
including transactions chosen by a malicious actor selecting only the most
complex transactions. Those transactions can also be unbalanced in what
bottlenecks they hit. For example, a chunk can be filled with transactions that
fully utilize the CPU's floating point units. Or they could be using all the
available disk IO bandwidth.</p>
<p>Because the minimum required hardware needs to meet the timing requirements for
any of those scenarios, the typical, more balanced case is usually computed
faster than the gas rule states.</p>
<h2 id="transaction-gas-cost-model"><a class="header" href="#transaction-gas-cost-model">Transaction Gas Cost Model</a></h2>
<p>A transaction is essentially just a list of actions to be executed on the same
account. For example it could be <code>CreateAccount</code> combined with
<code>FunctionCall(&quot;hello_world&quot;)</code>.</p>
<p>The <a href="https://nomicon.io/RuntimeSpec/Actions">reference for available action</a>
shows the conclusive list of possible actions. The protocol defines fixed fees
for each of them. More details on <a href="architecture/gas/index.html#action-costs">actions fees</a> follow below.</p>
<p>Fixed fees are an important design decision. It means that a given action will
always cost the exact same amount of gas, no matter on what hardware it
executes. But the content of the action can impact the cost, for example a
<code>DeployContract</code> action's cost scales with the size of the contract code.</p>
<p>So, to be more precise, the protocol defines fixed gas cost <em>parameters</em> for
each action, together with a formula to compute the gas cost for the action. All
actions today either use a single fixed gas cost or they use a base cost and a
linear scaling parameter. With one important exception, <code>FunctionCall</code>, which
shall be discussed <a href="architecture/gas/index.html#fn-call-costs">further below</a>.</p>
<p>There is an entire section on <a href="architecture/gas/./parameter_definition.html">Parameter Definitions</a>
that explains how to find the source of truth for parameter values in the
nearcore repository, how they can be referenced in code, and what steps are
necessary to add a new parameter.</p>
<p>Let us dwell a bit more on the linear scaling factors. The fact that contract
deployment cost, which includes code compilation, scales linearly limits the
compiler to use only algorithms of linear complexity. Either that, or the
parameters must be set to match the 1ms = 1Tgas rule at the largest possible
contract size. Today, we limit ourselves to linear-time algorithms in the
compiler.</p>
<p>Likewise, an action that has no scaling parameters must only use constant time
to execute. Taking the <code>CreateAcccount</code> action as an example, with a cost of 0.1
Tgas, it has to execute within 0.1ms. Technically, the execution time depends
ever so slightly on the account name length. But there is a fairly low upper
limit on that length and it makes sense to absorb all the cost in the constant
base cost.</p>
<p>This concept of picking parameters according to algorithmic complexity is key.
If you understand this, you know how to think about gas as a nearcore developer.
This should be enough background to understand what the estimator does.</p>
<p>The <a href="architecture/gas/./estimator.html">runtime parameter estimator</a> is a separate binary within
the nearcore repository. It contains benchmarking-like code used to validate
existing parameters values against the 1ms = 1 Tgas rule. When implementing new
features, code should be added there to estimate the safe values of the new
parameters. This section is for you if you are adding new features such as a new
pre-compiled method or other host functions.</p>
<p>Next up are more details on the specific costs that occur when executing NEAR
transactions, which helps to understand existing parameters and how they are
organized.</p>
<h2 id="action-costs"><a class="header" href="#action-costs">Action Costs</a></h2>
<p>Actions are executed in two steps. First, an action is verified and inserted to
an action receipt, which is sent to the receiver of the action. The <code>send</code> fee
is paid for this. It is charged either in <code>fn process_transaction(..)</code> if the
action is part of a fresh transaction, or inside
<a href="https://github.com/near/nearcore/blob/14b8ae2c7465444c9b672a23b044c00be98f6e34/runtime/near-vm-logic/src/logic.rs">logic.rs</a>
through <code>fn pay_action_base(..)</code> if the action is generated by a function call.
The send fee is meant to cover the cost to validate an action and transmit it
over the network.</p>
<p>The second step is action execution. It is charged in <code>fn apply_action(..)</code>.
The execution cost has to cover everything required to apply the action to the
blockchain's state.</p>
<p>These two steps are done on the same shard for local receipts. Local receipts
are defined as those where the sender account is also the receiver, abbreviated
as <code>sir</code> which stands for &quot;sender is receiver&quot;.</p>
<p>For remote receipts, which is any receipt where the sender and receiver accounts
are different, we charge a different fee since sending between shards is extra
work. Notably, we charge that extra work even if the accounts are on the same
shard. In terms of gas costs, each account is conceptually its own shard. This
makes dynamic resharding possible without user-observable impact.</p>
<p>When the send step is performed, the minimum required gas to start execution of
that action is known. Thus, if the receipt has not enough gas, it can be aborted
instead of forwarding it. Here we have to introduce the concept of used gas.</p>
<p><code>gas_used</code> is different from <code>gas_burnt</code>. The former includes the gas that needs
to be reserved for the execution step whereas the latter only includes the gas
that has been burnt in the current chunk. The difference between the two is
sometimes also called prepaid gas, as this amount of gas is paid for during the
send step and it is available in the execution step for free.</p>
<p>If execution fails, the prepaid cost that has not been burned will be refunded.
But this is not the reason why it must burn on the receiver shard instead of the
sender shard. The reason is to properly compute the gas limits on the chunk that
does the execution work.</p>
<p>In conclusion, each action parameter is split into three costs, <code>send_sir</code>,
<code>send_not_sir</code>, and <code>execution</code>. Local receipts charge the first and last
parameters, remote receipts charge the second and third. They should be
estimated, defined, and charged separately. But the reality is that today almost
all actions are estimated as a whole and the parameters are split 50/50 between
send and execution cost, without discrimination on local vs remote receipts
i.e. <code>send_sir</code> cost is the same as <code>send_not_sir</code>.</p>
<p>The <a href="architecture/gas/./gas_profile.html">Gas Profile</a> section goes into more details on how gas
costs of a transaction are tracked in nearcore.</p>
<h2 id="dynamic-function-call-costs"><a class="header" href="#dynamic-function-call-costs">Dynamic Function Call Costs</a></h2>
<p><a name="fn-call-costs"></a></p>
<p>Costs that occur while executing a function call on a deployed WASM app (a.k.a.
smart contract) are charged only at the receiver. Thus, they have only one value
to define them, in contrast to action costs.</p>
<p>The most fundamental dynamic gas cost is <code>wasm_regular_op_cost</code>. It is
multiplied with the exact number of WASM operations executed. You can read about
<a href="https://nomicon.io/RuntimeSpec/Preparation#gas-instrumentation">Gas Instrumentation</a>
if you are curious how we count WASM ops.</p>
<p>Currently, all operations are charged the same, although it could be more
efficient to charge less for opcodes like <code>i32.add</code> compared to <code>f64.sqrt</code>.</p>
<p>The remaining dynamic costs are for work done during host function calls. Each
host function charges a base cost. Either the general <code>wasm_base</code> cost, or a
specific cost such as <code>wasm_utf8_decoding_base</code>, or sometimes both. New host
function calls should define a separate base cost and not charge <code>wasm_base</code>.</p>
<p>Additional host-side costs can be scaled per input byte, such as
<code>wasm_sha256_byte</code>, or costs related to moving data between host and guest, or
any other cost that is specific to the host function. Each host function must
clearly define what its costs are and how they depend on the input.</p>
<h2 id="non-gas-parameters"><a class="header" href="#non-gas-parameters">Non-gas parameters</a></h2>
<p>Not all runtime parameters are directly related to gas costs. Here is a brief
overview.</p>
<ul>
<li><strong>Gas economics config</strong>: Defines the conversion rate when purchasing gas with
NEAR tokens and how gas rewards are split.</li>
<li><strong>Storage usage config</strong>: Costs in tokens, not gas, for storing data on chain.</li>
<li><strong>Account creation config</strong>: Rules for account creation.</li>
<li><strong>Smart contract limits</strong>: Rules for WASM execution.</li>
</ul>
<p>None of the above define any gas costs directly. But there can be interplay
between those parameters and gas costs. For example, the limits on smart
contracts changes the assumptions for how slow a contract compilation could be,
hence it affects the deploy action costs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parameter-definitions"><a class="header" href="#parameter-definitions">Parameter Definitions</a></h1>
<p>Gas parameters are a subset of runtime parameters that are defined in
<a href="https://github.com/near/nearcore/blob/d0dc37bf81f7e7bde9c560403b085fae04108659/core/primitives/res/runtime_configs/parameters.txt">core/primitives/res/runtime_configs/parameters.txt</a>.
IMPORTANT: This is not the final list of parameters, it contains the base values
which can be overwritten per protocol version. For example,
<a href="https://github.com/near/nearcore/blob/d0dc37bf81f7e7bde9c560403b085fae04108659/core/primitives/res/runtime_configs/53.txt">53.txt</a>
changes several parameters
starting from version 53. To see all parameter values for a specific version at
once, check out the list of JSON snapshots generated in this directory:
<a href="https://github.com/near/nearcore/blob/d0dc37bf81f7e7bde9c560403b085fae04108659/core/primitives/src/runtime/snapshots">core/primitives/src/runtime/snapshots</a>.</p>
<h2 id="using-parameters-in-code"><a class="header" href="#using-parameters-in-code">Using Parameters in Code</a></h2>
<p>As the introduction on this page already hints at it, parameter values are
versioned. In other words, they can change if the protocol version changes. A
nearcore binary has to support multiple versions and choose the correct
parameter value at runtime.</p>
<p>To make this easy, there is
<a href="https://github.com/near/nearcore/blob/a8964d200b3938a63d389263bc39c1bcd75b1de4/core/primitives/src/runtime/config_store.rs#L43"><code>RuntimeConfigStore</code></a>.
It contains a sparse map from protocol versions to complete runtime
configurations (<code>BTreeMap&lt;ProtocolVersion, Arc&lt;RuntimeConfig&gt;&gt;</code>).
The runtime then uses <code>store.get_config(protocol_version)</code> to access a runtime
configuration for a specific version.</p>
<p>It is crucial to always use this runtime config store. Never hard-code parameter
values. Never look them up in a different way.</p>
<p>In practice, this usually translates to a <code>&amp;RuntimeConfig</code> argument for any
function that depends on parameter values. This config object implicitly defines
the protocol version. It should therefore not be cached. It should be read from
the store once per chunk and then passed down to all functions that need it.</p>
<h2 id="how-to-add-a-new-parameter"><a class="header" href="#how-to-add-a-new-parameter">How to Add a New Parameter</a></h2>
<p>First and foremost, if you are feeling lost, open a topic in our Zulip chat
(<a href="https://near.zulipchat.com/#narrow/stream/295306-pagoda.2Fcontract-runtime">pagoda/contract-runtime</a>).
We are here to help.</p>
<h3 id="principles"><a class="header" href="#principles">Principles</a></h3>
<p>Before adding anything, please review the basic principles for gas parameters.</p>
<ul>
<li>A parameter must correspond to a clearly defined workload.</li>
<li>When the workload is scalable by a factor <code>N</code> that depends on user input,
likely it will require a base parameter and a second parameter that is
multiplied by <code>N</code>. (Example: <code>N</code> = number of bytes when reading a value from
storage.)</li>
<li>Charge gas before executing the workload.</li>
<li>Parameters should be independent from specific implementation choices in
nearcore.</li>
<li>Ideally, contract developers can easily understand what the cost is simply by
reading the name in a gas profile.</li>
</ul>
<p>The section on <a href="architecture/gas/./gas_profile.html#charging-gas">Gas Profiles</a> explains how to
charge gas, please also take that into considerations when defining a new
parameter.</p>
<h3 id="necessary-code-changes"><a class="header" href="#necessary-code-changes">Necessary Code Changes</a></h3>
<p>Adding the parameter in code involves several steps.</p>
<ol>
<li>Define the parameter by adding it to the list in <code>core/primitives/res/runtime_configs/parameters.txt.</code></li>
<li>Update the Rust view of parameters by adding a variant to <code>enum Parameter</code>
in <code>core/primitives-core/src/parameter.rs</code>. In the same file, update
<code>enum FeeParameter</code> if you add an action cost or update <code>ext_costs()</code>
if you add a cost inside function calls.</li>
<li>Update <code>RuntimeConfig</code>, the configuration used to reference parameters in
code. Depending on the type of parameter, you will need to update
<code>RuntimeFeesConfig</code> (for action costs) or <code>ExtCostsConfig</code> (for gas costs).</li>
<li>Update the list used for gas profiles. This is defined by <code>enum Cost</code> in
<code>core/primitives-core/src/profile.rs</code>. You need to add a variant to either
<code>enum ActionCosts</code> or <code>enum ExtCost</code>. Please also update <code>fn index()</code> that
maps each profile entry to a unique position in serialized gas profiles.</li>
<li>The parameter should be available to use in the code section you need it. Now
is a good time to ensure <code>cargo check</code> and <code>cargo test --no-run</code> pass. Most
likely you have to update some testing code, such as
<code>ExtCostsConfig::test()</code>.</li>
<li>To merge your changes into nearcore, you will have to hide your parameter
behind a feature flag. Add the feature to the <code>Cargo.toml</code> of each crate
touched in step 3 and 4 and hide the code behind <code>#[cfg(feature = &quot;protocol_feature_MY_NEW_FEATURE&quot;)]</code>. Do not hide code in step 2 so that
non-nightly builds can still read <code>parameters.txt</code>. Also add your feature as
a dependency on <code>nightly</code> in <code>core/primitives/Cargo.toml</code> to make sure it
gets included when compiling for nightly. After that, check <code>cargo check</code> and
<code>cargo test --no-run</code> with and without <code>features=nightly</code>.</li>
</ol>
<h3 id="what-gas-value-should-the-parameter-have"><a class="header" href="#what-gas-value-should-the-parameter-have">What Gas Value Should the Parameter Have?</a></h3>
<p>For a first draft, the exact gas value used in the parameter is not crucial.
Make sure the right set of parameters exists and try to set a number that roughly
makes sense. This should be enough to enable discussions on the NEP around
feasibility and usefulness of the proposed feature. If you are not sure, a good
rule of thumb is 0.1 Tgas for each disk operation and at least 1 Tgas for each
ms of CPU time. Then round it up generously.</p>
<p>The value will have to be refined later. This is usually the last step, after
the implementation is complete and reviewed. Have a look at the section on
<a href="architecture/gas/./estimator.html">estimating gas parameters</a> of the book.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gas-profile"><a class="header" href="#gas-profile">Gas Profile</a></h1>
<p>The transaction runtime charges gas in various places around the code. The
charges end up as summaries inside an <code>ActionResult</code>. More specifically, the
<code>gas_burnt</code> and <code>gas_used</code> counters track the total gas required and the
<code>profile</code> field keeps track of what the gas was spent on.</p>
<h2 id="charging-gas"><a class="header" href="#charging-gas">Charging Gas</a></h2>
<p>Generally speaking, gas is charged right before the computation that it pays for
is executed. It has to be before to avoid cheap resource exhaustion attacks.
Imagine the user has only 1 gas unit left but we start executing an expensive
step, we would waste a significant duration of compute on all validators without
anyone paying for it.</p>
<p>When charging gas for an action, the <code>ActionResult</code> can be updated directly. But
when charging WASM costs, it would be too slow to do a context switch each time,
Therefore, a fast gas counter exists that can be updated from within the VM.
(See
<a href="https://github.com/near/nearcore/blob/06711f8460f946b8d2042aa1df6abe03c5184767/runtime/near-vm-logic/src/gas_counter.rs">gas_counter.rs</a>)
At the end of a function call execution, the gas counter is read by the host and
merged into the <code>ActionResult</code>.</p>
<!-- TODO: Difference between `Cost` in profiles and `Parameter` -->
<!-- TODO: Transaction profiles vs Receipt Profiles -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime-parameter-estimator"><a class="header" href="#runtime-parameter-estimator">Runtime Parameter Estimator</a></h1>
<p>The runtime parameter estimator is a byzantine benchmarking suite. Byzantine
benchmarking is not really commonly used term but I feel it describes it quite
well. It measures the performance assuming that up to a third of validators and
all users collude to make the system as slow as possible.</p>
<p>This benchmarking suite is used to check that the gas parameters defined in the
protocol are correct. Correct in this context means, a chunk filled with 1 Pgas
will take at most 1 second to be applied. Or more generally, per 1 Tgas of
execution, we spend no more than 1ms wall-clock time. </p>
<p>For now, nearcore timing is the only one that matters. Things will become more
complicated once there are multiple client implementations. But knowing that
nearcore can serve requests fast enough proofs that it is possible to be at
least as fast. However, we should be careful to not couple costs too tightly
with the specific implementation of nearcore to allow for innovation in new
clients.</p>
<p>The estimator code is part of the nearcore repository in the directory
<a href="https://github.com/near/nearcore/tree/master/runtime/runtime-params-estimator">runtime/runtime-params-estimator</a>.</p>
<p>For a practical guide on how to run the estimator, please take a look at
<a href="architecture/gas/../../practices/workflows/gas_estimations.html">Running the Estimator</a> in the
workflows chapter.</p>
<h2 id="code-structure"><a class="header" href="#code-structure">Code Structure</a></h2>
<p>The estimator contains a binary and a library module. The
<a href="https://github.com/near/nearcore/blob/e40863c9ba61a0de140c869583b2113358605771/runtime/runtime-params-estimator/src/main.rs">main.rs</a>
contains the CLI arguments parsing code and logic to fill the test database.</p>
<p>The interesting code lives in
<a href="https://github.com/near/nearcore/blob/e40863c9ba61a0de140c869583b2113358605771/runtime/runtime-params-estimator/src/lib.rs">lib.rs</a>
and its submodules. The comments on the top of that file provide a
high-level overview of how estimations work. More details on specific
estimations are available as comments on the enum variants of <code>Cost</code> in
<a href="https://github.com/near/nearcore/blob/e40863c9ba61a0de140c869583b2113358605771/runtime/runtime-params-estimator/src/cost.rs#L9">costs.rs</a>.</p>
<p>If you roughly understand the three files above, you already have a great
overview around the estimator.
<a href="https://github.com/near/nearcore/blob/e40863c9ba61a0de140c869583b2113358605771/runtime/runtime-params-estimator/src/estimator_context.rs">estimator_context.rs</a>
is another central file. A full estimation run creates a single
<code>EstimatorContext</code>. Each individual estimation will use it to spawn a new
<code>Testbed</code> with a fresh database that contains the same data as setup in the
estimator context.</p>
<p>Most estimations fill blocks with transactions to be executed and hands them to
<code>Testbed::measure_blocks</code>. To allow for easy repetitions, the block is usually
filled by an instance of the
<a href="https://github.com/near/nearcore/blob/e40863c9ba61a0de140c869583b2113358605771/runtime/runtime-params-estimator/src/transaction_builder.rs"><code>TransactionBuilder</code></a>,
which can be retrieved from a testbed.</p>
<p>But even filling blocks with transactions becomes repetitive since many
parameters are estimated similarly.
<a href="https://github.com/near/nearcore/blob/master/runtime/runtime-params-estimator/src/utils.rs">utils.rs</a>
has a collection of helpful functions that let you write estimations very
quickly.</p>
<h2 id="estimation-metrics"><a class="header" href="#estimation-metrics">Estimation Metrics</a></h2>
<p>Estimation code is generally not concerned with the metric used to estimate gas.
We use <code>let clock = GasCost::measure();</code> and <code>clock.elapsed()</code> to measure the
cost in whatever metric has been specified in the CLI argument <code>--metric</code>. But
when you run estimations and especially when you want to interpret the results,
you want to understand the metric used. Available metrics are <code>time</code> and
<code>icount</code>.</p>
<p>Starting with <code>time</code>, this is a simple wall-clock time measurement. At the end
of the day, this is what counts in a validator setup. But unfortunately, this
metrics is very dependent on the specific hardware and what else is running on
that hardware right now. Dynamic voltage and frequency scaling (DVFS) also plays
a role here. To a certain degree, all these factors can be controlled. But it
requires full control over a system (often not the case when running on
cloud-hosted VMs) and manual labour to set it up.</p>
<p>The other supported metric <code>icount</code> is much more stable. It uses
<a href="https://www.qemu.org/">qemu</a> to emulate an x86 CPU. We then insert a custom
<a href="https://qemu.readthedocs.io/en/latest/devel/tcg-plugins.html">TCG plugin</a>
(<a href="https://github.com/near/nearcore/blob/08c4a1bd4b16847eb1c2fccee36bf16f6efb71fd/runtime/runtime-params-estimator/emu-cost/counter_plugin/counter.c">counter.c</a>)
that counts the number of executed x86 instructions. It also intercepts system
calls and counts the number of bytes seen in <code>sys_read</code>, <code>sys_write</code> and their
variations. This gives an approximation for IO bytes, as seen on the interface
between operating system and nearcore. To convert to gas, we use  three
constants to multiply with instruction count, read bytes, and write bytes.</p>
<p>We run qemu inside a Docker container, to make sure the qemu and qemu plugin
versions match with system libraries. Make sure to add <code>--docker</code> when running
with <code>--metric icount</code>.</p>
<p>The great thing about <code>icount</code> is how you can run it on different machines and
it will always return the same result. It is not 100% deterministic but very
close, so it can usually detect code changes that degrade performance in major
ways.</p>
<p>The problem with <code>icount</code> is how unrepresentative it is for real-life
performance. First, <code>x86</code> instructions are not all equally complex. Second, how
many of them are executed per cycle depends on instruction level pipelining,
branch prediction, memory prefetching, and more CPU features like that which are
just not captured by an emulator like qemu. Third, the time it takes to serve
bytes in system calls depends less on the sum of all bytes and more on data
locality and how it can be cached in the OS page cache. But regardless of all
these inaccuracies, it can still be useful to compare different implementations
both measured using <code>icount</code>.</p>
<h2 id="from-estimations-to-parameter-values"><a class="header" href="#from-estimations-to-parameter-values">From Estimations to Parameter Values</a></h2>
<p>To calculate the final gas parameter values, there is more to be done than just
running a single command. After all, these parameters are part of the protocol
specification. They cannot be changed easily. And setting them to a wrong value
can cause severe system instability.</p>
<p>Our current strategy is to run estimations
with two different metrics and do so on standardized cloud hardware. The output
is then sanity checked manually by several people. Based on that, the final gas
parameter value is determined. Usually it will be the higher output of the two
metrics rounded up.</p>
<p>The PR <a href="https://github.com/near/nearcore/pull/8031">#8031</a> to set the ed25519
verification gas parameters is a good example for how such an analysis and
report could look.</p>
<p>More details on the process will be added to this document
in due time.</p>
<!-- TODO: how to add a new host function estimation -->
<!-- TODO: state of IO estimations -->
<!-- TODO: CE and Warehouse -->
<!-- TODO: ... -->
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-1"><a class="header" href="#overview-1">Overview</a></h1>
<p>This chapter describes various development processes and best practices employed
at nearcore.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rust-"><a class="header" href="#rust-">Rust 🦀</a></h1>
<p>This short chapter collects various useful general resources about the Rust
programming language. If you are already are familiar with Rust, skip this
chapter. Otherwise, this chapter is for you!</p>
<h2 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h2>
<p>Rust community actively encourages beginners to ask questions, take advantage of that!</p>
<p>We have a dedicated stream for Rust questions on our Zulip: <a href="https://near.zulipchat.com/#narrow/stream/300659-Rust-.F0.9F.A6.80">Rust
🦀</a>.</p>
<p>There's a general Rust forum at <a href="https://users.rust-lang.org">https://users.rust-lang.org</a>.</p>
<p>For a more interactive chat, take a look at Discord:
<a href="https://discord.com/invite/rust-lang">https://discord.com/invite/rust-lang</a>.</p>
<h2 id="reference-material"><a class="header" href="#reference-material">Reference Material</a></h2>
<p>Rust is <em>very</em> well documented. It's possible to learn the whole language and
most of the idioms by just reading the official docs. Starting points are</p>
<ul>
<li><a href="https://doc.rust-lang.org/book/">The Rust Book</a> (any resemblance to &quot;Guide to
Nearcore Development&quot; is purely coincidental)</li>
<li><a href="https://doc.rust-lang.org/stable/std/">Standard Library API</a></li>
</ul>
<p>Alternatives are:</p>
<ul>
<li><a href="https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283">Programming
Rust</a>
is an alternative book which moves a bit faster.</li>
<li><a href="https://doc.rust-lang.org/rust-by-example/">Rust By Example</a> is a great
resource for learning by doing.</li>
</ul>
<p>Rust has some great tooling, which also is documented:</p>
<ul>
<li><a href="https://doc.rust-lang.org/cargo/">Cargo</a>, the build system. Worth at least skimming through!</li>
<li>For IDE support, see <a href="https://www.jetbrains.com/rust/">IntelliJ Rust</a> if you
like JetBrains products or
<a href="https://rust-analyzer.github.io/manual.html">rust-analyzer</a> if you use any
other editor (fun fact: NEAR was one of the sponsors of rust-analyzer!).</li>
<li><a href="https://rust-lang.github.io/rustup/">Rustup</a> manages versions of the Rust
itself. It's unobtrusive, so feel free to skip this.</li>
</ul>
<h2 id="cheat-sheet"><a class="header" href="#cheat-sheet">Cheat Sheet</a></h2>
<p>This is a thing in its own category, do check it out:</p>
<p><a href="https://cheats.rs">https://cheats.rs</a></p>
<h2 id="language-mastery"><a class="header" href="#language-mastery">Language Mastery</a></h2>
<ul>
<li><a href="https://nostarch.com/rust-rustaceans">Rust for Rustaceans</a> — the book to read
after &quot;The Book&quot;.</li>
<li><a href="https://tokio.rs/tokio/tutorial">Tokio docs</a> explain asynchronous programming
in Rust (async/await).</li>
<li><a href="https://rust-lang.github.io/api-guidelines/about.html">Rust API Guidelines</a>
codify rules for idiomatic Rust APIs. Note that guidelines apply to <em>semver
surface</em> of libraries, and most of the code in nearcore is not on the semver
boundary. Still, a lot of insight there!</li>
<li><a href="https://doc.rust-lang.org/nomicon/">Rustonomicon</a> explains <code>unsafe</code>. (any
resemblance to <a href="https://nomicon.io">https://nomicon.io</a> is purely coincidental)</li>
</ul>
<h2 id="selected-blog-posts"><a class="header" href="#selected-blog-posts">Selected Blog Posts</a></h2>
<p>A lot of finer knowledge is hidden away in various dusty corners of Web-2.0.
Here are some favorites:</p>
<ul>
<li><a href="https://docs.rs/dtolnay/latest/dtolnay/macro._02__reference_types.html">https://docs.rs/dtolnay/latest/dtolnay/macro._02__reference_types.html</a></li>
<li><a href="https://limpet.net/mbrubeck/2019/02/07/rust-a-unique-perspective.html">https://limpet.net/mbrubeck/2019/02/07/rust-a-unique-perspective.html</a></li>
<li><a href="https://smallcultfollowing.com/babysteps/blog/2018/02/01/in-rust-ordinary-vectors-are-values/">https://smallcultfollowing.com/babysteps/blog/2018/02/01/in-rust-ordinary-vectors-are-values/</a></li>
<li><a href="https://smallcultfollowing.com/babysteps/blog/2016/10/02/observational-equivalence-and-unsafe-code/">https://smallcultfollowing.com/babysteps/blog/2016/10/02/observational-equivalence-and-unsafe-code/</a></li>
<li><a href="https://matklad.github.io/2021/09/05/Rust100k.html">https://matklad.github.io/2021/09/05/Rust100k.html</a></li>
</ul>
<p>And on the easiest topic of error-handling specifically:</p>
<ul>
<li><a href="http://sled.rs/errors.html">http://sled.rs/errors.html</a></li>
<li><a href="https://kazlauskas.me/entries/errors">https://kazlauskas.me/entries/errors</a></li>
<li><a href="http://joeduffyblog.com/2016/02/07/the-error-model/">http://joeduffyblog.com/2016/02/07/the-error-model/</a></li>
<li><a href="https://blog.burntsushi.net/rust-error-handling/">https://blog.burntsushi.net/rust-error-handling/</a></li>
</ul>
<p>Finally, as a dessert, the first rust slide deck:
<a href="http://venge.net/graydon/talks/rust-2012.pdf">http://venge.net/graydon/talks/rust-2012.pdf</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="workflows"><a class="header" href="#workflows">Workflows</a></h1>
<p>This chapter documents various way you can run <code>neard</code> during development: running a local net, joining a test net, doing benchmarking and load testing. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="run-a-node"><a class="header" href="#run-a-node">Run a Node</a></h1>
<p>This chapter focuses on the basics of running a node you've just build from
source. It tries to explain how the thing works under the hood, and pays
relatively little attention to various shortcuts we have.</p>
<h2 id="building-the-node"><a class="header" href="#building-the-node">Building the Node</a></h2>
<p>Start with the following command:</p>
<pre><code class="language-console">$ cargo run --profile quick-release -p neard -- --help
</code></pre>
<p>This command builds <code>neard</code> and asks it to show <code>--help</code>. Building <code>neard</code> takes
a while, take a look at <a href="practices/workflows/../fast_builds.html">Fast Builds</a> chapter to learn how to
speed it up.</p>
<p>Let's dissect the command:</p>
<ul>
<li><code>cargo run</code> asks <code>Cargo</code>, the package manager/build tool to run our
application. If you don't have <code>cargo</code>, install it via <a href="https://rustup.rs">https://rustup.rs</a></li>
<li><code>--profile quick-release</code> is our <a href="https://doc.rust-lang.org/cargo/reference/profiles.html#custom-profiles">custom profile</a> to build to build a somewhat
optimized version of the code. Default debug profile is faster to compile, but
produces a node which is too slow to participate in a real network. The
<code>--release</code> profile produces a fully optimized node, but that's very slow to
compile. So <code>--quick-release</code> is a sweat spot for us!</li>
<li><code>-p neard</code> asks to build the <code>neard</code> package. We use a <a href="https://doc.rust-lang.org/book/ch14-03-cargo-workspaces.html">cargo workspace</a> to
organize our code. The <code>neard</code> package in the top-level <code>/neard</code> directory is
the final binary which ties everything together.</li>
<li><code>--</code> tells cargo to pass the rest of the arguments thorough to <code>neard</code>.</li>
<li><code>--help</code> instructs <code>neard</code> to list available CLI arguments and subcommands.</li>
</ul>
<p><strong>Note:</strong> building <code>neard</code> might fail with an openssl or CC error. This means
that you lack some non-rust dependencies we use (openssl and rocksdb mainly). We
currently don't have docs on how to install those, but, basically, you want to
<code>sudo apt install</code> missing bits.</p>
<h2 id="preparing-tiny-network"><a class="header" href="#preparing-tiny-network">Preparing Tiny Network</a></h2>
<p>Typically, you want <code>neard</code> to connect to some network, like <code>mainnet</code> or
<code>testnet</code>. We'll get there in time, but we'll start small. For the current
chapter, we will run a network consisting of just a single node -- our own.</p>
<p>The first step there is creating required configuration. Run the <code>init</code> command
to create config files:</p>
<pre><code class="language-console">$ cargo run --profile quick-release -p neard -- init
INFO neard: version=&quot;trunk&quot; build=&quot;1.1.0-3091-ga8964d200-modified&quot; latest_protocol=57
INFO near: Using key ed25519:B41GMfqE2jWHVwrPLbD7YmjZxxeQE9WA9Ua2jffP5dVQ for test.near
INFO near: Using key ed25519:34d4aFJEmc2A96UXMa9kQCF8g2EfzZG9gCkBAPcsVZaz for node
INFO near: Generated node key, validator key, genesis file in ~/.near
</code></pre>
<p>As the log output says, we just generating <em>somethings</em> in <code>~/.near</code>. Let's take
a look:</p>
<pre><code class="language-console">$ ls ~/.near
config.json
genesis.json
node_key.json
validator_key.json
</code></pre>
<p>The most interesting file here is perhaps <code>genesis.json</code> -- it specifies the
initial state of our blockchain. There's a bunch of hugely important fields
there, which we'll ignore here. The part we'll look at is the <code>.records</code>, which
contains the actual initial data:</p>
<pre><code class="language-console">$ cat ~/.near/genesis.json | jq '.records'
[
  {
    &quot;Account&quot;: {
      &quot;account_id&quot;: &quot;test.near&quot;,
      &quot;account&quot;: {
        &quot;amount&quot;: &quot;1000000000000000000000000000000000&quot;,
        &quot;locked&quot;: &quot;50000000000000000000000000000000&quot;,
        &quot;code_hash&quot;: &quot;11111111111111111111111111111111&quot;,
        &quot;storage_usage&quot;: 0,
        &quot;version&quot;: &quot;V1&quot;
      }
    }
  },
  {
    &quot;AccessKey&quot;: {
      &quot;account_id&quot;: &quot;test.near&quot;,
      &quot;public_key&quot;: &quot;ed25519:B41GMfqE2jWHVwrPLbD7YmjZxxeQE9WA9Ua2jffP5dVQ&quot;,
      &quot;access_key&quot;: {
        &quot;nonce&quot;: 0,
        &quot;permission&quot;: &quot;FullAccess&quot;
      }
    }
  },
  {
    &quot;Account&quot;: {
      &quot;account_id&quot;: &quot;near&quot;,
      &quot;account&quot;: {
        &quot;amount&quot;: &quot;1000000000000000000000000000000000&quot;,
        &quot;locked&quot;: &quot;0&quot;,
        &quot;code_hash&quot;: &quot;11111111111111111111111111111111&quot;,
        &quot;storage_usage&quot;: 0,
        &quot;version&quot;: &quot;V1&quot;
      }
    }
  },
  {
    &quot;AccessKey&quot;: {
      &quot;account_id&quot;: &quot;near&quot;,
      &quot;public_key&quot;: &quot;ed25519:546XB2oHhj7PzUKHiH9Xve3Ze5q1JiW2WTh6abXFED3c&quot;,
      &quot;access_key&quot;: {
        &quot;nonce&quot;: 0,
        &quot;permission&quot;: &quot;FullAccess&quot;
      }
    }
  }
</code></pre>
<p>(I am using <a href="https://stedolan.github.io/jq/">jq</a> utility here)</p>
<p>We see that we have two accounts here, and we also see their public keys (but
not the private ones).</p>
<p>One of these accounts is a validator:</p>
<pre><code>$ cat ~/.near/genesis.json | jq '.validators'
[
  {
    &quot;account_id&quot;: &quot;test.near&quot;,
    &quot;public_key&quot;: &quot;ed25519:B41GMfqE2jWHVwrPLbD7YmjZxxeQE9WA9Ua2jffP5dVQ&quot;,
    &quot;amount&quot;: &quot;50000000000000000000000000000000&quot;
  }
]
</code></pre>
<p>Now, if we</p>
<pre><code class="language-console">$ cat ~/.near/validator_key.json
}
</code></pre>
<p>we'll see</p>
<pre><code class="language-json">{
  &quot;account_id&quot;: &quot;test.near&quot;,
  &quot;public_key&quot;: &quot;ed25519:B41GMfqE2jWHVwrPLbD7YmjZxxeQE9WA9Ua2jffP5dVQ&quot;,
  &quot;secret_key&quot;: &quot;ed25519:3x2dUQgBoEqNvKwPjfDE8zDVJgM8ysqb641PYHV28mGPu61WWv332p8keMDKHUEdf7GVBm4f6z4D1XRgBxnGPd7L&quot;
}
</code></pre>
<p>That is, we have a secret key for the sole validator in our network, how
convenient.</p>
<p>To recap, <code>neard init</code> without arguments creates a config for a new network
which starts with a single validator, for which we have the keys.</p>
<p>You might be wondering what <code>~/.near/node_key.json</code> is. That's not too
important, but in our network there's no 1-1 correspondence between machines
participating in the peer-to-peer network and accounts on the blockchain. So the
<code>node_key</code> specifies the keypair we'll use then signing network packets. These
packets internally will contain messages signed with the validator's key, and
these internal messages will drive the evolution of blockchain state.</p>
<p>Finally, <code>~/.near/config.json</code> contains various configs for the node itself.
That is, configs which don't actually affect the rules guiding the evolution of
the blockchain state, but rather things like timeous, database settings and
such.</p>
<p>The only field we'll look at is <code>boot_nodes</code>:</p>
<pre><code class="language-console">$ cat ~/.near/config.json | jq '.network.boot_nodes'
&quot;&quot;
</code></pre>
<p>It's empty! The <code>boot_nodes</code> specifies IPs of the initial nodes our node will
try to connect to on startup. As we are looking into running a single node
network, we want to leave it empty. But, if you would like to connect to
mainnet, you'd have to set this to some nodes from the mainnet you already know.
You'd also have to ensure that you use the same genesis as the the mainnet
though -- if the nodes tries to connect to a network with a different genesis,
it is rejected.</p>
<h2 id="running-the-network"><a class="header" href="#running-the-network">Running the Network</a></h2>
<p>Finally,</p>
<pre><code class="language-console">$ cargo run --profile quick-release -p neard -- run
INFO neard: version=&quot;trunk&quot; build=&quot;1.1.0-3091-ga8964d200-modified&quot; latest_protocol=57
INFO near: Creating a new RocksDB database path=/home/matklad/.near/data
INFO db: Created a new RocksDB instance. num_instances=1
INFO stats: #       0 4xecSHqTKx2q8JNQNapVEi5jxzewjxAnVFhMd4v5LqNh Validator | 1 validator 0 peers ⬇ 0 B/s ⬆ 0 B/s NaN bps 0 gas/s CPU: 0%, Mem: 50.8 MB
INFO near_chain::doomslug: ready to produce block @ 1, has enough approvals for 59.907µs, has enough chunks
INFO near_chain::doomslug: ready to produce block @ 2, has enough approvals for 40.732µs, has enough chunks
INFO near_chain::doomslug: ready to produce block @ 3, has enough approvals for 65.341µs, has enough chunks
INFO near_chain::doomslug: ready to produce block @ 4, has enough approvals for 51.916µs, has enough chunks
INFO near_chain::doomslug: ready to produce block @ 5, has enough approvals for 37.155µs, has enough chunks
...
</code></pre>
<p>🎉 it's alive!</p>
<p>So, what's going one here?</p>
<p>Our node is running a single node network. As the network only has a single
validator, and the node has the keys for the validator, the node is able to
produce blocks by itself. Note the increasing <code>@ 1</code>, <code>@ 2</code>, ... numbers. That
means that our network grows.</p>
<p>Let's stop the node with <code>^C</code> and look around</p>
<pre><code class="language-console">INFO near_chain::doomslug: ready to produce block @ 42, has enough approvals for 56.759µs, has enough chunks
^C WARN neard: SIGINT, stopping... this may take a few minutes.
INFO neard: Waiting for RocksDB to gracefully shutdown
INFO db: Waiting for remaining RocksDB instances to shut down num_instances=1
INFO db: All RocksDB instances shut down
$
</code></pre>
<p>The main change now is that we have <code>~/.near/data</code> directory which holds the
state of the network in various rocksdb tables:</p>
<pre><code class="language-console">$ ls ~/.near/data
 000004.log
 CURRENT
 IDENTITY
 LOCK
 LOG
 MANIFEST-000005
 OPTIONS-000107
 OPTIONS-000109
</code></pre>
<p>It doesn't really matter what those are, &quot;rocksdb stuff&quot; is a fine level of
understanding here. The important bit here is that the node remembers the state
of the network, so, when we restart it, it continues from around the last block:</p>
<pre><code class="language-console">$ cargo run --profile quick-release -p neard -- run
INFO neard: version=&quot;trunk&quot; build=&quot;1.1.0-3091-ga8964d200-modified&quot; latest_protocol=57
INFO db: Created a new RocksDB instance. num_instances=1
INFO db: Dropped a RocksDB instance. num_instances=0
INFO near: Opening an existing RocksDB database path=/home/matklad/.near/data
INFO db: Created a new RocksDB instance. num_instances=1
INFO stats: #       5 Cfba39eH7cyNfKn9GoKTyRg8YrhoY1nQxQs66tLBYwRH Validator | 1 validator 0 peers ⬇ 0 B/s ⬆ 0 B/s NaN bps 0 gas/s CPU: 0%, Mem: 49.4 MB
INFO near_chain::doomslug: not ready to produce block @ 43, need to wait 366.58789ms, has enough approvals for 78.776µs
INFO near_chain::doomslug: not ready to produce block @ 43, need to wait 265.547148ms, has enough approvals for 101.119518ms
INFO near_chain::doomslug: not ready to produce block @ 43, need to wait 164.509153ms, has enough approvals for 202.157513ms
INFO near_chain::doomslug: not ready to produce block @ 43, need to wait 63.176926ms, has enough approvals for 303.48974ms
INFO near_chain::doomslug: ready to produce block @ 43, has enough approvals for 404.41498ms, does not have enough chunks
INFO near_chain::doomslug: ready to produce block @ 44, has enough approvals for 50.07µs, has enough chunks
INFO near_chain::doomslug: ready to produce block @ 45, has enough approvals for 45.093µs, has enough chunks
</code></pre>
<h2 id="interacting-with-the-node"><a class="header" href="#interacting-with-the-node">Interacting With the Node</a></h2>
<p>Ok, now our node is running, let's poke it! The node exposes JSON RPC interface
which can be used to interact with the node itself (to, eg, do a health check)
or with the blockchain (to query information about blockchain state, or to
submit a transaction).</p>
<pre><code class="language-console">$ http get http://localhost:3030/status
HTTP/1.1 200 OK
access-control-allow-credentials: true
access-control-expose-headers: accept-encoding, accept, connection, host, user-agent
content-length: 1010
content-type: application/json
date: Tue, 15 Nov 2022 13:58:13 GMT
vary: Origin, Access-Control-Request-Method, Access-Control-Request-Headers

{
    &quot;chain_id&quot;: &quot;test-chain-rR8Ct&quot;,
    &quot;latest_protocol_version&quot;: 57,
    &quot;node_key&quot;: &quot;ed25519:71QRP9qKcYRUYXTLNnrmRc1NZSdBaBo9nKZ88DK5USNf&quot;,
    &quot;node_public_key&quot;: &quot;ed25519:5A5QHyLayA9zksJZGBzveTgBRecpsVS4ohuxujMAFLLa&quot;,
    &quot;protocol_version&quot;: 57,
    &quot;rpc_addr&quot;: &quot;0.0.0.0:3030&quot;,
    &quot;sync_info&quot;: {
        &quot;earliest_block_hash&quot;: &quot;6gJLCnThQENYFbnFQeqQvFvRsTS5w87bf3xf8WN1CMUX&quot;,
        &quot;earliest_block_height&quot;: 0,
        &quot;earliest_block_time&quot;: &quot;2022-11-15T13:45:53.062613669Z&quot;,
        &quot;epoch_id&quot;: &quot;6gJLCnThQENYFbnFQeqQvFvRsTS5w87bf3xf8WN1CMUX&quot;,
        &quot;epoch_start_height&quot;: 501,
        &quot;latest_block_hash&quot;: &quot;9JC9o3rZrDLubNxVr91qMYvaDiumzwtQybj1ZZR9dhbK&quot;,
        &quot;latest_block_height&quot;: 952,
        &quot;latest_block_time&quot;: &quot;2022-11-15T13:58:13.185721125Z&quot;,
        &quot;latest_state_root&quot;: &quot;9kEYQtWczrdzKCCuFzPDX3Vtar1pFPXMdLU5HJyF8Ght&quot;,
        &quot;syncing&quot;: false
    },
    &quot;uptime_sec&quot;: 570,
    &quot;validator_account_id&quot;: &quot;test.near&quot;,
    &quot;validator_public_key&quot;: &quot;ed25519:71QRP9qKcYRUYXTLNnrmRc1NZSdBaBo9nKZ88DK5USNf&quot;,
    &quot;validators&quot;: [
        {
            &quot;account_id&quot;: &quot;test.near&quot;,
            &quot;is_slashed&quot;: false
        }
    ],
    &quot;version&quot;: {
        &quot;build&quot;: &quot;1.1.0-3091-ga8964d200-modified&quot;,
        &quot;rustc_version&quot;: &quot;1.65.0&quot;,
        &quot;version&quot;: &quot;trunk&quot;
    }
}
</code></pre>
<p>(I am using <a href="https://httpie.io/cli">HTTPie here</a>)</p>
<p>Note how <code>&quot;latest_block_height&quot;: 952</code> corresponds to <code>@ 952</code> we see in the logs.</p>
<p>Let's query blockchain state:</p>
<pre><code>$ http post http://localhost:3030/ method=query jsonrpc=2.0 id=1 \
     params:='{&quot;request_type&quot;: &quot;view_account&quot;, &quot;finality&quot;: &quot;final&quot;, &quot;account_id&quot;: &quot;test.near&quot;}'
λ http post http://localhost:3030/ method=query jsonrpc=2.0 id=1 \
           params:='{&quot;request_type&quot;: &quot;view_account&quot;, &quot;finality&quot;: &quot;final&quot;, &quot;account_id&quot;: &quot;test.near&quot;}'

HTTP/1.1 200 OK
access-control-allow-credentials: true
access-control-expose-headers: content-length, accept, connection, user-agent, accept-encoding, content-type, host
content-length: 294
content-type: application/json
date: Tue, 15 Nov 2022 14:04:54 GMT
vary: Origin, Access-Control-Request-Method, Access-Control-Request-Headers

{
    &quot;id&quot;: &quot;1&quot;,
    &quot;jsonrpc&quot;: &quot;2.0&quot;,
    &quot;result&quot;: {
        &quot;amount&quot;: &quot;1000000000000000000000000000000000&quot;,
        &quot;block_hash&quot;: &quot;Hn4v5CpfWf141AJi166gdDK3e3khCxgfeDJ9dSXGpAVi&quot;,
        &quot;block_height&quot;: 1611,
        &quot;code_hash&quot;: &quot;11111111111111111111111111111111&quot;,
        &quot;locked&quot;: &quot;50003138579594550524246699058859&quot;,
        &quot;storage_paid_at&quot;: 0,
        &quot;storage_usage&quot;: 182
    }
}
</code></pre>
<p>Note how we use a <code>post</code> HTTP method when we interact with blockchain RPC. The
full set of RPC endpoints is documented at</p>
<p><a href="https://docs.near.org/api/rpc/introduction">https://docs.near.org/api/rpc/introduction</a></p>
<h2 id="sending-transactions"><a class="header" href="#sending-transactions">Sending Transactions</a></h2>
<p>Transactions are submitted via RPC as well. Submitting a transaction manually
with <code>http</code> is going to be cumbersome though — transactions are are borsh
encoded to bytes, then signed, then encoded in base64 for JSON.</p>
<p>So we will use the official <a href="https://docs.near.org/tools/near-cli">NEAR CLI</a> utility.</p>
<p>Install it via <code>npm</code>:</p>
<pre><code class="language-console">$ npm install -g near-cli
$ near -h
Usage: near &lt;command&gt; [options]

Commands:
  near create-account &lt;accountId&gt;    create a new developer account
....
</code></pre>
<p>Note that, although you install <code>near-cli</code>, the name of the utility is <code>near</code>.</p>
<p>As a first-step, let's redo the <code>view_account</code> call we did with raw <code>httpie</code>
with <code>near-cli</code>:</p>
<pre><code class="language-console">$ NEAR_ENV=local near state test.near
Loaded master account test.near key from ~/.near/validator_key.json with public key = ed25519:71QRP9qKcYRUYXTLNnrmRc1NZSdBaBo9nKZ88DK5USNf
Account test.near
{
  amount: '1000000000000000000000000000000000',
  block_hash: 'ESGN7H1kVLp566CTQ9zkBocooUFWNMhjKwqHg4uCh2Sg',
  block_height: 2110,
  code_hash: '11111111111111111111111111111111',
  locked: '50005124762657986708532525400812',
  storage_paid_at: 0,
  storage_usage: 182,
  formattedAmount: '1,000,000,000'
}
</code></pre>
<p><code>NEAR_ENV=local</code> tells <code>near-cli</code> to use our local network, rather than the
<code>mainnet</code>.</p>
<p>Now, let's create a couple of accounts and send tokes between them:</p>
<pre><code>$ NEAR_ENV=local near create-account alice.test.near --masterAccount test.near
NOTE: In most cases, when connected to network &quot;local&quot;, masterAccount will end in &quot;.node0&quot;
Loaded master account test.near key from /home/matklad/.near/validator_key.json with public key = ed25519:71QRP9qKcYRUYXTLNnrmRc1NZSdBaBo9nKZ88DK5USNf
Saving key to 'undefined/local/alice.test.near.json'
Account alice.test.near for network &quot;local&quot; was created.

$ NEAR_ENV=local near create-account bob.test.near --masterAccount test.near
NOTE: In most cases, when connected to network &quot;local&quot;, masterAccount will end in &quot;.node0&quot;
Loaded master account test.near key from /home/matklad/.near/validator_key.json with public key = ed25519:71QRP9qKcYRUYXTLNnrmRc1NZSdBaBo9nKZ88DK5USNf
Saving key to 'undefined/local/bob.test.near.json'
Account bob.test.near for network &quot;local&quot; was created.

$ NEAR_ENV=local near send alice.test.near bob.test.near 10
Sending 10 NEAR to bob.test.near from alice.test.near
Loaded master account test.near key from /home/matklad/.near/validator_key.json with public key = ed25519:71QRP9qKcYRUYXTLNnrmRc1NZSdBaBo9nKZ88DK5USNf
Transaction Id BBPndo6gR4X8pzoDK7UQfoUXp5J8WDxkf8Sq75tK5FFT
To see the transaction in the transaction explorer, please open this url in your browser
http://localhost:9001/transactions/BBPndo6gR4X8pzoDK7UQfoUXp5J8WDxkf8Sq75tK5FFT
</code></pre>
<p>NEAR CLI printouts are not always the most useful or accurate, but this seems to
work.</p>
<p>Note that <code>near</code> automatically creates keypairs and stores them at
<code>.near-credentials</code>:</p>
<pre><code class="language-console">$ ls ~/.near-credentials/local
  alice.test.near.json
  bob.test.near.json
</code></pre>
<p>To verify this actually did work, and that <code>near-cli</code> didn't cheat us, let's
query the state of accounts manually:</p>
<pre><code class="language-console">$ http post http://localhost:3030/ method=query jsonrpc=2.0 id=1 \
    params:='{&quot;request_type&quot;: &quot;view_account&quot;, &quot;finality&quot;: &quot;final&quot;, &quot;account_id&quot;: &quot;alice.test.near&quot;}' \
    | jq '.result.amount'
&quot;89999955363487500000000000&quot;

14:30:52|~
λ http post http://localhost:3030/ method=query jsonrpc=2.0 id=1 \
    params:='{&quot;request_type&quot;: &quot;view_account&quot;, &quot;finality&quot;: &quot;final&quot;, &quot;account_id&quot;: &quot;bob.test.near&quot;}' \
    | jq '.result.amount'
&quot;110000000000000000000000000&quot;
</code></pre>
<p>Indeed, some amounts of tokes was transferred from <code>alice</code> to <code>bob</code>, and then
some amount of tokes was deducted to account for transaction fees.</p>
<h2 id="recap"><a class="header" href="#recap">Recap</a></h2>
<p>Great! So we've learned how to run our very own single-node NEAR network using a
binary we've build from source. The steps are:</p>
<ul>
<li>Create configs with <code>cargo run --profile quick-release -p neard -- init</code></li>
<li>Run the node with <code>cargo run --profile quick-release -p neard -- run</code></li>
<li>Poke the node with <code>http</code> or</li>
<li>Install <code>near-cli</code> via <code>npm install -g near-cli</code></li>
<li>Submit transactions via <code>NEAR_ENV=local near create-account ...</code></li>
</ul>
<p>In the <a href="practices/workflows/./deploy_a_contract.html">next chapter</a> we'll learn how to deploy a simple
WASM contract.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deploy-a-contract"><a class="header" href="#deploy-a-contract">Deploy a Contract</a></h1>
<p>In this chapter, we'll learn how to build, deploy, and call a minimal
smart-contract on our local node.</p>
<h2 id="preparing-ground"><a class="header" href="#preparing-ground">Preparing Ground</a></h2>
<p>Let's start with creating a fresh local network with an account we'll deploy a
contract to. You might want to re-read <a href="practices/workflows/./run_a_node.html">how to run a node</a> to
understand what's going one here:</p>
<pre><code class="language-console">$ cargo run --profile quick-release -p neard -- init
$ cargo run --profile quick-release -p neard -- run
$ NEAR_ENV=local near create-account alice.test.near --masterAccount test.near
</code></pre>
<p>As a sanity check, querying the state of <code>alice.test.near</code> account should work:</p>
<pre><code class="language-console">$ NEAR_ENV=local near state alice.test.near
Loaded master account test.near key from /home/matklad/.near/validator_key.json with public key = ed25519:7tU4NtFozPWLotcfhbT9KfBbR3TJHPfKJeCri8Me6jU7
Account alice.test.near
{
  amount: '100000000000000000000000000',
  block_hash: 'EEMiLrk4ZiRzjNJXGdhWPJfKXey667YBnSRoJZicFGy9',
  block_height: 24,
  code_hash: '11111111111111111111111111111111',
  locked: '0',
  storage_paid_at: 0,
  storage_usage: 182,
  formattedAmount: '100'
}
</code></pre>
<h2 id="minimal-contract"><a class="header" href="#minimal-contract">Minimal Contract</a></h2>
<p>NEAR contracts are <a href="https://webassembly.org">WebAssembly</a> blobs of bytes. To
create a contract, contract developer typically uses an SDK for some high-level
programming language, such as JavaScript, which takes care of producing the
right <code>.wasm</code>.</p>
<p>In this guide, we are interested in how things work under the hood, so we'll
do everything manually, and implement a contract in Rust without any help from
SDKs.</p>
<p>As we are looking for something simple, let's create a contract with a single
&quot;method&quot;, <code>hello</code>, which returns <code>&quot;hello world&quot;</code> string. To &quot;define a method&quot;, a
wasm module should export a function. To &quot;return a value&quot;, the contract needs to
interact with the environment to say &quot;hey, this is the value I am returning&quot;.
Such &quot;interractions&quot; are carried through host-functions, which are quite a bit
like syscalls in traditional operating system.</p>
<p>The set of host functions which the contract can import is defined in
<a href="https://github.com/near/nearcore/blob/aeccaaab334275f6d0a62deabd184675bc3c6a23/runtime/near-vm-runner/src/imports.rs#L71-L242"><code>imports.rs</code></a>.</p>
<p>In this particular case, we need <code>value_return</code> function:</p>
<pre><code>value_return&lt;[value_len: u64, value_ptr: u64] -&gt; []&gt;
</code></pre>
<p>That is, <code>value_return</code> function takes a pointer to slice of bytes, a length of
the slice, and returns nothing. If the contract calls this function, the slice
would be considered a result of the function.</p>
<p>To recap, we want to produce a <code>.wasm</code> file with roughly the following content:</p>
<pre><code class="language-wasm">(module
  (import &quot;env&quot; &quot;value_return&quot; (func $value_return (param i64 i64)))
  (func (export &quot;hello&quot;) ... ))
</code></pre>
<h2 id="cargo-boilerplate"><a class="header" href="#cargo-boilerplate">Cargo Boilerplate</a></h2>
<p>Armed with this knowledge, we can write Rust code to produce the required WASM.
Before we actually start doing that, some amount of setup code is required.</p>
<p>Let's start with creating a new crate:</p>
<pre><code class="language-console">$ cargo new hello-near --lib
</code></pre>
<p>To compile to wasm, we also need to add a relevant rustup toolchain:</p>
<pre><code class="language-console">$ rustup toolchain add wasm32-unknown-unknown
</code></pre>
<p>Then, we need to tell Cargo that the final artifact we want to get is a
WebAssembly module.</p>
<p>This requires the following cryptic spell in Cargo.toml:</p>
<pre><code class="language-toml"># hello-near/Cargo.toml

[lib]
crate-type = [&quot;cdylib&quot;]
</code></pre>
<p>Here, we ask Cargo to build a &quot;C dynamic library&quot;. When compiling for wasm,
this'll give us a <code>.wasm</code> module. This is just confusing, sorry about that
:(</p>
<p>Next, as we are aiming for minimalism here, we need to disable optional bits
of Rust runtime. Namely, we want to make our crate <code>no_std</code>, set <code>panic=abort</code>
as our panic strategy and define a panic handler to abort execution.</p>
<pre><code class="language-toml"># hello-near/Cargo.toml

[package]
name = &quot;hello-near&quot;
version = &quot;0.1.0&quot;
edition = &quot;2021&quot;

[lib]
crate-type = [&quot;cdylib&quot;]

[profile.release]
panic = &quot;abort&quot;
</code></pre>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// hello-near/src/lib.rs

#![no_std]

#[panic_handler]
fn panic_handler(_info: &amp;core::panic::PanicInfo) -&gt; ! {
    core::arch::wasm32::unreachable()
}
<span class="boring">}
</span></code></pre></pre>
<p>At this point, we should be able to compile our code to wasm, and it should be
fairly small. Let's do that:</p>
<pre><code class="language-console">$ cargo b -r --target wasm32-unknown-unknown
   Compiling hello-near v0.1.0 (~/hello-near)
    Finished release [optimized] target(s) in 0.24s
$ ls target/wasm32-unknown-unknown/release/hello_near.wasm
.rwxr-xr-x 106 matklad 15 Nov 15:34 target/wasm32-unknown-unknown/release/hello_near.wasm
</code></pre>
<p>106 bytes is pretty small! Let's see what's inside. For that, we'll use
<code>wasm-tools</code> suite of cli utilities.</p>
<pre><code class="language-console">$ cargo install wasm-tools
λ wasm-tools print target/wasm32-unknown-unknown/release/hello_near.wasm
(module
  (memory (;0;) 16)
  (global $__stack_pointer (;0;) (mut i32) i32.const 1048576)
  (global (;1;) i32 i32.const 1048576)
  (global (;2;) i32 i32.const 1048576)
  (export &quot;memory&quot; (memory 0))
  (export &quot;__data_end&quot; (global 1))
  (export &quot;__heap_base&quot; (global 2))
)
</code></pre>
<h2 id="rust-contract"><a class="header" href="#rust-contract">Rust Contract</a></h2>
<p>Finally, let's implement an actual contract. We'll need <code>extern &quot;C&quot;</code> block to
declare <code>value_return</code> import, and a <code>#[no_mangle] extern &quot;C&quot;</code> function to declare <code>hello</code> export:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// hello-near/src/lib.rs

#![no_std]

extern &quot;C&quot; {
    fn value_return(len: u64, ptr: u64);
}

#[no_mangle]
pub extern &quot;C&quot; fn hello() {
    let msg = &quot;hello world&quot;;
    unsafe { value_return(msg.len() as u64, msg.as_ptr() as u64) }
}

#[panic_handler]
fn panic_handler(_info: &amp;core::panic::PanicInfo) -&gt; ! {
    core::arch::wasm32::unreachable()
}
<span class="boring">}
</span></code></pre></pre>
<p>Building and inspecting output shows that it's roughly what we want:</p>
<pre><code class="language-console">$ cargo b -r --target wasm32-unknown-unknown
   Compiling hello-near v0.1.0 (/home/matklad/hello-near)
    Finished release [optimized] target(s) in 0.05s
$ wasm-tools print target/wasm32-unknown-unknown/release/hello_near.wasm
(module
  (type (;0;) (func (param i64 i64)))
  (type (;1;) (func))
  (import &quot;env&quot; &quot;value_return&quot;        (; &lt;- Here's our import. ;)
    (func $value_return (;0;) (type 0)))
  (func $hello (;1;) (type 1)
    i64.const 11
    i32.const 1048576
    i64.extend_i32_u
    call $value_return
  )
  (memory (;0;) 17)
  (global $__stack_pointer (;0;) (mut i32) i32.const 1048576)
  (global (;1;) i32 i32.const 1048587)
  (global (;2;) i32 i32.const 1048592)
  (export &quot;memory&quot; (memory 0))
  (export &quot;hello&quot; (func $hello))      (; &lt;- And export! ;)
  (export &quot;__data_end&quot; (global 1))
  (export &quot;__heap_base&quot; (global 2))
  (data $.rodata (;0;) (i32.const 1048576) &quot;hello world&quot;)
)
</code></pre>
<h2 id="deploying-the-contract"><a class="header" href="#deploying-the-contract">Deploying the Contract</a></h2>
<p>Now that we have WASM, let's deploy it!</p>
<pre><code class="language-console">$ NEAR_ENV=local near deploy alice.test.near \
    ./target/wasm32-unknown-unknown/release/hello_near.wasm
Loaded master account test.near key from /home/matklad/.near/validator_key.json with public key = ed25519:ChLD1qYic3G9qKyzgFG3PifrJs49CDYeERGsG58yaSoL
Starting deployment. Account id: alice.test.near, node: http://127.0.0.1:3030, helper: http://localhost:3000, file: ./target/wasm32-unknown-unknown/release/hello_near.wasm
Transaction Id GDbTLUGeVaddhcdrQScVauYvgGXxSssEPGUSUVAhMWw8
To see the transaction in the transaction explorer, please open this url in your browser
http://localhost:9001/transactions/GDbTLUGeVaddhcdrQScVauYvgGXxSssEPGUSUVAhMWw8
Done deploying to alice.test.near
</code></pre>
<p>And, finally, let's call our contract:</p>
<pre><code class="language-console">$ NEAR_ENV=local $near call alice.test.near hello --accountId alice.test.near
Scheduling a call: alice.test.near.hello()
Loaded master account test.near key from /home/matklad/.near/validator_key.json with public key = ed25519:ChLD1qYic3G9qKyzgFG3PifrJs49CDYeERGsG58yaSoL
Doing account.functionCall()
Transaction Id 9WMwmTf6pnFMtj1KBqjJtkKvdFXS4kt3DHnYRnbFpJ9e
To see the transaction in the transaction explorer, please open this url in your browser
http://localhost:9001/transactions/9WMwmTf6pnFMtj1KBqjJtkKvdFXS4kt3DHnYRnbFpJ9e
'hello world'
</code></pre>
<p>Note that we pass <code>alice.test.near</code> twice: first time to specify which contract
we are calling, the second time to determine who calls the contract. That is,
the second account actually spends tokens. In the following example <code>bob</code> spends
NEAR to call contact deployed to alice account:</p>
<pre><code class="language-console">$ NEAR_ENV=local $near call alice.test.near hello --accountId bob.test.near
Scheduling a call: alice.test.near.hello()
Loaded master account test.near key from /home/matklad/.near/validator_key.json with public key = ed25519:ChLD1qYic3G9qKyzgFG3PifrJs49CDYeERGsG58yaSoL
Doing account.functionCall()
Transaction Id 4vQKtP6zmcR4Xaebw8NLF6L5YS96gt5mCxc5BUqUcC41
To see the transaction in the transaction explorer, please open this url in your browser
http://localhost:9001/transactions/4vQKtP6zmcR4Xaebw8NLF6L5YS96gt5mCxc5BUqUcC41
'hello world'
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-the-estimator"><a class="header" href="#running-the-estimator">Running the Estimator</a></h1>
<p>This workflow describes how to run the gas estimator byzantine-benchmark suite.
To learn about its background and purpose, refer to <a href="practices/workflows/../../architecture/gas/estimator.html">Runtime Parameter
Estimator</a> in the architecture chapter.</p>
<p>Type this in your console to quickly run estimations on a couple of action costs.</p>
<pre><code class="language-bash">cargo run -p runtime-params-estimator --features required -- \
    --accounts-num 20000 --additional-accounts-num 20000 \
    --iters 3 --warmup-iters 1 --metric time \
    --costs=ActionReceiptCreation,ActionTransfer,ActionCreateAccount,ActionFunctionCallBase
</code></pre>
<p>You should get an output like this.</p>
<pre><code>[elapsed 00:00:17 remaining 00:00:00] Writing into storage ████████████████████   20000/20000  
ActionReceiptCreation         4_499_673_502_000 gas [  4.499674ms]    (computed in 7.22s) 
ActionTransfer                  410_122_090_000 gas [   410.122µs]    (computed in 4.71s) 
ActionCreateAccount             237_495_890_000 gas [   237.496µs]    (computed in 4.64s) 
ActionFunctionCallBase          770_989_128_914 gas [   770.989µs]    (computed in 4.65s) 


Finished in 40.11s, output saved to:

    /home/you/near/nearcore/costs-2022-11-11T11:11:11Z-e40863c9b.txt
</code></pre>
<p>This shows how much gas a parameter should cost to satisfy the 1ms = 1Tgas rule.
It also shows how much time that corresponds to and how long it took to compute
each of the estimations.</p>
<p>Note that the above does not produce very accurate results and it can have high
variance as well. It runs an unoptimized binary, the state is small, and the
metric used is wall-clock time which is always prone to variance in hardware and
can be affected by other processes currently running on your system.</p>
<p>Once your estimation code is ready, it is better to run it with a larger state
and an optimized binary.</p>
<pre><code class="language-bash">cargo run --release -p runtime-params-estimator --features required -- \
    --accounts-num 20000 --additional-accounts-num 2000000 \
    --iters 3 --warmup-iters 1 --metric time \
    --costs=ActionReceiptCreation,ActionTransfer,ActionCreateAccount,ActionFunctionCallBase
</code></pre>
<p>You might also want to run a hardware-agnostic estimation using the following
command. It uses <code>docker</code> and <code>qemu</code> under the hood, so it will be quite a bit
slower. You will need to install <code>docker</code> to run this command.</p>
<pre><code class="language-bash">cargo run --release -p runtime-params-estimator --features required -- \
    --accounts-num 20000 --additional-accounts-num 2000000 \
    --iters 3 --warmup-iters 1 --metric icount --docker --full \
    --costs=ActionReceiptCreation,ActionTransfer,ActionCreateAccount,ActionFunctionCallBase
</code></pre>
<p>Note how the output looks a bit different now. The <code>i</code>, <code>r</code> and <code>w</code> values show
instruction count, read IO bytes, and write IO bytes respectively. The IO byte
count is known to be inaccurate.</p>
<pre><code>+ /host/nearcore/runtime/runtime-params-estimator/emu-cost/counter_plugin/qemu-x86_64 -plugin file=/host/nearcore/runtime/runtime-params-estimator/emu-cost/counter_plugin/libcounter.so -cpu Westmere-v1 /host/nearcore/target/release/runtime-params-estimator --home /.near --accounts-num 20000 --iters 3 --warmup-iters 1 --metric icount --costs=ActionReceiptCreation,ActionTransfer,ActionCreateAccount,ActionFunctionCallBase --skip-build-test-contract --additional-accounts-num 0 --in-memory-db
ActionReceiptCreation         214_581_685_500 gas [  1716653.48i 0.00r 0.00w]     (computed in 6.11s) 
ActionTransfer                 21_528_212_916 gas [   172225.70i 0.00r 0.00w]     (computed in 4.71s) 
ActionCreateAccount            26_608_336_250 gas [   212866.69i 0.00r 0.00w]     (computed in 4.67s) 
ActionFunctionCallBase         12_193_364_898 gas [    97546.92i 0.00r 0.00w]     (computed in 2.39s) 


Finished in 17.92s, output saved to:

    /host/nearcore/costs-2022-11-01T16:27:36Z-e40863c9b.txt
</code></pre>
<p>The difference between the metrics is discussed in the <a href="practices/workflows/../../architecture/gas/estimator.html#estimation-metrics">Estimation
Metrics</a>.</p>
<p>You should now be all setup for running estimations on your local machine. Also
check <code>cargo run -p runtime-params-estimator --features required -- --help</code> for
the list of available options.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="code-style"><a class="header" href="#code-style">Code Style</a></h1>
<p>This document specifies the code style to use in the nearcore repository. The
primary goal here is to achieve consistency, maintain it over time, and cut down
on the mental overhead related to style choices.</p>
<p>Right now, <code>nearcore</code> codebase is not perfectly consistent, and the style
acknowledges this. It guides newly written code and serves as a tie breaker for
decisions. Rewriting existing code to conform 100% to the style is not a goal.
Local consistency is more important: if new code is added to a specific file,
it's more important to be consistent with the file rather than with this style
guide.</p>
<p>This is a live document, which intentionally starts in a minimal case. When
doing code-reviews, consider if some recurring advice you give could be moved
into this document.</p>
<h2 id="formatting"><a class="header" href="#formatting">Formatting</a></h2>
<p>Use <code>rustfmt</code> for minor code formatting decisions. This rule is enforced by CI</p>
<p><strong>Rationale:</strong> <code>rustfmt</code> style is almost always good enough, even if not always
perfect. The amount of bikeshedding saved by <code>rustfmt</code> far outweighs any
imperfections.</p>
<h2 id="idiomatic-rust"><a class="header" href="#idiomatic-rust">Idiomatic Rust</a></h2>
<p>While the most important thing is to solve the problem at hand, we strive to
implement the solution in idiomatic Rust, if possible. To learn what is
considered idiomatic Rust, a good start are the Rust API guidelines (but keep in
mind that <code>nearcore</code> is not a library with public API, not all advice applies
literally):</p>
<p>https://rust-lang.github.io/api-guidelines/about.html</p>
<p>When in doubt, ask question in the <a href="https://near.zulipchat.com/#narrow/stream/300659-Rust-.F0.9F.A6.80">Rust
🦀</a> Zulip
stream or during code review.</p>
<p><strong>Rationale:</strong> Consistency, as there's usually only one idiomatic solution
amidst many non-idiomatic ones. Predictability, you can use the APIs without
consulting documentation. Performance, ergonomics and correctness: language
idioms usually reflect learned truths, which might not be immediately obvious.</p>
<h2 id="style"><a class="header" href="#style">Style</a></h2>
<p>This section documents all micro-rules which are not otherwise enforced by
<code>rustfmt</code>.</p>
<h3 id="avoid-asrefas_ref"><a class="header" href="#avoid-asrefas_ref">Avoid <code>AsRef::as_ref</code></a></h3>
<p>When you have some concrete type, prefer <code>.as_str</code>, <code>.as_bytes</code>, <code>.as_path</code> over
generic <code>.as_ref</code>. Only use <code>.as_ref</code> when the type in question is a generic
<code>T: AsRef&lt;U&gt;</code>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
fn log_validator(account_id: AccountId) {
    metric_for(account_id.as_str())
       .increment()
}

// BAD
fn log_validator(account_id: AccountId) {
    metric_for(account_id.as_ref())
       .increment()
}
<span class="boring">}
</span></code></pre></pre>
<p>Note that <code>Option::as_ref</code>, <code>Result::as_ref</code> are great, do use them!</p>
<p><strong>Rationale:</strong> readability and churn-resistance. There might be more than one
<code>AsRef&lt;U&gt;</code> implementation for a given type (with different <code>U</code>s). If a new
implementation is added, some of the <code>.as_ref()</code> calls might break. See also
https://github.com/rust-lang/rust/issues/62586.</p>
<h3 id="avoid-references-to-copy-types"><a class="header" href="#avoid-references-to-copy-types">Avoid references to <code>Copy</code>-types</a></h3>
<p>Various generic APIs in Rust often return references to data (<code>&amp;T</code>). When <code>T</code> is
a small <code>Copy</code> type like <code>i32</code>, you end up with <code>&amp;i32</code> while many API expect
<code>i32</code>, so dereference has to happen <em>somewhere</em>. Prefer dereferencing as early
as possible, typically in a pattern:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
fn compute(map: HashMap&lt;&amp;'str, i32&gt;) {
    if let Some(&amp;value) = map.get(&quot;key&quot;) {
        process(value)
    }
}
fn process(value: i32) { ... }

// BAD
fn compute(map: HashMap&lt;&amp;'str, i32&gt;) {
    if let Some(value) = map.get(&quot;key&quot;) {
        process(*value)
    }
}
fn process(value: i32) { ... }
<span class="boring">}
</span></code></pre></pre>
<p><strong>Rationale:</strong> if the value is used multiple times, dereferencing in the pattern
saves keystrokes. If the value is used exactly once, we just want to be
consistent. Additional benefit of early deref is reduced scope of borrow.</p>
<p>Note that for some <em>big</em> <code>Copy</code> types, notably <code>CryptoHash</code>, we sometimes use
references for performance reasons. As a rule of thumb, <code>T</code> is considered <em>big</em> if
<code>size_of::&lt;T&gt;() &gt; 2 * size_of::&lt;usize&gt;()</code>.</p>
<h3 id="prefer-for-loops-over-for_each-and-try_for_each-methods"><a class="header" href="#prefer-for-loops-over-for_each-and-try_for_each-methods">Prefer for loops over <code>for_each</code> and <code>try_for_each</code> methods</a></h3>
<p>Iterators offer <code>for_each</code> and <code>try_for_each</code> methods which allow executing
a closure over all items of the iterator.  This is similar to using a for loop
but comes with various complications and may lead to less readable code.  Prefer
using a loop rather than those methods, for example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
for outcome_with_id in result? {
    *total_gas_burnt =
        safe_add_gas(*total_gas_burnt, outcome_with_id.outcome.gas_burnt)?;
    outcomes.push(outcome_with_id);
}

// BAD
result?.into_iter().try_for_each(
    |outcome_with_id: ExecutionOutcomeWithId| -&gt; Result&lt;(), RuntimeError&gt; {
        *total_gas_burnt =
            safe_add_gas(*total_gas_burnt, outcome_with_id.outcome.gas_burnt)?;
        outcomes.push(outcome_with_id);
        Ok(())
    },
)?;
<span class="boring">}
</span></code></pre></pre>
<p><strong>Rationale:</strong> The <code>for_each</code> and <code>try_for_each</code> method don’t play nice with
<code>break</code> and <code>continue</code> statements nor do they mesh well with async IO (since
<code>.await</code> inside of the closure isn’t possible).  And while <code>try_for_each</code> allows
for the use of question mark operator, one may end up having to uses it twice:
once inside the closure and second time outside the call to <code>try_for_each</code>.
Furthermore, usage of the functions often introduce some minor syntax noise.</p>
<p>There are situations when those methods may lead to more readable code.  Common
example are long call chains.  Even then such code may evolve with the closure
growing and leading to less readable code.  If advantages of using the methods
aren’t clear cut, it’s usually better to err on side of more imperative style.</p>
<p>Lastly, anecdotally the methods (e.g. when used with <code>chain</code> or <code>flat_map</code>) may
lead to faster code.  This intuitively makes sense but it’s worth to keep in
mind that compilers are pretty good at optimising and in practice may generate
optimal code anyway.  Furthermore, optimising code for readability may be more
important (especially outside of hot path) than small performance gains.</p>
<h3 id="prefer-to_string-to-format"><a class="header" href="#prefer-to_string-to-format">Prefer <code>to_string</code> to <code>format!(&quot;{}&quot;)</code></a></h3>
<p>Prefer calling <code>to_string</code> method on an object rather than passing it through
<code>format!(&quot;{}&quot;)</code> if all you’re doing is converting it to a <code>String</code>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
lat hash = block_hash.to_string();
let msg = format!(&quot;{}: failed to open&quot;, path.display());

// BAD
lat hash = format!(&quot;{block_hash}&quot;);
let msg = path.display() + &quot;: failed to open&quot;;
<span class="boring">}
</span></code></pre></pre>
<p><strong>Rationale:</strong> <code>to_string</code> is shorter to type and also faster.</p>
<h3 id="import-granularity"><a class="header" href="#import-granularity">Import Granularity</a></h3>
<p>Group import by module, but not deeper:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
use std::collections::{hash_map, BTreeSet};
use std::sync::Arc;

// BAD - nested groups.
use std::{
    collections::{hash_map, BTreeSet},
    sync::Arc,
};

// BAD - not grouped together.
use std::collections::BTreeSet;
use std::collections::hash_map;
use std::sync::Arc;
<span class="boring">}
</span></code></pre></pre>
<p>This corresponds to <code>&quot;rust-analyzer.assist.importGranularity&quot;: &quot;module&quot;</code> setting
in rust-analyzer
(<a href="https://rust-analyzer.github.io/manual.html#rust-analyzer.assist.importGranularity">docs</a>).</p>
<p><strong>Rationale:</strong> Consistency, matches existing practice.</p>
<h3 id="import-blocks"><a class="header" href="#import-blocks">Import Blocks</a></h3>
<p>Do not separate imports into groups with blank lines. Write a single block of
imports and rely on <code>rustfmt</code> to sort them.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
use crate::types::KnownPeerState;
use borsh::BorshSerialize;
use near_primitives::utils::to_timestamp;
use near_store::{DBCol::Peers, Store};
use rand::seq::SliceRandom;
use std::collections::HashMap;
use std::net::SocketAddr;

// BAD -- several groups of imports
use std::collections::HashMap;
use std::net::SocketAddr;

use borsh::BorshSerialize;
use rand::seq::SliceRandom;

use near_primitives::utils::to_timestamp;
use near_store::{DBCol::Peers, Store};

use crate::types::KnownPeerState;
<span class="boring">}
</span></code></pre></pre>
<p><strong>Rationale:</strong> Consistency, ease of automatic enforcement. Today stable rustfmt
can't split imports into groups automatically, and doing that manually
consistently is a chore.</p>
<h3 id="derives"><a class="header" href="#derives">Derives</a></h3>
<p>When deriving an implementation of a trait, specify a full path to the traits provided by the
external libraries:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
#[derive(Copy, Clone, serde::Serialize, thiserror::Error, strum::Display)]
struct Grapefruit;

// BAD
use serde::Serialize;
use thiserror::Error;
use strum::Display;

#[derive(Copy, Clone, Serialize, Error, Display)]
struct Banana;
<span class="boring">}
</span></code></pre></pre>
<p>As an exception to this rule, it is okay to use either style when the derived trait already
includes the name of the library (as would be the case for <code>borsh::BorshSerialize</code>.)</p>
<p><strong>Rationale:</strong> Specifying a full path to the externally provided derivations here makes it
straightforward to differentiate between the built-in derivations and those provided by the
external crates. The surprise factor for derivations sharing a name with the standard
library traits (<code>Display</code>) is reduced and it also acts as natural mechanism to tell apart names
prone to collision (<code>Serialize</code>), all without needing to look up the list of imports.</p>
<h2 id="standard-naming"><a class="header" href="#standard-naming">Standard Naming</a></h2>
<ul>
<li>Use <code>-</code> rather than <code>_</code> in crate names and in corresponding folder names.</li>
<li>Avoid single-letter variable names especially in long functions.  Common <code>i</code>,
<code>j</code> etc. loop variables are somewhat of an exception but since Rust encourages
use of iterators those cases aren’t that common anyway.</li>
<li>Follow standard <a href="https://rust-lang.github.io/api-guidelines/naming.html">Rust naming patterns</a> such as:
<ul>
<li>Don’t use <code>get_</code> prefix for getter methods.  A getter method is one which
returns (reference to) a field of an object.</li>
<li>Use <code>set_</code> prefix for setter methods.  An exception are builder objects
which may use different naming style.</li>
<li>Use <code>into_</code> prefix for methods which consume <code>self</code> and <code>to_</code> prefix for
methods which don’t.</li>
</ul>
</li>
<li>Use <code>get_block_header</code> rather than <code>get_header</code> for methods which return
a block header.</li>
<li>Don’t use <code>_by_hash</code> suffix for methods which lookup chain objects (blocks,
chunks, block headers etc.) by their hash (i.e. their primary identifier).</li>
<li>Use <code>_by_height</code> and similar suffixes for methods which lookup chain objects
(blocks, chunks, block headers etc.) by their height or other property which
is not their hash.</li>
</ul>
<p><strong>Rationale:</strong> Consistency.</p>
<h2 id="documentation"><a class="header" href="#documentation">Documentation</a></h2>
<p>When writing documentation in <code>.md</code> files, wrap lines at approximately 80
columns.</p>
<pre><code class="language-markdown">&lt;!-- GOOD --&gt;
Manually reflowing paragraphs is tedious. Luckily, most editors have this
functionality built in or available via extensions. For example, in Emacs you
can use `fill-paragraph` (&lt;kbd&gt;M-q&lt;/kbd&gt;), (neo)vim allows rewrapping with `gq`,
and VS Code has `stkb.rewrap` extension.

&lt;!-- BAD --&gt;
One sentence per-line is also occasionally used for technical writing.
We avoid that format though.
While convenient for editing, it may be poorly legible in unrendered form

&lt;!-- BAD --&gt;
Definitely don't use soft-wrapping. While markdown mostly ignores source level line breaks, relying on soft wrap makes the source completely unreadable, especially on modern wide displays.
</code></pre>
<h2 id="tracing"><a class="header" href="#tracing"><a href="https://tracing.rs">Tracing</a></a></h2>
<p>When emitting events and spans with <code>tracing</code> prefer adding variable data via
<a href="https://docs.rs/tracing/latest/tracing/#recording-fields"><code>tracing</code>'s field mechanism</a>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// GOOD
debug!(
    target: &quot;client&quot;,
    validator_id = self.client.validator_signer.as_ref().map(|vs| {
        tracing::field::display(vs.validator_id())
    }),
    %hash,
    &quot;block.previous_hash&quot; = %block.header().prev_hash(),
    &quot;block.height&quot; = block.header().height(),
    %peer_id,
    was_requested
    &quot;Received block&quot;,
);
<span class="boring">}
</span></code></pre></pre>
<p>Most apparent violation of this rule will be when the event message utilizes any
form of formatting, as seen in the following example:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// BAD
debug!(
    target: &quot;client&quot;,
    &quot;{:?} Received block {} &lt;- {} at {} from {}, requested: {}&quot;,
    self.client.validator_signer.as_ref().map(|vs| vs.validator_id()),
    hash,
    block.header().prev_hash(),
    block.header().height(),
    peer_id,
    was_requested
);
<span class="boring">}
</span></code></pre></pre>
<p>Always specify the <code>target</code> explicitly. A good default value to use is the crate
name, or the module path (e.g. <code>chain::client</code>) so that events and spans common
to a topic can be grouped together. This grouping can later be used for
customizing of which events to output.</p>
<p><strong>Rationale:</strong> This makes the events structured – one of the major value
propositions of the tracing ecosystem. Structured events allow for immediately
actionable data without additional post-processing, especially when using some
of the more advanced tracing subscribers. Of particular interest would be those
that output events as JSON, or those that publish data to distributed event
collection systems such as opentelemetry. Maintaining this rule will also
usually result in faster execution (when logs at the relevant level are enabled.)</p>
<h3 id="spans"><a class="header" href="#spans">Spans</a></h3>
<p>Use the <a href="https://docs.rs/tracing/latest/tracing/#spans">spans</a> to introduce context and grouping to and between events
instead of manually adding such information as part of the events themselves.
Most of the subscribers ingesting spans also provide a built-in timing facility,
so prefer using spans for measuring the amount of time a section of code needs
to execute.</p>
<p>Give spans simple names that make them both easy to trace back to code, and to
find a particular span in logs or other tools ingesting the span data. If a
span begins at the top of a function, prefer giving it a name of that function,
otherwise prefer a <code>snake_case</code> name.</p>
<p>Use the regular span API over convenience macros such as <code>#[instrument]</code>, as
this allows instrumenting portions of a function without affecting the code
structure:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn compile_and_serialize_wasmer(code: &amp;[u8]) -&gt; Result&lt;wasmer::Module&gt; {
    let _span = tracing::debug_span!(target: &quot;vm&quot;, &quot;compile_and_serialize_wasmer&quot;).entered();
    // ...
    // _span will be dropped when this scope ends, terminating the span created above.
    // You can also `drop` it manually, to end the span early with `drop(_span)`.
}
<span class="boring">}
</span></code></pre></pre>
<p><strong>Rationale:</strong> Much as with events, this makes the information provided by spans
structured and contextual. This information can then be output to tooling in an
industry standard format, and can be interpreted by an extensive ecosystem of
<code>tracing</code> subscribers.</p>
<h3 id="event-and-span-levels"><a class="header" href="#event-and-span-levels">Event and span levels</a></h3>
<p>The <code>INFO</code> level is enabled by default, use it for information useful for node
operators. The <code>DEBUG</code> level is enabled on the canary nodes, use it for
information useful in debugging testnet failures. The <code>TRACE</code> level is not
generally enabled, use it for arbitrary debug output.</p>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<p>Consider adding metrics to new functionality. For example, how often each type
of error was triggered, how often each message type was processed.</p>
<p><strong>Rationale:</strong> Metrics are cheap to increment, and they often provide a significant
insight into operation of the code, almost as much as logging. But unlike logging
metrics don't incur a significant runtime cost.</p>
<h3 id="naming"><a class="header" href="#naming">Naming</a></h3>
<p>Prefix all <code>nearcore</code> metrics with <code>near_</code>.
Follow [https://prometheus.io/docs/practices/naming/](Prometheus naming convention)
for new metrics.</p>
<p><strong>Rationale:</strong> The <code>near_</code> prefix makes it trivial to separate metrics exported
by <code>nearcore</code> from other metrics, such as metrics about the state of the machine
that runs <code>neard</code>.</p>
<h3 id="performance"><a class="header" href="#performance">Performance</a></h3>
<p>In most cases incrementing a metric is cheap enough never to give it a second
thought. However accessing a metric with labels on a hot path needs to be done
carefully.</p>
<p>If a label is based on an integer, use a faster way of converting an integer
to the label, such as the <code>itoa</code> crate.</p>
<p>For hot code paths, re-use results of <code>with_label_values()</code> as much as possible.</p>
<p><strong>Rationale:</strong> We've encountered issues caused by the runtime costs of
incrementing metrics before. Avoid runtime costs of incrementing metrics too
often.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="documentation-1"><a class="header" href="#documentation-1">Documentation</a></h1>
<p>This chapter describes nearcore's approach to documentation. There are three
primary types of documentation to keep in mind:</p>
<ul>
<li><a href="https://nomicon.io"><strong>The NEAR Protocol Specification</strong></a> (<a href="https://github.com/near/NEPs">source</a>) is the
formal description of the NEAR protocol. The reference nearcore implementation
and any other NEAR client implementations must follow this specification.</li>
<li><a href="https://docs.near.org"><strong>User docs</strong></a> (<a href="https://github.com/near/docs">source</a>) explain what is NEAR and
how to participate in the network. In particular, they contain information
pertinent to the users of NEAR: validators and smart contract developers.</li>
<li><a href="https://near.github.io/nearcore/"><strong>Documentation for nearcore developers</strong></a> (<a href="https://github.com/near/nearcore/tree/master/docs">source</a>) is the
book you are reading right now! The target audience here are the contributors
to the main implementation of the NEAR protocol (nearcore).</li>
</ul>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<p>The bulk of the internal docs is within this book. If you want to write some
kind of a document, add it here! The <a href="practices/../architecture/">architecture</a> and
<a href="practices/../practices/">practices</a> chapters are intended for somewhat up-to-date
normative documents. The <a href="practices/../misc/">misc</a> chapter holds everything else.</p>
<p>This book is not intended for user-facing documentation, so don't worry about
proper English, typos, or beautiful diagrams -- just write stuff! It can easily
be improved over time with pull requests. For docs, we use a light-weight review
process and try to merge any improvement as quickly as possible. Rather than
blocking a PR on some stylistic changes, just merge it and submit a follow up.</p>
<p>Note the &quot;edit&quot; button at the top-right corner -- super useful for fixing any
typos you spot!</p>
<p>In addition to the book, we also have some &quot;inline&quot; documentation in the code.
For Rust, it is customary to have a per-crate <code>README.md</code> file and include it as
a doc comment via <code>#![doc = include_str!(&quot;../README.md&quot;)]</code> in <code>lib.rs</code>. We don't
<em>require</em> every item to be documented, but we certainly encourage documenting as
much as possible. If you spend some time refactoring or fixing a function,
consider adding a doc comment (<code>///</code>) to it as a drive-by improvement.</p>
<p>We currently don't render <code>rustdoc</code>, see <a href="https://github.com/near/nearcore/issues/7836">#7836</a>.</p>
<h2 id="book-how-to"><a class="header" href="#book-how-to">Book How To</a></h2>
<p>We use mdBook to render a bunch of markdown files as a static website with table
of contents, search and themes. Full docs are <a href="https://rust-lang.github.io/mdBook/">here</a>, but the basics are
very simple.</p>
<p>To add a new page to the book:</p>
<ol>
<li>Add an <code>.md</code> file somewhere in the
<a href="https://github.com/near/nearcore/tree/master/docs"><code>./docs</code></a> folder.</li>
<li>Add a link to this page to the
<a href="https://github.com/near/nearcore/blob/master/docs/SUMMARY.md"><code>SUMMARY.md</code></a>.</li>
<li>Submit a PR (again, we promise to merge it without much ceremony).</li>
</ol>
<p>The doc itself is vanilla markdown.</p>
<p>To render documentation locally:</p>
<pre><code class="language-console"># Install mdBook
$ cargo install mdbook
$ mdbook serve --open ./docs
</code></pre>
<p>This will generate the book from the docs folder, open it in a browser and
start a file watcher to rebuild the book every time the source files change.</p>
<p>Note that GitHub's default rendering mostly works just as well, so you don't
need to go out of your way to preview your changes when drafting a page or
reviewing pull requests to this book.</p>
<p>The book is deployed via the <a href="https://github.com/near/nearcore/blob/master/.github/workflows/book.yml">book GitHub Action workflow</a>. This workflow
runs mdBook and then deploys the result to <a href="https://docs.github.com/en/pages/getting-started-with-github-pages/about-github-pages">GitHub Pages</a>.</p>
<p>For internal docs, you often want to have pretty pictures. We don't currently
have a recommended workflow, but here are some tips:</p>
<ul>
<li>
<p>Don't add binary media files to Git to avoid inflating repository size.
Rather, upload images as comments to this super-secret issue
<a href="https://github.com/near/nearcore/issues/7821">#7821</a>, and then link to
the images as</p>
<pre><code>![image](https://user-images.githubusercontent.com/1711539/195626792-7697129b-7f9c-4953-b939-0b9bcacaf72c.png)
</code></pre>
<p>Use single comment per page with multiple images.</p>
</li>
<li>
<p>Google Docs is an OK way to create technical drawings, you can add a link to
the doc with source to that secret issue as well.</p>
</li>
<li>
<p>There's some momentum around using mermaid.js for diagramming, and there's
appropriate plugin for that: https://github.com/badboy/mdbook-mermaid.
Consider if that's something you might want to use.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tracking-issues"><a class="header" href="#tracking-issues">Tracking issues</a></h1>
<p><code>nearcore</code> uses so called &quot;tracking issues&quot; to coordinate larger pieces of work
(e.g. implementation of new NEPs).  Such issues are tagged with the
<a href="https://github.com/near/nearcore/issues?q=is%3Aopen+is%3Aissue+label%3AC-tracking-issue"><code>C-tracking-issue</code>
label</a>.</p>
<p>The goal of tracking issues is to serve as a coordination point. They can help
new contributors and other interested parties come up-to-speed with the current
state of projects.  As such, they should link to things like design docs,
todo-lists of sub-issues, existing implementation PRs, etc.</p>
<p>One can further use tracking issues to:</p>
<ul>
<li>get a feeling for what's happening in <code>nearcore</code> by looking at the set of
open tracking issues.</li>
<li>find larger efforts to contribute to as tracking issues usually contain
up-for-grabs todo lists.</li>
<li>follow progress of a specific features by subscribing to the issue on Github.</li>
</ul>
<p>If you are leading or participating in a larger effort, please create a tracking
issue for your work.</p>
<h2 id="guidelines"><a class="header" href="#guidelines">Guidelines</a></h2>
<ul>
<li>Tracking issues should be maintained in the <code>nearcore</code> repository.  If the
projects are security sensitive, then they should be maintained in the
<code>nearcore-private</code> repository.</li>
<li>The issues should be kept up-to-date.  At a minimum, all new context
should be added as comments, but preferably the original description should be
edited to reflect the current status.</li>
<li>The issues should contain links to all the relevant design documents
which should also be kept up-to-date.</li>
<li>The issues should link to any relevant NEP if applicable.</li>
<li>The issues should contain a list of todo tasks which should be kept
up-to-date as new work items are discovered and other items are done.  This
helps others gauge progress and helps lower the barrier of entry for others to
participate.</li>
<li>The issues should contain links to relevant Zulip discussions.  Prefer
open forums like Zulip for discussions.  When necessary, closed forums like
video calls can also be used but care should be taken to document a summary of
the discussions.</li>
<li>For security sensitive discussions, use the appropriate private Zulip streams.</li>
</ul>
<p><a href="https://github.com/near/nearcore/issues/7670">This issue</a> is a good example of
how tracking issues should be maintained.</p>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>The idea of tracking issues is also used to track project work in the Rust
language.  See <a href="https://internals.rust-lang.org/t/how-the-rust-issue-tracker-works/3951">this
post</a>
for a rough description and
<a href="https://github.com/rust-lang/rust/issues/101840">these</a>
<a href="https://github.com/rust-lang/rust/issues/100717">issues</a> for how they are used
in Rust.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="security-vulnerabilities"><a class="header" href="#security-vulnerabilities">Security Vulnerabilities</a></h2>
<blockquote style="background: rgba(255, 200, 0, 0.1); border: 5px solid rgba(255, 200, 0, 0.4);">
<p>The intended audience of the information presented here are developers working
on the implementation of NEAR.</p>
<p>Are you a security researcher?  Please report security vulnerabilities to
<a href="mailto:security@near.org">security@near.org</a>.</p>
</blockquote>
<p>As nearcore is open source, all of its issues and pull requests are also
publicly tracked on github.  However, from time to time, if a security sensitive
issue is discovered, those cannot be tracked publicly on github.  However, we
should promote as similar a development process to work on such issues as
possible.  To enable this, below is the high level process for working on
security sensitive issues.</p>
<ol>
<li>
<p>There is a <a href="https://github.com/near/nearcore-private">private fork of
nearcore</a> on github.  Access to
this repository is restricted to the set of people who are trusted to work on
and have knowledge about security sensitive issues pertaining to nearcore.</p>
<p>This repository can be manually synced with the public nearcore repository
using the following commands:</p>
<pre><code class="language-console">$ git remote add nearcore-public git@github.com:near/nearcore
$ git remote add nearcore-private git@github.com:near/nearcore-private
$ git fetch nearcore-public
$ git push nearcore-private nearcore-public/master:master
</code></pre>
</li>
<li>
<p>All security sensitive issues must be created on the private nearcore
repository.  You must also assign one of the <code>[P-S0, P-S1]</code> labels to the
issue to indicate the severity of the issue.  The two criteria to use to help
you judge the severity are ease of carrying out the attack and the impact of
the attack. An attack that is easy to do or can have a huge impact should
have the <code>P-S0</code> label and <code>P-S1</code> otherwise.</p>
</li>
<li>
<p>All security sensitive pull requests should also be created on the private
nearcore repository.  Note that once a PR has been approved, it should not be
merged into the private repository.  Instead it should be first merged into
the public repository and then the private fork should be updated using the
steps above.</p>
</li>
<li>
<p>Once work on a security issue is finished, it needs to be deployed to all the
impacted networks.  Please contact the node team for help with this.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fast-builds"><a class="header" href="#fast-builds">Fast Builds</a></h1>
<p>nearcore is implemented in Rust and is a fairly sizable project, so it takes a
while to build. This chapter collects various tips to make the development
process faster.</p>
<p>Optimizing build times is a bit of a black art, so please do benchmarks on your
machine to verify that the improvements work for you. Changing some configuration
and making a typo, which prevents it from improving build times is an
extremely common failure mode!</p>
<p><a href="https://nnethercote.github.io/perf-book/compile-times.html">Rust Perf Book</a>
contains a section on compilation times as well!</p>
<h2 id="release-builds-and-link-time-optimization"><a class="header" href="#release-builds-and-link-time-optimization">Release Builds and Link Time Optimization</a></h2>
<p>Obviously, <code>cargo build --release</code> is slower than <code>cargo build</code>. We enable full
lto (link time optimization), so our <code>-r</code> builds are very slow, use a lot of
RAM, and don't utilize the available parallelism fully.</p>
<p>As debug builds are much too slow at runtime for many purposes, we have a custom
profile <code>--profile quick-release</code> which is equivalent to <code>-r</code>, except that time
consuming options such as LTO are disabled.</p>
<p>Use <code>--profile quick-release</code> when doing comparative benchmarking, or when
connecting a locally built node to a network. Use <code>-r</code> if you want to get
absolute performance numbers.</p>
<h2 id="linker"><a class="header" href="#linker">Linker</a></h2>
<p>By default, <code>rustc</code> uses the default system linker, which tends to be quite
slow. Using <code>lld</code> (LLVM linker) or <code>mold</code> (very new, very fast linker) provides
big wins for many setups.</p>
<p>I don't know what's the official source of truth for using alternative linkers,
I usually refer to <a href="https://github.com/rust-lang/rust/issues/39915#issuecomment-538049306">this
comment</a>.</p>
<p>Usually, adding</p>
<pre><code class="language-toml">[build]
rustflags = [&quot;-C&quot;, &quot;link-arg=-fuse-ld=lld&quot;]
</code></pre>
<p>to <code>~/.cargo/config</code> is the most convenient approach.</p>
<p>lld itself can be installed with <code>sudo apt install lld</code>.</p>
<h2 id="prebuilt-rocksdb"><a class="header" href="#prebuilt-rocksdb">Prebuilt RocksDB</a></h2>
<p>By default, we compile RocksDB (a C++ project) from source during the neard
build. By linking to a prebuilt copy of RocksDB this work can be avoided
entirely. This is a huge win, especially if you clean the <code>./target</code> directory
frequently.</p>
<p>In order to use prebuilt RocksDB, set <code>ROCKSDB_LIB_DIR</code> environment variable to
a location containing <code>librocksdb.a</code>:</p>
<pre><code class="language-console">$ export ROCKSDB_LIB_DIR=/usr/lib/x86_64-linux-gnu
$ cargo build -p neard
</code></pre>
<p>Note, that the system must provide a recent version of the library which,
depending on operating system you’re using, may require installing packages from
a testing branch.  For example, on Debian it requires installing
<code>librocksdb-dev</code> from <code>experimental</code> version:</p>
<pre><code class="language-bash">echo 'deb http://ftp.debian.org/debian experimental main contrib non-free' |
    sudo tee -a /etc/apt/sources.list
sudo apt update
sudo apt -t experimental install librocksdb-dev

ROCKSDB_LIB_DIR=/usr/lib/x86_64-linux-gnu
export ROCKSDB_LIB_DIR
</code></pre>
<h2 id="global-compilation-cache"><a class="header" href="#global-compilation-cache">Global Compilation Cache</a></h2>
<p>By default, Rust compiles incrementally, with the incremental cache and
intermediate outputs stored in the project-local <code>./target</code> directory.</p>
<p><a href="https://github.com/mozilla/sccache"><code>sccache</code></a> utility can be used to share
these artifacts between machines or checkouts within the same machine. <code>sccache</code>
works by intercepting calls to <code>rustc</code> and will fetch the cached outputs from
the global cache whenever possible. This tool can be set up as such:</p>
<pre><code class="language-console">$ cargo install sccache
$ export RUSTC_WRAPPER=&quot;sccache&quot;
$ export SCCACHE_CACHE_SIZE=&quot;30G&quot;
$ cargo build -p neard
</code></pre>
<p>Refer to the <a href="https://github.com/mozilla/sccache">project’s README</a> for further
configuration options.</p>
<h2 id="ides-are-bad-for-environment"><a class="header" href="#ides-are-bad-for-environment">IDEs Are Bad For Environment</a></h2>
<p>Generally, the knobs in this section are controlled either via global
configuration in <code>~/.cargo/config</code> or environment variables.</p>
<p>Environment variables are notoriously easy to lose, especially if you are
working both from a command line and graphical IDE. Double check that the
environment within which builds are executed is identical in order to avoid
nasty failure modes such as full cache invalidation when switching
from the CLI to an IDE or vice-versa.</p>
<p><a href="https://direnv.net"><code>direnv</code></a> sometimes can be used to conveniently manage
project-specific environment variables.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-principles"><a class="header" href="#general-principles">General principles</a></h1>
<ol>
<li>Every PR needs to have test coverage in place. Sending the code change and
deferring tests for a future change is not acceptable.</li>
<li>Tests need to either be sufficiently simple to follow, or have good
documentation to explain why certain actions are made and conditions are
expected.</li>
<li>When implementing a PR, <strong>make sure to run the new tests with the change
disabled and confirm that they fail</strong>! It is extremely common to have tests
that pass without the change that is being tested.</li>
<li>The general rule of thumb for a reviewer is to first review the tests, and
ensure that they can convince themselves that the code change that passes the
tests must be correct. Only then the code should be reviewed.</li>
<li>Have the assertions in the tests as specific as possible,
however do not make the tests change-detectors of the concrete implementation.
(assert only properties which are required for correctness).
For example, do not do <code>assert!(result.is_err())</code>, expect the specific error instead.</li>
</ol>
<h1 id="tests-hierarchy"><a class="header" href="#tests-hierarchy">Tests hierarchy</a></h1>
<p>In NEAR Reference Client we largely split tests into three categories:</p>
<ol>
<li>Relatively cheap sanity or fast fuzz tests. It includes all the <code>#[test]</code>
Rust tests not decorated by features. Our repo is configured in such a way
that all such tests are ran on every PR, and failing at least one of them is
blocking the PR from being merged.</li>
</ol>
<p>To run such tests locally run <code>cargo nextest run --all</code>.
It requires nextest harness which can be installed by running <code>cargo install cargo-nextest</code> first.</p>
<ol start="2">
<li>Expensive tests. This includes all the fuzzy tests that run many iterations,
as well as tests that spin up multiple nodes and run them until they reach a
certain condition. Such tests are decorated with
<code>#[cfg(feature=&quot;expensive-tests&quot;)]</code>. It is not trivial to enable features
that are not declared in the top level crate, and thus the easiest way to run
such tests is to enable all the features by passing <code>--all-features</code> to
<code>cargo nextest run</code>, e.g:</li>
</ol>
<p><code>cargo nextest run --package near-client --test cross_shard_tx tests::test_cross_shard_tx --all-features</code></p>
<ol start="3">
<li>Python tests. We have an infrastructure to spin up nodes, both locally and
remotely, in python, and interact with them using RPC. The infrastructure and
the tests are located in <code>pytest</code> folder. The infrastructure is relatively
straightforward, see for example <code>block_production.py</code>
<a href="https://github.com/nearprotocol/nearcore/blob/master/pytest/tests/sanity/block_production.py">here</a>.
See the <code>Test infrastructure</code> section below for details.</li>
</ol>
<p>Expensive and python tests are not part of CI, and are run by a custom nightly
runner. The results of the latest runs are available
<a href="http://nightly.neartest.com/">here</a>. With today tests runs launch approximately
every 5-6 hours. For the latest results look at the <strong>second</strong> run, since the
first one has some tests still scheduled to run.</p>
<h1 id="test-infrastructure"><a class="header" href="#test-infrastructure">Test infrastructure</a></h1>
<p>Different levels of the reference implementation have different infrastructure
available to test them.</p>
<h2 id="client"><a class="header" href="#client">Client</a></h2>
<p>Client is separated from the runtime via a <code>RuntimeAdapter</code> trait. In production
it uses <code>NightshadeRuntime</code> that uses real runtime and epoch managers. To test
client without instantiating runtime and epoch manager, we have a mock runtime
<code>KeyValueRuntime</code>.</p>
<p>Most of the tests in the client work by setting up either a single node (via
<code>setup_mock()</code>) or multiple nodes (via <code>setup_mock_all_validators()</code>) and then
launching the nodes and waiting for a particular message to occur, with a
predefined timeout.</p>
<p>For the most basic example of using this infrastructure see <code>produce_two_blocks</code>
in
<a href="https://github.com/nearprotocol/nearcore/blob/master/chain/client/tests/process_blocks.rs"><code>tests/process_blocks.rs</code></a>.</p>
<ol>
<li>The callback (<code>Box::new(move |msg, _ctx, _| { ...</code>) is what is executed
whenever the client sends a message. The return value of the callback is sent
back to the client, which allows testing relatively complex scenarios. The
tests generally expect a particular message to occur, in this case the tests
expects two blocks to be produced. <code>System::current().stop();</code> is the way to
stop the test and mark it as passed.</li>
<li><code>near_network::test_utils::wait_or_panic(5000);</code> is how the timeout for the
test is set (in milliseconds).</li>
</ol>
<p>For an example of a test that launches multiple nodes, see
<code>chunks_produced_and_distributed_common</code> in
<a href="https://github.com/nearprotocol/nearcore/blob/master/chain/client/tests/chunks_management.rs">tests/chunks_management.rs</a>.
The <code>setup_mock_all_validators</code> function is the key piece of infrastructure here.</p>
<h2 id="runtime"><a class="header" href="#runtime">Runtime</a></h2>
<p>Tests for Runtime are listed in
<a href="https://github.com/near/nearcore/blob/master/tests/test_cases_runtime.rs">tests/test_cases_runtime.rs</a>.</p>
<p>To run a test, usually a mock <code>RuntimeNode</code> is created via
<code>create_runtime_node()</code>. In its constructor the <code>Runtime</code> is created in the
<code>get_runtime_and_trie_from_genesis</code> function.</p>
<p>Inside a test an abstraction <code>User</code> is used for sending specific actions to the
runtime client. The helper functions <code>function_call</code>, <code>deploy_contract</code>, etc.
eventually lead to the <code>Runtime.apply</code> method call.</p>
<p>For setting usernames during playing with transactions, use default names
<code>alice_account</code>, <code>bob_account</code>, <code>eve_dot_alice_account</code>, etc.</p>
<h2 id="network"><a class="header" href="#network">Network</a></h2>
<p>TODO: explain the <code>runner</code> here</p>
<h2 id="chain-epoch-manager-runtime-and-other-low-level-changes"><a class="header" href="#chain-epoch-manager-runtime-and-other-low-level-changes">Chain, Epoch Manager, Runtime and other low level changes</a></h2>
<p>When building new features in the <code>chain</code>, <code>epoch_manager</code>, <code>network</code>, make sure
to build new components sufficiently abstract so that they can be tested without
relying on other components.</p>
<p>For example, see tests for doomslug
<a href="https://github.com/nearprotocol/nearcore/blob/master/chain/chain/tests/doomslug.rs">here</a>,
for network cache
<a href="https://github.com/nearprotocol/nearcore/blob/master/chain/network/tests/cache_edges.rs">here</a>,
or for promises in runtime
<a href="https://github.com/nearprotocol/nearcore/blob/master/runtime/near-vm-logic/tests/test_promises.rs">here</a>.</p>
<h2 id="python-tests"><a class="header" href="#python-tests">Python tests</a></h2>
<p>See <a href="https://github.com/nearprotocol/nearcore/wiki/Writing-integration-tests-for-nearcore">this
page</a>
for a detailed coverage of how to write a python test.</p>
<p>We have a python library that allows one to create and run python tests.</p>
<p>To run python tests, from the <code>nearcore</code> repo the first time do the following:</p>
<pre><code>cd pytest
virtualenv . --python=python3
pip install -r requirements.txt
. .env/bin/activate
python tests/sanity/block_production.py
</code></pre>
<p>After the first time:</p>
<pre><code>cd pytest
. .env/bin/activate
python tests/sanity/block_production.py
</code></pre>
<p>Use <code>pytest/tests/sanity/block_production.py</code> as the basic example of starting a
cluster with multiple nodes, and doing RPC calls.</p>
<p>See <code>pytest/tests/sanity/deploy_call_smart_contract.py</code> to see how contracts can
be deployed, or transactions called.</p>
<p>See <code>pytest/tests/sanity/staking1.py</code> to see how staking transactions can be
issued</p>
<p>See <code>pytest/tests/sanity/state_sync.py</code> to see how to delay the launch of the
whole cluster by using <code>init_cluster</code> instead of <code>start_cluster</code>, and then
launching nodes manually.</p>
<h3 id="enabling-adversarial-behavior"><a class="header" href="#enabling-adversarial-behavior">Enabling adversarial behavior</a></h3>
<p>To allow testing adversarial behavior, or generally behaviors that a node should
not normally exercise, we have certain features in the code decorated with
<code>#[cfg(feature=&quot;adversarial&quot;)]</code>. The binary normally is compiled with the
feature disabled, and when compiled with the feature enabled, it traces a
warning on launch.</p>
<p>The nightly runner runs all the python tests against the binary compiled with
the feature enabled, and thus the python tests can make the binary perform
actions that it normally would not perform.</p>
<p>The actions can include lying about the known chain height, producing multiple
blocks for the same height, or disabling doomslug.</p>
<p>See all the tests under <code>pytest/tests/adversarial</code> for the examples.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="python-tests-1"><a class="header" href="#python-tests-1">Python Tests</a></h1>
<p>To simplify writing integration tests for nearcore we have a python
infrastructure that allows writing a large variety of tests that run small local
clusters, remove clusters, or run against full scale live deployments.</p>
<p>Such tests are written in python and not in Rust (in which the nearcore itself,
and most of the sanity and fuzz tests, are written) due to availability of
libraries to easily connect to remove nodes, and orchestrate cloud instances.</p>
<p>Nearcore itself has several features guarded by a
<a href="https://doc.rust-lang.org/1.29.0/book/first-edition/conditional-compilation.html">feature-flag</a>
that allow the python tests to invoke behaviors otherwise impossible to be
exercised by an honest actor.</p>
<h1 id="basics-1"><a class="header" href="#basics-1">Basics</a></h1>
<p>The infrastructure is located in <code>{nearcore}/pytest/lib</code> and the tests
themselves in subdirs of <code>{nearcore}/pytest/tests</code>. To prepare local machine to
run the tests you'd need python3 (python 3.7), and have several dependencies
installed, for which we recommend using virtualenv:</p>
<pre><code>cd pytest
virtualenv .env --python=python3
. .env/bin/activate
pip install -r requirements.txt
</code></pre>
<p>The tests are expected to be ran from the <code>pytest</code> dir itself. For example, once
the virtualenv is configured:</p>
<pre><code>cd pytest
. .env/bin/activate
python tests/sanity/block_production.py
</code></pre>
<p>will run the most basic tests that spins up a small cluster locally and waits
until it produces several blocks.</p>
<h2 id="compiling-the-client-for-tests"><a class="header" href="#compiling-the-client-for-tests">Compiling the client for tests</a></h2>
<p>The local tests by default expect the binary to be in the default location for a
debug build (<code>{nearcore}/target/debug</code>). Some tests might also expect
test-specific features guarded by a feature-flag to be available. To compile the
binary with such features run</p>
<pre><code>cargo build -p neard --features=adversarial
</code></pre>
<p>The feature is called <code>adversarial</code> to highlight that many functions it enables,
outside of tests, would constitute a malicious behavior. The node compiled with
such a flag will not start unless an environment variable <code>ADVERSARY_CONSENT=1</code>
is set and prints a noticeable warning when starts, thus minimizing the chance
that an honest participant accidentally launches a node compiled with such
functionality.</p>
<p>You can change the way the tests run (locally or using Google Cloud), and where
the local tests look for binary by supplying a config file. For example, if you
want to run tests against a release build, you can create a file with the
following config:</p>
<pre><code>{&quot;local&quot;: True, &quot;near_root&quot;: &quot;../target/release/&quot;}
</code></pre>
<p>and run the test with the following command line:</p>
<pre><code>NEAR_PYTEST_CONFIG=&lt;path to config&gt; python tests/sanity/block_production.py
</code></pre>
<h1 id="writing-tests"><a class="header" href="#writing-tests">Writing tests</a></h1>
<p>We differentiate between &quot;regular&quot; tests, or tests that spin up their own
cluster, either local or on the cloud, and &quot;mocknet&quot; tests, or tests that run
against an existing live deployment of NEAR.</p>
<p>In both cases the test starts by importing the infrastructure and starting or
connecting to a cluster</p>
<h2 id="starting-a-cluster"><a class="header" href="#starting-a-cluster">Starting a cluster</a></h2>
<p>In the simplest case a regular test starts by starting a cluster. The cluster
will run locally by default, but can be spun up on the cloud by supplying
corresponding config.</p>
<pre><code>import sys
sys.path.append('lib')
from cluster import start_cluster

nodes = start_cluster(4, 0, 4, None, [[&quot;epoch_length&quot;, 10], [&quot;block_producer_kickout_threshold&quot;, 80]], {})
</code></pre>
<p>In the example above the first three parameters are <code>num_validating_nodes</code>,
<code>num_observers</code>, <code>num_shards</code>. The third parameter is a config, which generally
should be <code>None</code>, in which case the config is picked up from the environment
variable as shown above.</p>
<p><code>start_cluster</code> will spin up <code>num_validating_nodes</code> nodes that are block
producers (with pre-staked tokens), <code>num_observers</code> non-validating nodes and
will configure the system to have <code>num_shards</code> shards. The fifth argument
changes the genesis config. Each element is a list of some length <code>n</code> where the
first <code>n-1</code> elements are a path in the genesis JSON file, and the last element
is the value. You'd often want to significantly reduce the epoch length, so that
your test triggers epoch switches, and reduce the kickout threshold, since with
shorter epochs it is easier for a block producer to get kicked out.</p>
<p>The last parameter is a dictionary from the node ordinal to changes to their
local config.</p>
<p>Note that <code>start_cluster</code> spins up all the nodes right away. Some tests (e.g.
tests that test syncing) might want to configure the nodes, but delay their
start. In such a case you will initialize the cluster by calling to
<code>init_cluster</code>, and will run the nodes manually, for example see
<a href="https://github.com/nearprotocol/nearcore/blob/master/pytest/tests/sanity/state_sync.py"><code>state_sync.py</code></a></p>
<h2 id="connecting-to-a-mocknet"><a class="header" href="#connecting-to-a-mocknet">Connecting to a mocknet</a></h2>
<p>Nodes that run against a mocknet would connect to an existing cluster instead of
running their own.</p>
<pre><code>import sys
sys.path.append('lib')
from cluster import connect_to_mocknet

nodes, accounts = connect_to_mocknet(None)
</code></pre>
<p>The only parameter is a config, with <code>None</code> meaning to use the config from the
environment variable. The config should have the following format:</p>
<pre><code>{
    &quot;nodes&quot;: [
        {&quot;ip&quot;: &quot;(some_ip)&quot;, &quot;port&quot;: 3030},
        {&quot;ip&quot;: &quot;(some_ip)&quot;, &quot;port&quot;: 3030},
        {&quot;ip&quot;: &quot;(some_ip)&quot;, &quot;port&quot;: 3030},
        {&quot;ip&quot;: &quot;(some_ip)&quot;, &quot;port&quot;: 3030}
    ],
    &quot;accounts&quot;: [
        {&quot;account_id&quot;: &quot;node1&quot;, &quot;pk&quot;: &quot;ed25519:&lt;public key&gt;&quot;, &quot;sk&quot;: &quot;edd25519:&lt;secret key&gt;&quot;},
        {&quot;account_id&quot;: &quot;node2&quot;, &quot;pk&quot;: &quot;ed25519:&lt;public key&gt;&quot;, &quot;sk&quot;: &quot;edd25519:&lt;secret key&gt;&quot;}
    ]
}
</code></pre>
<h2 id="manipulating-nodes"><a class="header" href="#manipulating-nodes">Manipulating nodes</a></h2>
<p>The nodes returned by <code>start_cluster</code> and <code>init_cluster</code> have certain
convenience functions. You can see the full interface in
<code>{nearcore}/pytest/lib/cluster.py</code>.</p>
<p><code>start(boot_public_key, (boot_ip, boot_port))</code> starts the node. If both
arguments are <code>None</code>, the node will start as a boot node (note that the concept
of a &quot;boot node&quot; is relatively vague in a decentralized system, and from the
perspective of the tests the only requirement is that the graph of &quot;node A
booted from node B&quot; is connected).</p>
<p>The particular way to get the <code>boot_ip</code> and <code>boot_port</code> when launching <code>node1</code>
with <code>node2</code> being its boot node is the following:</p>
<pre><code>node1.start(node2.node_key.pk, node2.addr())
</code></pre>
<p><code>kill()</code> shuts down the node by sending it <code>SIGKILL</code></p>
<p><code>reset_data()</code> cleans up the data dir, which could be handy between the calls to
<code>kill</code> and <code>start</code> to see if a node can start from a clean state.</p>
<p>Nodes on the mocknet do not expose <code>start</code>, <code>kill</code> and <code>reset_data</code>.</p>
<h2 id="issuing-rpc-calls"><a class="header" href="#issuing-rpc-calls">Issuing RPC calls</a></h2>
<p>Nodes in both regular and mocknet tests expose an interface to issue RPC calls.
In the most generic case one can just issue raw JSON rpc call by calling
<code>json_rpc</code> method:</p>
<pre><code>validator_info = nodes[0].json_rpc('validators', [&lt;some block_hash&gt;])
</code></pre>
<p>For the most popular calls there are convenience functions:</p>
<ul>
<li><code>send_tx</code> sends a signed transaction asynchronously</li>
<li><code>send_tx_and_waits</code> sends a signed transaction synchronously</li>
<li><code>get_status</code> returns the current status (the output of the `/status/
endpoint), which contains e.g. last block hash and height</li>
<li><code>get_tx</code> returns a transaction by the transaction hash and the recipient ID.</li>
</ul>
<p>See all the methods in <code>{nearcore}/pytest/lib/cluster.rs</code> after the definition
of the <code>json_rpc</code> method.</p>
<h3 id="signing-and-sending-transactions"><a class="header" href="#signing-and-sending-transactions">Signing and sending transactions</a></h3>
<p>There are two ways to send a transaction. A synchronous way (<code>send_tx_and_wait</code>)
sends a tx and blocks the test execution until either the TX is finished, or the
timeout is hit. An asynchronous way (<code>send_tx</code> + <code>get_tx</code>) sends a TX and then
verifies its result later. Here's an end-to-end example of sending a
transaction:</p>
<pre><code># the tx needs to include one of the recent hashes
last_block_hash = nodes[0].get_status()['sync_info']['latest_block_hash']
last_block_hash_decoded = base58.b58decode(last_block_hash.encode('utf8'))

# sign the actual transaction
# `fr` and `to` in this case are instances of class `Key`.
# In mocknet tests the list `Key`s for all the accounts are returned by `connect_to_mocknet`
# In regular tests each node is associated with a single account, and its key is stored in the
# `signer_key` field (e.g. `nodes[0].signer_key`)
# `15` in the example below is the nonce. Nonces needs to increase for consecutive transactions
# for the same sender account.
tx = sign_payment_tx(fr, to.account_id, 100, 15, last_block_hash_decoded)

# Sending the transaction synchronously. `10` is the timeout in seconds. If after 10 seconds the
# outcome is not ready, throws an exception
if want_sync:
    outcome = nodes[0].send_tx_and_wait(tx, 10)

# Sending the transaction asynchronously.
if want_async:
    tx_hash = nodes[from_ordinal % len(nodes)].send_tx(tx)['result']

    # and then sometime later fetch the result...
    resp = nodes[0].get_tx(tx_hash, to.account_id, timeout=1)
    # and see if the tx has finished
    finished = 'result' in resp and 'receipts_outcome' in resp['result'] and len(resp['result']['receipts_outcome']) &gt; 0
</code></pre>
<p>See
<a href="https://github.com/nearprotocol/nearcore/blob/master/pytest/tests/sanity/rpc_tx_forwarding.py">rpc_tx_forwarding.py</a>
for an example of signing and submitting a transaction.</p>
<h2 id="adversarial-behavior"><a class="header" href="#adversarial-behavior">Adversarial behavior</a></h2>
<p>Some tests need certain nodes in the cluster to exercise behavior that is
impossible to be invoked by a honest node. For such tests we provide
functionality that is protected by an &quot;adversarial&quot; feature flag.</p>
<p>It's an advanced feature, a more thorough documentation is a TODO. Most of the
tests that depend on the feature flag enabled are under
<code>{nearcore}/pytest/tests/adversarial</code>, refer to them for how such features can
be used. Search for code in the <code>nearcore</code> codebase guarded by the &quot;adversarial&quot;
feature flag for the example of how such features are added and exposed.</p>
<h2 id="interfering-with-the-network"><a class="header" href="#interfering-with-the-network">Interfering with the network</a></h2>
<p>We have a library that allows running a proxy in front of each node that would
intercept all the messages between nodes, deserialize them in python and run a
handler on each one. The handler can then either let the message pass (<code>return True</code>), drop it (<code>return False</code>) or replace it (<code>return &lt;new message&gt;</code>).</p>
<p>This technique can be used to both interfere with the network (by dropping or
replacing messages), and to inspect messages that flow through the network
without interfering with it. For the latter note that the handler for each node
runs in a separate <code>Process</code>, and thus you need to use <code>multiprocessing</code>
primitives if you want the handlers to exchange information with the main test
process, or between each other.</p>
<p>See the tests <code>tests/sanity/proxy_*.py</code> for examples.</p>
<h1 id="contributing-tests"><a class="header" href="#contributing-tests">Contributing tests</a></h1>
<p>We always welcome new tests, especially python tests that use the above
infrastructure. We have a list of test requests
<a href="https://github.com/nearprotocol/nearcore/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+test%22+">here</a>,
but also welcome any other tests that test aspects of the network we haven't
though about.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cheat-sheet--overview-of-testing-utils"><a class="header" href="#cheat-sheet--overview-of-testing-utils">Cheat-sheet / overview of testing utils</a></h1>
<p>This page covers the different testing utils / libraries that we have for easier unittesting in Rust.</p>
<h2 id="basics-2"><a class="header" href="#basics-2">Basics</a></h2>
<h3 id="cryptohash"><a class="header" href="#cryptohash">CryptoHash</a></h3>
<p>To create a new cryptohash:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>&quot;ADns6sqVyFLRZbSMCGdzUiUPaDDtjTmKCWzR8HxWsfDU&quot;.parse().unwrap()
<span class="boring">}
</span></code></pre></pre>
<h3 id="account"><a class="header" href="#account">Account</a></h3>
<p>Also prefer doing parse + unwrap</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let alice: AccountId = &quot;alice.near&quot;.parse().unwrap()
<span class="boring">}
</span></code></pre></pre>
<h3 id="signatures"><a class="header" href="#signatures">Signatures</a></h3>
<p>In memory signer (generates the key based on seed). There is a slight preference to use the seed that is matching the account name.</p>
<p>This will create a signer for account 'test' using 'test' as a seed.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let signer: InMemoryValidatorSigner = create_test_signer(&quot;test&quot;);
<span class="boring">}
</span></code></pre></pre>
<h3 id="block"><a class="header" href="#block">Block</a></h3>
<p>Use <code>TestBlockBuilder</code> to create the block that you need. This class allows you to set custom values for most of the fields.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let test_block = test_utils::TestBlockBuilder::new(prev, signer).height(33).build();
<span class="boring">}
</span></code></pre></pre>
<h2 id="store"><a class="header" href="#store">Store</a></h2>
<p>Use the in memory test store in tests:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let store = create_test_store();
<span class="boring">}
</span></code></pre></pre>
<h2 id="runtime-1"><a class="header" href="#runtime-1">Runtime</a></h2>
<p>You can use the KeyValueRuntime (instead of the Nightshade one):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>KeyValueRuntime::new(store, epoch_length)
<span class="boring">}
</span></code></pre></pre>
<h2 id="epochmanager"><a class="header" href="#epochmanager">EpochManager</a></h2>
<p>Currently still embedded into Runtime - see above to use the KeyValueRuntime</p>
<h2 id="chain"><a class="header" href="#chain">Chain</a></h2>
<p>No fakes or mock.</p>
<h3 id="chain-genesis"><a class="header" href="#chain-genesis">Chain genesis</a></h3>
<p>We have a test method:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ChainGenesis::test()
<span class="boring">}
</span></code></pre></pre>
<h2 id="client-1"><a class="header" href="#client-1">Client</a></h2>
<p>TestEnv - for testing multiple clients (without network)</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>TestEnvBuilder::new(genesis).client(vec![&quot;aa&quot;]).validators(..).runtime_adapters(..).build()
<span class="boring">}
</span></code></pre></pre>
<h2 id="network-1"><a class="header" href="#network-1">Network</a></h2>
<h3 id="peermanager"><a class="header" href="#peermanager">PeerManager</a></h3>
<p>To create a PeerManager handler:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let pm = peer_manager::testonly::start(...).await;
<span class="boring">}
</span></code></pre></pre>
<p>To connect to others:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pm.connect_to(&amp;pm2.peer_info).await;
<span class="boring">}
</span></code></pre></pre>
<h3 id="events-handling"><a class="header" href="#events-handling">Events handling</a></h3>
<p>To wait / handle a given event (as a lot of network code is running in async fashion):</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pm.events.recv_util(|event| match event {...}).await
<span class="boring">}
</span></code></pre></pre>
<h2 id="end-to-end"><a class="header" href="#end-to-end">End to End</a></h2>
<h3 id="chain-runtime-signer"><a class="header" href="#chain-runtime-signer">chain, runtime, signer</a></h3>
<p>in chain/chain/src/test_utils.rs:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Creates 1-validator (test):  chain, KVRuntime and a signer
let (chain, runtime, signer) = setup() 
<span class="boring">}
</span></code></pre></pre>
<h3 id="block-client-actor-view-client"><a class="header" href="#block-client-actor-view-client">block, client actor, view client</a></h3>
<p>in chain/client/src/test_utils.rs</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let (block, client, view_client) = setup(MANY_FIELDS)
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="protocol-upgrade"><a class="header" href="#protocol-upgrade">Protocol Upgrade</a></h2>
<p>This document describes the entire cycle of how a protocol upgrade is done, from
the initial PR to the final release. It is important for everyone who
contributes to the development of the protocol and its client(s) to understand
this process.</p>
<h3 id="background-1"><a class="header" href="#background-1">Background</a></h3>
<p>At NEAR, we use protocol version to mean the version of the blockchain protocol
and is separate from the version of some specific client (such as nearcore),
since the protocol version defines the protocol rather than some specific
implementation of the protocol. More concretely, for each epoch, there is a
corresponding protocol version that is agreed upon by validators through <a href="https://github.com/near/NEPs/blob/master/specs/ChainSpec/Upgradability.md">a
voting
mechanism</a>.
Our upgrade scheme dictates that protocol version X is backward compatible with
protocol version X-1, so that nodes in the network can seamlessly upgrade into
the new protocol. However, there is <strong>no guarantee</strong> that protocol version X is
backward compatible with protocol version X-2.</p>
<p>Despite the upgrade mechanism, rolling out a protocol change can be scary,
especially if the change is invasive. For those changes, we may want to have
several months of testing before we are confident that the change itself works
and that it doesn't break other parts of the system.</p>
<h3 id="nightly-protocol-features"><a class="header" href="#nightly-protocol-features">Nightly Protocol features</a></h3>
<p>To make protocol upgrades more robust, we introduce the concept of nightly
protocol version together with the protocol feature flags to allow easy testing
of the cutting-edge protocol changes without jeopardizing the stability of the
codebase overall. In <code>Cargo.toml</code> file of the crates we have in nearcore, we
introduce rust compile-time features <code>nightly_protocol</code> and <code>nightly</code></p>
<pre><code class="language-toml">nightly_protocol = []
nightly = [
    &quot;nightly_protocol&quot;,
    ...
]
</code></pre>
<p>where <code>nightly_protocol</code> is a marker feature that indicates that we are on
nightly protocol whereas <code>nightly</code> is a collection of new protocol features
which also implies <code>nightly_protocol</code>. For example, when we introduce EVM as a
new protocol change, suppose the current protocol version is 40, then we would
do the following change in Cargo.toml:</p>
<pre><code class="language-toml">nightly_protocol = []
nightly = [
    &quot;nightly_protocol&quot;,
    &quot;protocol_features_evm&quot;,
    ...
]
</code></pre>
<p>In <a href="practices/../core/primitives/src/version.rs">core/primitives/src/version.rs</a>, we would
change the protocol version by:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(feature = “nightly_protocol”)]
pub const PROTOCOL_VERSION: u32 = 100;
#[cfg(not(feature = “nightly_protocol”)]
pub const PROTOCOL_VERSION: u32 = 40;
<span class="boring">}
</span></code></pre></pre>
<p>This way the stable versions remain unaffected after the change. Note that
nightly protocol version intentionally starts at a much higher number to make
the distinction between stable protocol and nightly protocol more clear.</p>
<p>To determine whether a protocol feature is enabled, we do the following:</p>
<ul>
<li>We maintain a <code>ProtocolFeature</code> enum where each variant corresponds to some
protocol feature. For nightly protocol features, the variant is gated by the
corresponding rust compile-time feature.</li>
<li>We implement a function <code>protocol_version</code> to return, for each variant, the
corresponding protocol version in which the feature is enabled.</li>
<li>When we need to decide whether to use the new feature based on the protocol
version of the current network, we can simply compare it to the protocol
version of the feature. To make this simpler, we also introduced a macro
<code>checked_feature</code></li>
</ul>
<p>For more details, please refer to
<a href="practices/../core/primitives/src/version.rs">core/primitives/src/version.rs</a>.</p>
<h3 id="feature-gating"><a class="header" href="#feature-gating">Feature Gating</a></h3>
<p>It is worth mentioning that there are two types of checks related to protocol features:</p>
<ul>
<li>For stable features, we check whether they should be enabled by checking the
protocol version of the current epoch. This does not involve any rust
compile-time features.</li>
<li>For nightly features, we have both the check of protocol version and the rust
compile-time feature gating.</li>
</ul>
<h3 id="testing-1"><a class="header" href="#testing-1">Testing</a></h3>
<p>Nightly protocol features allow us to enable the most bleeding-edge code in some
testing environment. We can choose to enable all nightly protocol features by</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>cargo build -p neard --release --features nightly
<span class="boring">}
</span></code></pre></pre>
<p>or enable some specific protocol feature by</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>cargo build -p neard --release --features nightly_protocol,&lt;protocol_feature&gt;
<span class="boring">}
</span></code></pre></pre>
<p>In practice, we have all nightly protocol features enabled for Nayduck tests and
on betanet, which is updated daily.</p>
<h3 id="feature-stabilization"><a class="header" href="#feature-stabilization">Feature Stabilization</a></h3>
<p>New protocol features are introduced first as nightly features and when the
author of the feature thinks that the feature is ready to be stabilized, they
should submit a pull request to stabilize the feature using <a href="practices/../.github/PULL_REQUEST_TEMPLATE/feature_stabilization.html">this
template</a>. In this
pull request, they should the feature gating, increase the <code>PROTOCOL_VERSION</code>
constant (if it hasn't been increased since the last release), and change the
<code>protocol_version</code> implementation to map the stabilized features to the new
protocol version.</p>
<p>A feature stabilization request must be approved by at least <strong>two</strong> <a href="https://github.com/orgs/near/teams/nearcore-codeowners">nearcore
code owners</a> Unless it
is a security related fix, a protocol feature cannot be included in any release
until at least <strong>one</strong> week after its stabilization. This is to ensure that
feature implementation and stabilization are not rushed.</p>
<div style="break-before: page; page-break-before: always;"></div><p>This document describes the advanced network options that you can configure by modifying
the &quot;network&quot; section of your &quot;config.json&quot; file.</p>
<h3 id="tier1-network"><a class="header" href="#tier1-network">TIER1 network</a></h3>
<p>Participants of the BFT consensus (block &amp; chunk producers) now can establish direct (aka TIER1) connections
between each other, which will optimize the communication latency and minimize the number of dropped chunks. 
If you are a validator, you can enable TIER1 connections by setting the following fields in the config:</p>
<ul>
<li><a href="https://github.com/near/nearcore/blob/d95a5f58d998c69cb8d4e965ad6b0a440cf3f233/chain/network/src/config_json.rs#L154">public_addrs</a>
<ul>
<li>this is a list of the public addresses (in the format <code>&quot;&lt;node public key&gt;@&lt;IP&gt;:&lt;port&gt;&quot;</code>) of trusted nodes,
which are willing to route messages to your node</li>
<li>this list will be broadcasted to the network, so that other validator nodes can connect to your node.</li>
<li>if your node has a static public IP, set <code>public_addrs</code> to a list with a single entry with public key and address of your node, for example:
<code>&quot;public_addrs&quot;: [&quot;ed25519:86EtEy7epneKyrcJwSWP7zsisTkfDRH5CFVszt4qiQYw@31.192.22.209:24567&quot;]</code>.</li>
<li>if your node doesn't have a public IP (for example, it is hidden behind a NAT), set <code>public_addrs</code> to
a list (&lt;=10 entries) of proxy nodes that you trust (arbitrary nodes with static public IPs).</li>
<li>support for nodes with dynamic public IPs is not implemented yet.</li>
</ul>
</li>
<li><a href="https://github.com/near/nearcore/blob/d95a5f58d998c69cb8d4e965ad6b0a440cf3f233/chain/network/src/config_json.rs#L213">experimental.tier1_enable_outbound</a>
<ul>
<li>makes your node actively try to establish outbound TIER1 connections (recommended) once it learns about
public addresses of other validator nodes. If disabled, your node won't try to establish outbound TIER1 connections,
but it still may accept incoming TIER1 connections from other nodes.</li>
<li>currently <code>false</code> by default, but will be changed to <code>true</code> by default in the future</li>
</ul>
</li>
<li><a href="https://github.com/near/nearcore/blob/d95a5f58d998c69cb8d4e965ad6b0a440cf3f233/chain/network/src/config_json.rs#L209">experimental.tier1_enable_inbound</a>
<ul>
<li>makes your node accept inbound TIER1 connections from other validator nodes.</li>
<li>disable both <code>tier1_enable_inbound</code> and <code>tier1_enable_outbound</code> if you want to opt out from the TIER1 communication entirely</li>
<li>disable <code>tier1_enable_inbound</code> if you are not a validator AND you don't want your node to act as a proxy for validators.</li>
<li><code>true</code> by default</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-3"><a class="header" href="#overview-3">Overview</a></h1>
<p>This chapter holds various assorted bits of docs. If you want to document
something, but don't know where to put it, put it here!</p>
<h2 id="crate-versioning-and-publishing"><a class="header" href="#crate-versioning-and-publishing">Crate Versioning and Publishing</a></h2>
<p>While all the crates in the workspace are directly unversioned (<code>v0.0.0</code>), they
all share a unified variable version in the <a href="misc/Cargo.toml">workspace manifest</a>.
This keeps versions consistent across the workspace and informs their versions
at the moment of publishing.</p>
<p>We also have CI infrastructure set up to automate the publishing process to
crates.io.  So, on every merge to master, if there's a version change, it is
automatically applied to all the crates in the workspace and it attempts to
publish the new versions of all non-private crates.  All crates that should be
exempt from this process should be marked <code>private</code>.  That is, they should have
the <code>publish = false</code> specification in their package manifest.</p>
<p>This process is managed by
<a href="https://github.com/pksunkara/cargo-workspaces">cargo-workspaces</a>, with a <a href="https://github.com/pksunkara/cargo-workspaces/compare/master...miraclx:grouping-and-exclusion#files_bucket">bit
of
magic</a>
sprinkled on top.</p>
<h2 id="issue-labels"><a class="header" href="#issue-labels">Issue Labels</a></h2>
<p>Issue labels are of the following format <code>&lt;type&gt;-&lt;content&gt;</code> where <code>&lt;type&gt;</code> is a
capital letter indicating the type of the label and <code>&lt;content&gt;</code> is a hyphened
phrase indicating what is label is about.  For example, in the label <code>C-bug</code>,
<code>C</code> means category and <code>bug</code> means that the label is about bugs.  Common types
include <code>C</code>, which means category, <code>A</code>, which means area, <code>T</code>, which means team.</p>
<p>An issue can have multiple labels including which area it touches, which team
should be responsible for the issue, and so on.  Each issue should have at least
one label attached to it after it is triaged and the label could be a general
one, such as <code>C-enhancement</code> or <code>C-bug</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="on-disk-database-format"><a class="header" href="#on-disk-database-format">On-Disk Database Format</a></h1>
<p>We store the database in RocksDB. This document is an attempt to give hints about how to navigate it.</p>
<h2 id="rocksdb"><a class="header" href="#rocksdb">RocksDB</a></h2>
<ul>
<li>The column families are defined in <code>DBCol</code>, defined in <code>core/store/src/columns.rs</code></li>
<li>The column families are seen on the rocksdb side as per the <code>col_name</code> function defined in <code>core/store/src/db/rocksdb.rs</code></li>
</ul>
<h2 id="the-trie-col5"><a class="header" href="#the-trie-col5">The Trie (col5)</a></h2>
<ul>
<li>The trie is stored in column family <code>State</code>, number 5</li>
<li>In this family, each key is of the form <code>ShardUId | CryptoHash</code> where <code>ShardUId: u64</code> and <code>CryptoHash: [u8; 32]</code></li>
</ul>
<h2 id="all-historical-state-changes-col35"><a class="header" href="#all-historical-state-changes-col35">All Historical State Changes (col35)</a></h2>
<ul>
<li>The state changes are stored in column family <code>StateChanges</code>, number 35</li>
<li>In this family, each key is of the form <code>BlockHash | Column | AdditionalInfo</code> where:
<ul>
<li><code>BlockHash: [u8; 32]</code> is the block hash for this change</li>
<li><code>Column: u8</code> is defined near the top of <code>core/primitives/src/trie_key.rs</code></li>
<li><code>AdditionalInfo</code> depends on <code>Column</code> and is can be found in the code for the <code>TrieKey</code> struct, same file as <code>Column</code></li>
</ul>
</li>
</ul>
<h3 id="contract-deployments"><a class="header" href="#contract-deployments">Contract Deployments</a></h3>
<ul>
<li>Contract deployments happen with <code>Column = 0x01</code></li>
<li><code>AdditionalInfo</code> is the account id for which the contract is being deployed</li>
<li>The key value contains the contract code alongside other pieces of data. It is possible to extract the contract code by removing everything until the wasm magic number, 0061736D01000000</li>
<li>As such, it is possible to dump all the contracts that were ever deployed on-chain using this command:
<pre><code>ldb --db=. scan --column_family=col35 --hex | \
    grep -E '^0x.{64}01' | \
    sed 's/^.*0061736D01000000/0061736D01000000/' | \
    grep -v ' : '
</code></pre>
(Note that the last grep is required because not every such value appears to contain contract code)
We should implement a feature to state-viewer that’d allow better visualization of this data, but in the meantime this seems to work.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
